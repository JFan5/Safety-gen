
CondaError: Run 'conda init' before 'conda activate'

[torchao|WARNING]Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
wandb: Currently logged in as: fjl2401 (fjl2401-university-of-notre-dame) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
============================================================
PDDL Fine-tuning with unsloth/gpt-oss-20b-unsloth-bnb-4bit
============================================================
GPU count: 1
GPU 0: NVIDIA A100 80GB PCIe

Loading model and tokenizer...
==((====))==  Unsloth 2025.10.12: Fast Gpt_Oss patching. Transformers: 4.56.2.
   \\   /|    NVIDIA A100 80GB PCIe. Num GPUs = 1. Max memory: 79.151 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:03,  1.00s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.11s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.47it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.22it/s]
Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.
Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.
Configuring LoRA...
Unsloth: Making `model.base_model.model.model` require gradients

Loading dataset from data/sft/spanner/pddl3.hf...
Loading HuggingFace dataset...
Dataset loaded with 2000 entries
Scenario distribution:
  spanner: 2000

Filtering scenarios to: ['spanner']
Filtered dataset size: 2000
Processing dataset format (chat template)...
Map:   0%|          | 0/1900 [00:00<?, ? examples/s]Map:  10%|â–ˆ         | 194/1900 [00:00<00:00, 1925.56 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 957/1900 [00:00<00:00, 5265.75 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1529/1900 [00:00<00:00, 5470.16 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1900/1900 [00:00<00:00, 5107.93 examples/s]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 5604.44 examples/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
wandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /home/ubuntu/Safety-gen/wandb/run-20251031_010521-0w7rpush
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pddl_sft_pddl3
wandb: â­ï¸ View project at https://wandb.ai/fjl2401-university-of-notre-dame/pddl-finetune
wandb: ðŸš€ View run at https://wandb.ai/fjl2401-university-of-notre-dame/pddl-finetune/runs/0w7rpush
wandb: Detected [huggingface_hub.inference] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/

Testing initial model performance...
Initial model output:
analysisWe need to produce a valid plan for given PDDL problem.

First, parse domain. Types: location, locatable, man, nut, spanner? Wait types: location locatable - object. Then man, nut, spanner - locatable. So spanner type spanner.

Predicates:

(at ?m - locatable ?l - location)
(carrying ?m - man ?s - spanner)
(useable ?s - spanner)
(link ?l1 - location ?l2 - location)
(tightened ?n - nut)
(loose ?n - nut)

Actions:

walk: from start to end: preconditions: at ?m ?start, link ?start ?end. eff...

Resolved training arguments:
  num_train_epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2e-05
  warmup_ratio: 0.1
  weight_decay: 0.05
  max_seq_length: 2048
  load_in_4bit: True

Creating trainer...
Unsloth: Tokenizing ["text"] (num_proc=32):   0%|          | 0/1900 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):   3%|â–Ž         | 60/1900 [00:01<00:59, 31.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):   6%|â–‹         | 120/1900 [00:02<00:29, 61.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):   9%|â–‰         | 180/1900 [00:02<00:17, 98.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  13%|â–ˆâ–Ž        | 240/1900 [00:02<00:11, 139.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  16%|â–ˆâ–Œ        | 300/1900 [00:02<00:08, 179.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  22%|â–ˆâ–ˆâ–       | 420/1900 [00:02<00:05, 291.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 480/1900 [00:03<00:05, 253.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 540/1900 [00:03<00:05, 253.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–      | 660/1900 [00:03<00:03, 318.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 720/1900 [00:03<00:03, 300.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 838/1900 [00:04<00:03, 300.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 956/1900 [00:04<00:02, 345.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1015/1900 [00:04<00:02, 308.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1074/1900 [00:05<00:02, 290.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1133/1900 [00:05<00:02, 307.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1192/1900 [00:05<00:02, 316.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1251/1900 [00:05<00:02, 223.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1369/1900 [00:06<00:01, 267.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1487/1900 [00:06<00:01, 354.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1546/1900 [00:06<00:01, 322.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1605/1900 [00:06<00:01, 280.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1723/1900 [00:07<00:00, 386.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1782/1900 [00:07<00:00, 369.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1841/1900 [00:07<00:00, 384.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1900/1900 [00:07<00:00, 333.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1900/1900 [00:08<00:00, 236.69 examples/s]
Unsloth: Tokenizing ["text"] (num_proc=32):   0%|          | 0/100 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):   4%|â–         | 4/100 [00:01<00:43,  2.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):   8%|â–Š         | 8/100 [00:01<00:19,  4.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  12%|â–ˆâ–        | 12/100 [00:02<00:11,  7.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  16%|â–ˆâ–Œ        | 16/100 [00:02<00:08, 10.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  19%|â–ˆâ–‰        | 19/100 [00:02<00:06, 11.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  22%|â–ˆâ–ˆâ–       | 22/100 [00:02<00:05, 13.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:02<00:05, 14.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:02<00:04, 15.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:03<00:06, 10.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:03<00:03, 15.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:03<00:03, 16.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:04<00:02, 17.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:04<00:02, 15.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:04<00:02, 18.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:05<00:01, 18.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:05<00:01, 18.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:05<00:01, 21.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:00, 21.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:05<00:00, 20.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 14.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:06<00:00, 19.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:06<00:00, 19.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:06<00:00, 18.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:07<00:00, 13.54 examples/s]
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998, 'pad_token_id': 200002}.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,900 | Num Epochs = 3 | Total steps = 357
O^O/ \_/ \    Batch size per device = 4 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16
 "-____-"     Trainable parameters = 15,925,248 of 20,930,682,432 (0.08% trained)

Starting training...
  0%|          | 0/357 [00:00<?, ?it/s]  0%|          | 1/357 [00:15<1:29:07, 15.02s/it]  1%|          | 2/357 [00:21<59:50, 10.11s/it]    1%|          | 3/357 [00:28<50:20,  8.53s/it]  1%|          | 4/357 [00:35<45:52,  7.80s/it]  1%|â–         | 5/357 [00:41<43:20,  7.39s/it]  2%|â–         | 6/357 [00:48<41:45,  7.14s/it]  2%|â–         | 7/357 [00:54<40:40,  6.97s/it]  2%|â–         | 8/357 [01:01<39:57,  6.87s/it]  3%|â–Ž         | 9/357 [01:08<39:25,  6.80s/it]  3%|â–Ž         | 10/357 [01:14<39:01,  6.75s/it]                                                  3%|â–Ž         | 10/357 [01:14<39:01,  6.75s/it]  3%|â–Ž         | 11/357 [01:21<38:44,  6.72s/it]  3%|â–Ž         | 12/357 [01:28<38:29,  6.69s/it]  4%|â–Ž         | 13/357 [01:34<38:16,  6.68s/it]  4%|â–         | 14/357 [01:41<38:06,  6.67s/it]  4%|â–         | 15/357 [01:48<37:57,  6.66s/it]  4%|â–         | 16/357 [01:54<37:50,  6.66s/it]  5%|â–         | 17/357 [02:01<37:43,  6.66s/it]  5%|â–Œ         | 18/357 [02:08<37:35,  6.65s/it]  5%|â–Œ         | 19/357 [02:14<37:29,  6.65s/it]  6%|â–Œ         | 20/357 [02:21<37:22,  6.65s/it]                                                  6%|â–Œ         | 20/357 [02:21<37:22,  6.65s/it]  6%|â–Œ         | 21/357 [02:28<37:15,  6.65s/it]  6%|â–Œ         | 22/357 [02:34<37:07,  6.65s/it]  6%|â–‹         | 23/357 [02:41<37:01,  6.65s/it]  7%|â–‹         | 24/357 [02:47<36:57,  6.66s/it]  7%|â–‹         | 25/357 [02:54<36:53,  6.67s/it]  7%|â–‹         | 26/357 [03:01<36:48,  6.67s/it]  8%|â–Š         | 27/357 [03:08<36:41,  6.67s/it]  8%|â–Š         | 28/357 [03:14<36:34,  6.67s/it]  8%|â–Š         | 29/357 [03:21<36:27,  6.67s/it]  8%|â–Š         | 30/357 [03:28<36:21,  6.67s/it]                                                  8%|â–Š         | 30/357 [03:28<36:21,  6.67s/it]  9%|â–Š         | 31/357 [03:34<36:15,  6.67s/it]  9%|â–‰         | 32/357 [03:41<36:07,  6.67s/it]  9%|â–‰         | 33/357 [03:48<36:00,  6.67s/it] 10%|â–‰         | 34/357 [03:55<36:36,  6.80s/it] 10%|â–‰         | 35/357 [04:01<36:17,  6.76s/it] 10%|â–ˆ         | 36/357 [04:08<36:03,  6.74s/it] 10%|â–ˆ         | 37/357 [04:15<35:53,  6.73s/it] 11%|â–ˆ         | 38/357 [04:21<35:42,  6.72s/it] 11%|â–ˆ         | 39/357 [04:28<35:34,  6.71s/it] 11%|â–ˆ         | 40/357 [04:35<35:24,  6.70s/it]                                                 11%|â–ˆ         | 40/357 [04:35<35:24,  6.70s/it] 11%|â–ˆâ–        | 41/357 [04:41<35:15,  6.70s/it] 12%|â–ˆâ–        | 42/357 [04:48<35:07,  6.69s/it] 12%|â–ˆâ–        | 43/357 [04:55<34:59,  6.69s/it] 12%|â–ˆâ–        | 44/357 [05:02<34:51,  6.68s/it] 13%|â–ˆâ–Ž        | 45/357 [05:08<34:44,  6.68s/it] 13%|â–ˆâ–Ž        | 46/357 [05:15<34:37,  6.68s/it] 13%|â–ˆâ–Ž        | 47/357 [05:22<34:29,  6.68s/it] 13%|â–ˆâ–Ž        | 48/357 [05:28<34:24,  6.68s/it] 14%|â–ˆâ–Ž        | 49/357 [05:35<34:16,  6.68s/it] 14%|â–ˆâ–        | 50/357 [05:42<34:10,  6.68s/it]                                                 14%|â–ˆâ–        | 50/357 [05:42<34:10,  6.68s/it] 14%|â–ˆâ–        | 51/357 [05:48<34:04,  6.68s/it] 15%|â–ˆâ–        | 52/357 [05:55<33:58,  6.69s/it] 15%|â–ˆâ–        | 53/357 [06:02<33:54,  6.69s/it] 15%|â–ˆâ–Œ        | 54/357 [06:08<33:46,  6.69s/it] 15%|â–ˆâ–Œ        | 55/357 [06:15<33:39,  6.69s/it] 16%|â–ˆâ–Œ        | 56/357 [06:22<33:34,  6.69s/it] 16%|â–ˆâ–Œ        | 57/357 [06:28<33:27,  6.69s/it] 16%|â–ˆâ–Œ        | 58/357 [06:35<33:20,  6.69s/it] 17%|â–ˆâ–‹        | 59/357 [06:42<33:13,  6.69s/it] 17%|â–ˆâ–‹        | 60/357 [06:48<33:05,  6.69s/it]                                                 17%|â–ˆâ–‹        | 60/357 [06:48<33:05,  6.69s/it] 17%|â–ˆâ–‹        | 61/357 [06:55<32:58,  6.68s/it] 17%|â–ˆâ–‹        | 62/357 [07:02<32:51,  6.68s/it] 18%|â–ˆâ–Š        | 63/357 [07:09<32:44,  6.68s/it] 18%|â–ˆâ–Š        | 64/357 [07:15<32:38,  6.68s/it] 18%|â–ˆâ–Š        | 65/357 [07:22<32:32,  6.69s/it] 18%|â–ˆâ–Š        | 66/357 [07:29<32:26,  6.69s/it] 19%|â–ˆâ–‰        | 67/357 [07:36<32:54,  6.81s/it] 19%|â–ˆâ–‰        | 68/357 [07:42<32:38,  6.78s/it] 19%|â–ˆâ–‰        | 69/357 [07:49<32:25,  6.75s/it] 20%|â–ˆâ–‰        | 70/357 [07:56<32:12,  6.73s/it]                                                 20%|â–ˆâ–‰        | 70/357 [07:56<32:12,  6.73s/it] 20%|â–ˆâ–‰        | 71/357 [08:02<32:03,  6.72s/it] 20%|â–ˆâ–ˆ        | 72/357 [08:09<31:53,  6.71s/it] 20%|â–ˆâ–ˆ        | 73/357 [08:16<31:44,  6.71s/it] 21%|â–ˆâ–ˆ        | 74/357 [08:23<31:36,  6.70s/it] 21%|â–ˆâ–ˆ        | 75/357 [08:29<31:30,  6.70s/it] 21%|â–ˆâ–ˆâ–       | 76/357 [08:36<31:21,  6.70s/it] 22%|â–ˆâ–ˆâ–       | 77/357 [08:43<31:14,  6.70s/it] 22%|â–ˆâ–ˆâ–       | 78/357 [08:49<31:08,  6.70s/it] 22%|â–ˆâ–ˆâ–       | 79/357 [08:56<31:03,  6.70s/it] 22%|â–ˆâ–ˆâ–       | 80/357 [09:03<30:56,  6.70s/it]                                                 22%|â–ˆâ–ˆâ–       | 80/357 [09:03<30:56,  6.70s/it] 23%|â–ˆâ–ˆâ–Ž       | 81/357 [09:09<30:48,  6.70s/it] 23%|â–ˆâ–ˆâ–Ž       | 82/357 [09:16<30:41,  6.70s/it] 23%|â–ˆâ–ˆâ–Ž       | 83/357 [09:23<30:33,  6.69s/it] 24%|â–ˆâ–ˆâ–Ž       | 84/357 [09:29<30:27,  6.69s/it] 24%|â–ˆâ–ˆâ–       | 85/357 [09:36<30:21,  6.70s/it] 24%|â–ˆâ–ˆâ–       | 86/357 [09:43<30:14,  6.70s/it] 24%|â–ˆâ–ˆâ–       | 87/357 [09:50<30:07,  6.70s/it] 25%|â–ˆâ–ˆâ–       | 88/357 [09:56<30:00,  6.69s/it] 25%|â–ˆâ–ˆâ–       | 89/357 [10:03<29:55,  6.70s/it] 25%|â–ˆâ–ˆâ–Œ       | 90/357 [10:10<29:48,  6.70s/it]                                                 25%|â–ˆâ–ˆâ–Œ       | 90/357 [10:10<29:48,  6.70s/it] 25%|â–ˆâ–ˆâ–Œ       | 91/357 [10:16<29:41,  6.70s/it] 26%|â–ˆâ–ˆâ–Œ       | 92/357 [10:23<29:34,  6.70s/it] 26%|â–ˆâ–ˆâ–Œ       | 93/357 [10:30<29:28,  6.70s/it] 26%|â–ˆâ–ˆâ–‹       | 94/357 [10:36<29:20,  6.70s/it] 27%|â–ˆâ–ˆâ–‹       | 95/357 [10:43<29:14,  6.70s/it] 27%|â–ˆâ–ˆâ–‹       | 96/357 [10:50<29:07,  6.69s/it] 27%|â–ˆâ–ˆâ–‹       | 97/357 [10:57<29:00,  6.69s/it] 27%|â–ˆâ–ˆâ–‹       | 98/357 [11:03<28:53,  6.69s/it] 28%|â–ˆâ–ˆâ–Š       | 99/357 [11:10<28:46,  6.69s/it] 28%|â–ˆâ–ˆâ–Š       | 100/357 [11:17<28:39,  6.69s/it]                                                  28%|â–ˆâ–ˆâ–Š       | 100/357 [11:17<28:39,  6.69s/it] 28%|â–ˆâ–ˆâ–Š       | 101/357 [11:24<29:06,  6.82s/it] 29%|â–ˆâ–ˆâ–Š       | 102/357 [11:30<28:51,  6.79s/it] 29%|â–ˆâ–ˆâ–‰       | 103/357 [11:37<28:37,  6.76s/it] 29%|â–ˆâ–ˆâ–‰       | 104/357 [11:44<28:26,  6.75s/it] 29%|â–ˆâ–ˆâ–‰       | 105/357 [11:51<28:16,  6.73s/it] 30%|â–ˆâ–ˆâ–‰       | 106/357 [11:57<28:06,  6.72s/it] 30%|â–ˆâ–ˆâ–‰       | 107/357 [12:04<27:57,  6.71s/it] 30%|â–ˆâ–ˆâ–ˆ       | 108/357 [12:11<27:49,  6.71s/it] 31%|â–ˆâ–ˆâ–ˆ       | 109/357 [12:17<27:43,  6.71s/it] 31%|â–ˆâ–ˆâ–ˆ       | 110/357 [12:24<27:36,  6.71s/it]                                                  31%|â–ˆâ–ˆâ–ˆ       | 110/357 [12:24<27:36,  6.71s/it] 31%|â–ˆâ–ˆâ–ˆ       | 111/357 [12:31<27:30,  6.71s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 112/357 [12:37<27:22,  6.70s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 113/357 [12:44<27:15,  6.70s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 114/357 [12:51<27:08,  6.70s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 115/357 [12:58<27:01,  6.70s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 116/357 [13:04<26:54,  6.70s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 117/357 [13:11<26:47,  6.70s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 118/357 [13:18<26:39,  6.69s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 119/357 [13:23<24:33,  6.19s/it]Unsloth: Not an error, but GptOssForCausalLM does not accept `num_items_in_batch`.
Using gradient accumulation will be very slightly less accurate.
Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 4.2782, 'grad_norm': 42.62675094604492, 'learning_rate': 5e-06, 'epoch': 0.08}
{'loss': 3.4744, 'grad_norm': 20.60320281982422, 'learning_rate': 1.0555555555555557e-05, 'epoch': 0.17}
{'loss': 2.1955, 'grad_norm': 3.2329511642456055, 'learning_rate': 1.6111111111111115e-05, 'epoch': 0.25}
{'loss': 1.4664, 'grad_norm': 1.9826334714889526, 'learning_rate': 1.9995690062269985e-05, 'epoch': 0.34}
{'loss': 1.1923, 'grad_norm': 1.6013113260269165, 'learning_rate': 1.9919172253651637e-05, 'epoch': 0.42}
{'loss': 0.9076, 'grad_norm': 1.7431933879852295, 'learning_rate': 1.974772117649135e-05, 'epoch': 0.51}
{'loss': 0.6025, 'grad_norm': 2.4168782234191895, 'learning_rate': 1.9482977734962753e-05, 'epoch': 0.59}
{'loss': 0.3131, 'grad_norm': 1.9926213026046753, 'learning_rate': 1.9127475705028864e-05, 'epoch': 0.67}
{'loss': 0.1201, 'grad_norm': 1.1662225723266602, 'learning_rate': 1.8684617484471662e-05, 'epoch': 0.76}
{'loss': 0.054, 'grad_norm': 0.678080677986145, 'learning_rate': 1.815864152961624e-05, 'epoch': 0.84}
{'loss': 0.0415, 'grad_norm': 0.2976067066192627, 'learning_rate': 1.7554581790402372e-05, 'epoch': 0.93}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|â–Š         | 2/25 [00:01<00:13,  1.66it/s][A
 12%|â–ˆâ–        | 3/25 [00:02<00:20,  1.06it/s][A
 16%|â–ˆâ–Œ        | 4/25 [00:04<00:23,  1.12s/it][A
 20%|â–ˆâ–ˆ        | 5/25 [00:05<00:25,  1.28s/it][A
 24%|â–ˆâ–ˆâ–       | 6/25 [00:07<00:25,  1.33s/it][A
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:08<00:24,  1.35s/it][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:09<00:23,  1.37s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:11<00:23,  1.44s/it][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:12<00:21,  1.43s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:14<00:19,  1.43s/it][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:15<00:18,  1.42s/it][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:17<00:17,  1.48s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:18<00:16,  1.46s/it][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:20<00:14,  1.44s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:21<00:12,  1.43s/it][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:23<00:11,  1.48s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:24<00:10,  1.46s/it][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:25<00:08,  1.45s/it][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:27<00:07,  1.44s/it][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:28<00:05,  1.49s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:30<00:04,  1.47s/it][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:31<00:02,  1.45s/it][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:33<00:01,  1.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:34<00:00,  1.49s/it][A                                                 
                                               [A 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 119/357 [13:59<24:33,  6.19s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:35<00:00,  1.49s/it][A
                                               [A 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 120/357 [14:38<1:45:55, 26.82s/it]                                                    34%|â–ˆâ–ˆâ–ˆâ–Ž      | 120/357 [14:38<1:45:55, 26.82s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 121/357 [14:43<1:19:42, 20.26s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 122/357 [14:47<1:00:51, 15.54s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 123/357 [14:52<47:41, 12.23s/it]   35%|â–ˆâ–ˆâ–ˆâ–      | 124/357 [14:56<38:31,  9.92s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 125/357 [15:01<32:32,  8.42s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 126/357 [15:06<27:54,  7.25s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 127/357 [15:10<24:37,  6.43s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 128/357 [15:15<22:19,  5.85s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 129/357 [15:19<20:42,  5.45s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 130/357 [15:24<19:58,  5.28s/it]                                                  36%|â–ˆâ–ˆâ–ˆâ–‹      | 130/357 [15:24<19:58,  5.28s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 131/357 [15:28<19:01,  5.05s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 132/357 [15:33<18:19,  4.89s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 133/357 [15:38<17:49,  4.78s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 134/357 [15:42<17:27,  4.70s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 135/357 [15:47<17:37,  4.76s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 136/357 [15:51<17:15,  4.69s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 137/357 [15:56<16:58,  4.63s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 138/357 [16:00<16:45,  4.59s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 139/357 [16:05<17:00,  4.68s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 140/357 [16:10<16:44,  4.63s/it]                                                  39%|â–ˆâ–ˆâ–ˆâ–‰      | 140/357 [16:10<16:44,  4.63s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 141/357 [16:14<16:32,  4.60s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 142/357 [16:19<16:22,  4.57s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143/357 [16:23<16:14,  4.55s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 144/357 [16:28<16:31,  4.65s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 145/357 [16:33<16:17,  4.61s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 146/357 [16:37<16:06,  4.58s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 147/357 [16:42<15:57,  4.56s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148/357 [16:47<16:13,  4.66s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 149/357 [16:51<15:59,  4.61s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 150/357 [16:56<15:48,  4.58s/it]                                                  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 150/357 [16:56<15:48,  4.58s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 151/357 [17:00<15:39,  4.56s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152/357 [17:05<15:31,  4.55s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 153/357 [17:10<15:47,  4.65s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 154/357 [17:14<15:35,  4.61s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 155/357 [17:19<15:24,  4.58s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 156/357 [17:23<15:15,  4.56s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157/357 [17:28<15:08,  4.54s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/357 [17:33<15:25,  4.65s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/357 [17:37<15:11,  4.60s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/357 [17:42<15:01,  4.57s/it]                                                  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/357 [17:42<15:01,  4.57s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161/357 [17:46<14:52,  4.56s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 162/357 [17:51<15:07,  4.66s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 163/357 [17:55<14:54,  4.61s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 164/357 [18:00<14:43,  4.58s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 165/357 [18:04<14:35,  4.56s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 166/357 [18:09<14:27,  4.54s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 167/357 [18:14<14:43,  4.65s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 168/357 [18:18<14:30,  4.61s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 169/357 [18:23<14:20,  4.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170/357 [18:27<14:11,  4.56s/it]                                                  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170/357 [18:27<14:11,  4.56s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 171/357 [18:32<14:04,  4.54s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 172/357 [18:37<14:20,  4.65s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 173/357 [18:41<14:07,  4.61s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 174/357 [18:46<13:58,  4.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 175/357 [18:50<13:49,  4.56s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 176/357 [18:55<13:42,  4.54s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 177/357 [19:00<13:56,  4.65s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 178/357 [19:04<13:44,  4.61s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 179/357 [19:09<13:34,  4.58s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 180/357 [19:13<13:26,  4.56s/it]                                                  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 180/357 [19:13<13:26,  4.56s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 181/357 [19:18<13:39,  4.66s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 182/357 [19:23<13:27,  4.61s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183/357 [19:27<13:16,  4.58s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 184/357 [19:32<13:08,  4.56s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 185/357 [19:36<13:01,  4.54s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 186/357 [19:41<13:14,  4.65s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 187/357 [19:46<13:03,  4.61s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 188/357 [19:50<12:53,  4.58s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 189/357 [19:55<12:45,  4.56s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 190/357 [20:00<12:57,  4.66s/it]                                                  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 190/357 [20:00<12:57,  4.66s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 191/357 [20:04<12:45,  4.61s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192/357 [20:09<12:35,  4.58s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 193/357 [20:13<12:27,  4.56s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/357 [20:18<12:19,  4.54s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/357 [20:22<12:32,  4.64s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/357 [20:27<12:21,  4.60s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 197/357 [20:31<12:12,  4.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 198/357 [20:36<12:04,  4.55s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 199/357 [20:40<11:57,  4.54s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 200/357 [20:45<12:09,  4.64s/it]                                                  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 200/357 [20:45<12:09,  4.64s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 201/357 [20:50<11:57,  4.60s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 202/357 [20:54<11:48,  4.57s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 203/357 [20:59<11:40,  4.55s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 204/357 [21:03<11:33,  4.54s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 205/357 [21:08<11:45,  4.64s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 206/357 [21:13<11:34,  4.60s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 207/357 [21:17<11:25,  4.57s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 208/357 [21:22<11:18,  4.55s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 209/357 [21:27<11:28,  4.65s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 210/357 [21:31<11:17,  4.61s/it]                                                  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 210/357 [21:31<11:17,  4.61s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 211/357 [21:36<11:08,  4.58s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 212/357 [21:40<11:00,  4.56s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 213/357 [21:45<10:53,  4.54s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 214/357 [21:50<11:04,  4.65s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 215/357 [21:54<10:53,  4.60s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 216/357 [21:59<10:45,  4.57s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 217/357 [22:03<10:37,  4.56s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 218/357 [22:08<10:31,  4.54s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 219/357 [22:12<10:41,  4.65s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 220/357 [22:17<10:31,  4.61s/it]                                                  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 220/357 [22:17<10:31,  4.61s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 221/357 [22:22<10:22,  4.58s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 222/357 [22:26<10:15,  4.56s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223/357 [22:31<10:24,  4.66s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 224/357 [22:35<10:13,  4.62s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 225/357 [22:40<10:05,  4.59s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 226/357 [22:44<09:57,  4.56s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 227/357 [22:49<09:51,  4.55s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 228/357 [22:54<10:00,  4.65s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 229/357 [22:58<09:50,  4.61s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/357 [23:03<09:41,  4.58s/it]                                                  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/357 [23:03<09:41,  4.58s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 231/357 [23:07<09:34,  4.56s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/357 [23:12<09:28,  4.55s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 233/357 [23:17<09:37,  4.66s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 234/357 [23:21<09:27,  4.61s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 235/357 [23:26<09:19,  4.58s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 236/357 [23:30<09:12,  4.56s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 237/357 [23:35<09:19,  4.66s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 238/357 [23:39<08:29,  4.28s/it]{'eval_loss': 0.03517254814505577, 'eval_runtime': 36.7962, 'eval_samples_per_second': 2.718, 'eval_steps_per_second': 0.679, 'epoch': 1.0}

==================================================
Testing model performance...
==================================================
Generated solution:
commentaryExecuting function: plan_text with arguments:
- domain_pddl = `"""(\\\n\\\(\\\ndefine \\-\\\n \\\\s*\\\(\\\n# DOMAIN
\\\(\\\n\\\(spanner\\\n# DOMAIN (rest of DOMAIN omitted for brevity)
\\\(\\\n)\\\(\\\n)# DOMAIN
\\\(\\\n\\\(spanner\\\n# DOMAIN (repeated DOMAIN omitted)
\\\(\\\n\\\(\\\n)# DOMAIN
\\\(\\\n\\\(spanner\\\n# PROBLEM
\\\(\\\nproblem-s3-n2-l4-s1120813989\\\n# PROBLEM (rest of PROBLEM omitted for brevity)
\\\(\\\n\\\(\\\n)# PROBLEM
\\\(\\\n\\\(spanner-s3-n2-l4-s1120813989\\\n# PROBLEM (repeated/problem omitted)
\\\(\\\n\\\(\\\n)# PROBLEM
\\\(\\\n\\\(spanner-s3-n2-l4-s1120813989\\\n# PROBLEM
\\\(\\\nproblem-s3-n2-l4-s1120813989\\\n# PROTOCOL
AGV-3
SEND_COMMAND
\\\(\\\n\\\(\\\n)# PROTOCOL
\\\(\\\n\\\(\\\n)` (this is the full PDDL with DOMAIN, PROBLEM, constraints)
- problem_pddl = `"""(\\\n\\\(\\\nPROBLEM
\\\(spanner-s3-n2-l4-s1120813989\\\n# PROBLEM (rest of PROBLEM omitted for brevity)
\\\(\\\n\\\(\\\n)# PROBLEM
\\\(spanner-s3-n2-l4-s1120813989\\\n# PROBLEM (repeated/problem omitted)
\\\(\\\n\\\(\\\n)# PROBLEM
\\\(\\asterisk-s3-n2-l4-s1120813989\\\n# PROBLEM
\\\(problem-s3-n2-l4-s1120813989\\\n# PROTOCOL
AGV-3
SEND_COMMAND
\\\(\\\n\\\(\\\n)# PROTOCOL
\\\(\\\n\\\(\\\n)` (this is the PRO
==================================================
{'loss': 0.0373, 'grad_norm': 0.27688631415367126, 'learning_rate': 1.687821953203765e-05, 'epoch': 1.01}
{'loss': 0.034, 'grad_norm': 0.4451828896999359, 'learning_rate': 1.613602800433194e-05, 'epoch': 1.09}
{'loss': 0.0318, 'grad_norm': 0.18023942410945892, 'learning_rate': 1.5335110488265497e-05, 'epoch': 1.18}
{'loss': 0.0305, 'grad_norm': 0.1620432287454605, 'learning_rate': 1.4483132312727501e-05, 'epoch': 1.26}
{'loss': 0.0296, 'grad_norm': 0.14066149294376373, 'learning_rate': 1.358824749207136e-05, 'epoch': 1.35}
{'loss': 0.0288, 'grad_norm': 0.14570967853069305, 'learning_rate': 1.2659020686615602e-05, 'epoch': 1.43}
{'loss': 0.0277, 'grad_norm': 0.18510042130947113, 'learning_rate': 1.170434523298175e-05, 'epoch': 1.51}
{'loss': 0.0273, 'grad_norm': 0.11059781163930893, 'learning_rate': 1.073335802877504e-05, 'epoch': 1.6}
{'loss': 0.0271, 'grad_norm': 0.3192748725414276, 'learning_rate': 9.755352086219733e-06, 'epoch': 1.68}
{'loss': 0.027, 'grad_norm': 0.07696374505758286, 'learning_rate': 8.779687591670687e-06, 'epoch': 1.77}
{'loss': 0.0269, 'grad_norm': 0.10295800864696503, 'learning_rate': 7.815702322222539e-06, 'epoch': 1.85}
{'loss': 0.0268, 'grad_norm': 0.12473810464143753, 'learning_rate': 6.872622276790804e-06, 'epoch': 1.93}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|â–Š         | 2/25 [00:01<00:16,  1.42it/s][A
 12%|â–ˆâ–        | 3/25 [00:02<00:21,  1.00it/s][A
 16%|â–ˆâ–Œ        | 4/25 [00:04<00:24,  1.15s/it][A
 20%|â–ˆâ–ˆ        | 5/25 [00:06<00:28,  1.40s/it][A
 24%|â–ˆâ–ˆâ–       | 6/25 [00:07<00:26,  1.41s/it][A
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:08<00:25,  1.41s/it][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:10<00:23,  1.41s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:11<00:23,  1.47s/it][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:13<00:21,  1.45s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:14<00:20,  1.44s/it][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:16<00:18,  1.43s/it][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:17<00:17,  1.48s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:19<00:16,  1.46s/it][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:20<00:14,  1.45s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:22<00:12,  1.44s/it][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:23<00:11,  1.49s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:25<00:10,  1.47s/it][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:26<00:08,  1.45s/it][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:27<00:07,  1.44s/it][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:29<00:05,  1.49s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:30<00:04,  1.46s/it][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:32<00:02,  1.45s/it][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:33<00:01,  1.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:35<00:00,  1.49s/it][A                                                 
                                               [A 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 238/357 [24:15<08:29,  4.28s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:35<00:00,  1.49s/it][A
                                               [A 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 239/357 [24:51<48:37, 24.72s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 240/357 [24:56<36:23, 18.66s/it]                                                  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 240/357 [24:56<36:23, 18.66s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 241/357 [25:00<27:52, 14.42s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 242/357 [25:05<22:10, 11.57s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 243/357 [25:10<17:57,  9.45s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 244/357 [25:14<15:00,  7.97s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 245/357 [25:19<12:56,  6.93s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 246/357 [25:23<11:42,  6.33s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 247/357 [25:28<10:36,  5.79s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 248/357 [25:33<09:49,  5.41s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 249/357 [25:37<09:15,  5.14s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 250/357 [25:42<08:49,  4.95s/it]                                                  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 250/357 [25:42<08:49,  4.95s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 251/357 [25:46<08:44,  4.94s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 252/357 [25:51<08:25,  4.81s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 253/357 [25:55<08:11,  4.72s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 254/357 [26:00<08:00,  4.66s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 255/357 [26:05<07:50,  4.62s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 256/357 [26:09<07:55,  4.71s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 257/357 [26:14<07:44,  4.65s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258/357 [26:18<07:36,  4.61s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 259/357 [26:23<07:28,  4.58s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 260/357 [26:28<07:22,  4.56s/it]                                                  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 260/357 [26:28<07:22,  4.56s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 261/357 [26:32<07:28,  4.67s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 262/357 [26:37<07:19,  4.62s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 263/357 [26:41<07:11,  4.59s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 264/357 [26:46<07:04,  4.57s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 265/357 [26:50<06:58,  4.55s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 266/357 [26:55<07:03,  4.65s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 267/357 [27:00<06:55,  4.61s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 268/357 [27:04<06:47,  4.58s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 269/357 [27:09<06:41,  4.56s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 270/357 [27:14<06:45,  4.66s/it]                                                  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 270/357 [27:14<06:45,  4.66s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 271/357 [27:18<06:37,  4.62s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 272/357 [27:23<06:29,  4.59s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 273/357 [27:27<06:23,  4.56s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 274/357 [27:32<06:17,  4.55s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 275/357 [27:37<06:21,  4.66s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 276/357 [27:41<06:13,  4.61s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 277/357 [27:46<06:07,  4.59s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 278/357 [27:50<06:00,  4.57s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 279/357 [27:55<05:55,  4.55s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 280/357 [28:00<05:59,  4.66s/it]                                                  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 280/357 [28:00<05:59,  4.66s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 281/357 [28:04<05:51,  4.62s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 282/357 [28:09<05:44,  4.59s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 283/357 [28:13<05:38,  4.57s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 284/357 [28:18<05:32,  4.55s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 285/357 [28:23<05:35,  4.66s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 286/357 [28:27<05:27,  4.62s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 287/357 [28:32<05:21,  4.59s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 288/357 [28:36<05:14,  4.56s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 289/357 [28:41<05:17,  4.67s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 290/357 [28:46<05:09,  4.62s/it]                                                  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 290/357 [28:46<05:09,  4.62s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 291/357 [28:50<05:03,  4.59s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 292/357 [28:55<04:56,  4.57s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 293/357 [28:59<04:51,  4.55s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294/357 [29:04<04:53,  4.65s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 295/357 [29:09<04:45,  4.61s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 296/357 [29:13<04:39,  4.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 297/357 [29:18<04:33,  4.56s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 298/357 [29:22<04:27,  4.54s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 299/357 [29:27<04:29,  4.65s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 300/357 [29:32<04:22,  4.61s/it]                                                  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 300/357 [29:32<04:22,  4.61s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 301/357 [29:36<04:16,  4.58s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 302/357 [29:41<04:10,  4.56s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 303/357 [29:45<04:05,  4.55s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 304/357 [29:50<04:06,  4.65s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 305/357 [29:55<03:59,  4.61s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 306/357 [29:59<03:53,  4.58s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 307/357 [30:04<03:48,  4.56s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 308/357 [30:09<03:48,  4.67s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 309/357 [30:13<03:41,  4.62s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 310/357 [30:18<03:35,  4.59s/it]                                                  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 310/357 [30:18<03:35,  4.59s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 311/357 [30:22<03:30,  4.57s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 312/357 [30:27<03:24,  4.55s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 313/357 [30:32<03:25,  4.66s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 314/357 [30:36<03:18,  4.62s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 315/357 [30:41<03:12,  4.59s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 316/357 [30:45<03:07,  4.56s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 317/357 [30:50<03:01,  4.55s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 318/357 [30:54<03:01,  4.65s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 319/357 [30:59<02:55,  4.61s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 320/357 [31:03<02:49,  4.58s/it]                                                  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 320/357 [31:03<02:49,  4.58s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 321/357 [31:08<02:44,  4.56s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 322/357 [31:13<02:39,  4.54s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 323/357 [31:17<02:38,  4.66s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 324/357 [31:22<02:32,  4.61s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 325/357 [31:26<02:26,  4.58s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 326/357 [31:31<02:21,  4.56s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 327/357 [31:36<02:20,  4.67s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 328/357 [31:40<02:13,  4.62s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329/357 [31:45<02:08,  4.59s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330/357 [31:49<02:03,  4.56s/it]                                                  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330/357 [31:49<02:03,  4.56s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 331/357 [31:54<01:58,  4.55s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 332/357 [31:59<01:56,  4.66s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 333/357 [32:03<01:50,  4.61s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 334/357 [32:08<01:45,  4.58s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 335/357 [32:12<01:40,  4.56s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 336/357 [32:17<01:35,  4.55s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 337/357 [32:22<01:33,  4.65s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 338/357 [32:26<01:27,  4.61s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 339/357 [32:31<01:22,  4.58s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 340/357 [32:35<01:17,  4.56s/it]                                                  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 340/357 [32:35<01:17,  4.56s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 341/357 [32:40<01:12,  4.55s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 342/357 [32:45<01:09,  4.66s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 343/357 [32:49<01:04,  4.61s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 344/357 [32:54<00:59,  4.58s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 345/357 [32:58<00:54,  4.56s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 346/357 [33:03<00:51,  4.66s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 347/357 [33:08<00:46,  4.62s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 348/357 [33:12<00:41,  4.58s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 349/357 [33:17<00:36,  4.56s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 350/357 [33:21<00:31,  4.55s/it]                                                  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 350/357 [33:21<00:31,  4.55s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 351/357 [33:26<00:27,  4.66s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 352/357 [33:31<00:23,  4.61s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 353/357 [33:35<00:18,  4.58s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 354/357 [33:40<00:13,  4.56s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 355/357 [33:44<00:09,  4.55s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 356/357 [33:49<00:04,  4.65s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 357/357 [33:52<00:00,  4.27s/it]{'eval_loss': 0.02664084918797016, 'eval_runtime': 36.7174, 'eval_samples_per_second': 2.724, 'eval_steps_per_second': 0.681, 'epoch': 2.0}

==================================================
Testing model performance...
==================================================
Generated solution:
commentary to=functions f=plangen}{"domain":"spanner","problem":"spanner-s3-n2-l4-s1120813989"}says}analysisWe need to produce a valid plan for the given problem, respecting domain constraints, type constraints, and any constraints in the problem (under :constraints). Ensure no constraints are violated. Since there are constraints, we must satisfy them.

Problem details:

Domain: spanner, with typing, requirements, predicates as described. No additional constants.

Problem: spanner-s3-n2-l4-s1120813989

Objects:

- bob - man
- spanner1 spanner2 spanner3 - spanner
- nut1 nut2 - nut
- location1 location2 location3 location4 - location
- shed - location
- gate - location

Initial state (:init):

1. (at bob shed)
2. (at spanner1 location3)
3. (useable spanner1)
4. (at spanner2 location3)
5. (useable spanner2)
6. (at spanner3 location3)
7. (useable spanner3)
8. (loose nut1)
9. (at nut1 gate)
10. (loose nut2)
11. (at nut2 gate)
12. (link shed location1)
13. (link location4 gate)
14. (link location1 location2)
15. (link location2 location3)
16. (link location3 location4)

Goal (:goal) under PDDL2 style:

(and
   (tightened nut1)
   (tightened nut2)
)

Constraints (:constraints) using PDDL3 constraint syntax:

(and
  (sometime-before (tightened nut1) (tightened nut2))
  (forall (?m - man) (at-most-once (at ?m shed)))
)

So constraints:

- nut1 tightened before nut2 tightened (i.e., nut2 cannot be tightened until after nut1 is tightened).
- For all m of type man, the action (at ?m shed) can occur at most once. That is, each man can start in shed at most once; they cannot return to shed more than once.

Given only bob is a man? Let's list objects of type man: under problem, objects: bob - man. No other manif? Domain defines type: man - object. Problem declares only bob as man. So the forall
==================================================
{'loss': 0.0267, 'grad_norm': 0.09475429356098175, 'learning_rate': 5.959473376986686e-06, 'epoch': 2.02}
{'loss': 0.0268, 'grad_norm': 0.11888913810253143, 'learning_rate': 5.084995082868658e-06, 'epoch': 2.1}
{'loss': 0.0267, 'grad_norm': 0.09032439440488815, 'learning_rate': 4.257556750327176e-06, 'epoch': 2.19}
{'loss': 0.0266, 'grad_norm': 0.09647957235574722, 'learning_rate': 3.485077530619664e-06, 'epoch': 2.27}
{'loss': 0.0267, 'grad_norm': 0.12625384330749512, 'learning_rate': 2.77495057867198e-06, 'epoch': 2.35}
{'loss': 0.0267, 'grad_norm': 0.1654863953590393, 'learning_rate': 2.133972295524875e-06, 'epoch': 2.44}
{'loss': 0.0268, 'grad_norm': 0.20013006031513214, 'learning_rate': 1.5682772821236192e-06, 'epoch': 2.52}
{'loss': 0.0265, 'grad_norm': 0.0987253487110138, 'learning_rate': 1.0832796269875757e-06, 'epoch': 2.61}
{'loss': 0.0266, 'grad_norm': 0.1269962638616562, 'learning_rate': 6.836210896769014e-07, 'epoch': 2.69}
{'loss': 0.0267, 'grad_norm': 0.07864081114530563, 'learning_rate': 3.731266759760854e-07, 'epoch': 2.77}
{'loss': 0.0266, 'grad_norm': 0.10578598082065582, 'learning_rate': 1.5476802997022812e-07, 'epoch': 2.86}
{'loss': 0.0266, 'grad_norm': 0.1138836145401001, 'learning_rate': 3.063499337692788e-08, 'epoch': 2.94}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|â–Š         | 2/25 [00:01<00:16,  1.42it/s][A
 12%|â–ˆâ–        | 3/25 [00:02<00:21,  1.00it/s][A
 16%|â–ˆâ–Œ        | 4/25 [00:04<00:24,  1.15s/it][A
 20%|â–ˆâ–ˆ        | 5/25 [00:06<00:28,  1.40s/it][A
 24%|â–ˆâ–ˆâ–       | 6/25 [00:07<00:26,  1.41s/it][A
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:08<00:25,  1.41s/it][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:10<00:23,  1.41s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:11<00:23,  1.47s/it][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:13<00:21,  1.45s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:14<00:20,  1.44s/it][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:16<00:18,  1.43s/it][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:17<00:17,  1.48s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:19<00:16,  1.46s/it][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:20<00:14,  1.45s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:22<00:12,  1.44s/it][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:23<00:11,  1.49s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:25<00:10,  1.46s/it][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:26<00:08,  1.45s/it][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:27<00:07,  1.44s/it][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:29<00:05,  1.49s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:30<00:04,  1.47s/it][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:32<00:02,  1.45s/it][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:33<00:01,  1.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:35<00:00,  1.49s/it][A                                                 
                                               [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 357/357 [34:29<00:00,  4.27s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:35<00:00,  1.49s/it][A
                                               [A                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 357/357 [35:00<00:00,  4.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 357/357 [35:00<00:00,  5.88s/it]
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:               eval/loss â–ˆâ–â–
wandb:            eval/runtime â–ˆâ–â–ƒ
wandb: eval/samples_per_second â–â–ˆâ–†
wandb:   eval/steps_per_second â–â–ˆâ–…
wandb:             train/epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:       train/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         train/grad_norm â–ˆâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     train/learning_rate â–ƒâ–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:              train/loss â–ˆâ–‡â–…â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               eval/loss 0.02648
wandb:            eval/runtime 36.7401
wandb: eval/samples_per_second 2.722
wandb:   eval/steps_per_second 0.68
wandb:              total_flos 7.127275269390336e+17
wandb:             train/epoch 3
wandb:       train/global_step 357
wandb:         train/grad_norm 0.11388
wandb:     train/learning_rate 0.0
wandb:              train/loss 0.0266
wandb:                      +4 ...
wandb: 
wandb: ðŸš€ View run pddl_sft_pddl3 at: https://wandb.ai/fjl2401-university-of-notre-dame/pddl-finetune/runs/0w7rpush
wandb: â­ï¸ View project at: https://wandb.ai/fjl2401-university-of-notre-dame/pddl-finetune
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251031_010521-0w7rpush/logs
{'eval_loss': 0.02647620625793934, 'eval_runtime': 36.7401, 'eval_samples_per_second': 2.722, 'eval_steps_per_second': 0.68, 'epoch': 3.0}

==================================================
Testing model performance...
==================================================
Generated solution:
commentaryExecuting function: plan_pddl with arguments:
- problem_text: `/home/fjl2401/Safety-gen/spanner/spanner-generator.py 3 2 4 --seed 1120813989 --problem-name spanner-s3-n2-l4-s1120813989`
- domain_pddl: A long PDDL domain string defining spanner domain with typing, constraints, predicates, actions, etc.
- problem_pddl: A problem definition:
  - (problem spanner-s3-n2-l4-s1120813989)
  - domain: spanner
  - objects: bob(man), spanner1,spanner2,spanner3(spanner); nut1,nut2(nut); location1-4, shed,gate(location); total objects 1+3+3+4+2+2=15
  - init facts:
    (at bob shed)
    (at spanner1 location3); (useable spanner1)
    (at spanner2 location3); (useable spanner2)
    (at spanner3 location3); (useable spanner3)
    (loose nut1); (at nut1 gate)
    (loose nut2); (at nut2 gate)
    (link shed location1)
    (link location4 gate)
    (link location1 location2)
    (link location2 location3)
    (link location3 location4)
  - goal constraints under :constraints:
    (and
      (sometime-before (tightened nut1) (tightened nut2))
      (forall (?m - man) (at-most-once (at ?m shed)))
    )
- No separate :goal since constraints used.

The solver will check domain/problem validity (typing, constants, etc.) and satisfy both constraints.

Plan must be grounded, sequential. Each action must respect its type constraints:
(:types 
	location locatable - object
	man nut spanner - locatable	
)  

Thus action parameters:
- walk: ?m-man ?l1-location ?l2-location
- pickup_spanner: ?l-location ?s-spanner ?m-man
- tighten_nut: ?l-location ?s-spanner ?m-man ?n-nut

All objects available: bob, spanner1,spanner2,spanner3, nut1,nut2, location1,location2,location3,location4, shed, gate.

Need to
==================================================
{'train_runtime': 2100.7584, 'train_samples_per_second': 2.713, 'train_steps_per_second': 0.17, 'train_loss': 0.42966121309945565, 'epoch': 3.0}

Saving model to sft_models/gpt_oss_20b/spanner/pddl3 ...
Training completed!
