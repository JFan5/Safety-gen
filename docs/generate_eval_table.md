# Generate Eval Table

Generate a formatted table from evaluation results showing success rates and error category breakdowns per scenario.

## Usage

### Python Script

```bash
python script/generate_eval_table.py <eval_folder> [--format markdown|csv|latex] [--output <file>]
```

### Shell Script

```bash
./shells/generate_eval_table.sh <eval_folder> [--format markdown|csv|latex] [--output <file>]
```

## Arguments

| Argument | Required | Description |
|----------|----------|-------------|
| `eval_folder` | Yes | Path to the eval folder containing `metrics.json` |
| `--format` | No | Output format: `markdown` (default), `csv`, or `latex` |
| `--output` | No | Output file path. If not specified, prints to stdout |

## Examples

### Basic Usage (Markdown Output)

```bash
python script/generate_eval_table.py runs/grpo/grpo_qwen3-14b-curriculum_v2-all-0111-stl_20260111_015253_seed3407/eval/solver_batch_json_temp0.6_max512_bs4_20260125_191206
```

Output:
```
| Scenario | Total | Success | Format Err | Precond Viol | Safety Viol | Goal Fail | Gen Err |
|----------|-------|---------|------------|--------------|-------------|-----------|---------|
| blocksworld | 50 | 37(74.0%) | 0(0.0%) | 5(10.0%) | 5(10.0%) | 3(6.0%) | 0(0.0%) |
| ferry | 50 | 46(92.0%) | 0(0.0%) | 0(0.0%) | 4(8.0%) | 0(0.0%) | 0(0.0%) |
| grippers | 50 | 33(66.0%) | 0(0.0%) | 1(2.0%) | 14(28.0%) | 2(4.0%) | 0(0.0%) |
| spanner | 50 | 49(98.0%) | 0(0.0%) | 1(2.0%) | 0(0.0%) | 0(0.0%) | 0(0.0%) |
| **Overall** | **200** | **165(82.5%)** | **0(0.0%)** | **7(3.5%)** | **23(11.5%)** | **5(2.5%)** | **0(0.0%)** |
```

### CSV Output

```bash
python script/generate_eval_table.py runs/grpo/.../eval/... --format csv --output results.csv
```

Output (results.csv):
```csv
Scenario,Total,Success_count,Success_rate,Format Err_count,Format Err_rate,...
blocksworld,50,37,74.0,0,0.0,...
ferry,50,46,92.0,0,0.0,...
...
Overall,200,165,82.5,0,0.0,...
```

### LaTeX Output

```bash
python script/generate_eval_table.py runs/grpo/.../eval/... --format latex
```

Output:
```latex
\begin{table}[h]
\centering
\caption{Evaluation Results by Scenario}
\label{tab:eval_results}
\begin{tabular}{lrrrrrrr}
\hline
\textbf{Scenario} & \textbf{Total} & \textbf{Success} & \textbf{Format Err} & ...
\hline
Blocksworld & 50 & 37(74.0\%) & 0(0.0\%) & ...
...
\hline
\textbf{Overall} & \textbf{200} & \textbf{165(82.5\%)} & ...
\hline
\end{tabular}
\end{table}
```

### Save Output to File

```bash
# Markdown
python script/generate_eval_table.py runs/grpo/.../eval/... --output results.md

# CSV
python script/generate_eval_table.py runs/grpo/.../eval/... --format csv --output results.csv

# LaTeX
python script/generate_eval_table.py runs/grpo/.../eval/... --format latex --output results.tex
```

## Error Categories

The table displays the following error categories:

| Category | Description |
|----------|-------------|
| Success | Plans that satisfy all goals and constraints |
| Format Err | Plans with parsing/format errors |
| Precond Viol | Plans with action precondition violations |
| Safety Viol | Plans violating PDDL3 safety constraints |
| Goal Fail | Plans that don't achieve the goal state |
| Gen Err | Generation errors (model failures) |

## Data Source

The script reads from `metrics.json` in the eval folder. This file is automatically generated by the evaluation scripts (`evaluate_llm_solver_batch.py`).

Required structure:
```json
{
  "per_scenario": {
    "blocksworld": {
      "total_tests": 50,
      "success_count": 37,
      "success_rate": 74.0,
      "category_counts": {...},
      "category_rates": {...}
    },
    ...
  },
  "overall": {
    "total_tests": 200,
    "success_count": 165,
    "success_rate": 82.5,
    "category_counts": {...},
    "category_rates": {...}
  }
}
```
