#!/bin/bash

#$ -M jfan5@nd.edu
#$ -m abe
#$ -q gpu
#$ -l gpu_card=4
#$ -o job_outputs/grpo/mistral/spanner/grpo-4gpu.o
#$ -pe smp 32
#$ -N grpo_mistral_spanner_4gpu

set -euo pipefail

# Ensure we are at the repo root so relative paths below work
cd /home/ubuntu/Safety-gen

conda activate llmstl

# Multi-GPU configuration (4 GPUs)
export CUDA_VISIBLE_DEVICES=0,1,2,3

# GRPO training for Mistral-7B on spanner scenario (4 GPUs)
accelerate launch --num_processes 4 --num_machines 1 script/train_grpo_unsloth.py \
  --base_model "/groups/fkong/jfan5/sft_models/mistral_7b-spanner-variant-500" \
  --dataset "/groups/fkong/jfan5/ppo_data/spanner.jsonl" \
  --output_dir "/groups/fkong/jfan5/grpo_models/mistral_7b-spanner-variant-500" \
  --num_epochs 3.0 \
  --batch_size 4 \
  --gradient_accumulation_steps 2 \
  --learning_rate 5e-6 \
  --max_prompt_length 768 \
  --max_new_tokens 160 \
  --num_generations 4 \
  --temperature 0.8 \
  --top_p 0.9 \
  --logging_steps 10 \
  --save_steps 500 \
  --wandb_project "pddl-grpo-mistral7b" \
  --run_name "grpo_mistral_spanner_4gpu" \
  --no_4bit
