#!/bin/bash

#$ -M jfan5@nd.edu
#$ -m abe
#$ -q gpu
#$ -l gpu_card=1
#$ -o job_outputs/dpo/mistral_7b/blocksworld/pddl3-500.o
#$ -pe smp 28
#$ -N dpo_mistral_blocksworld_pddl3-500

set -euo pipefail

# Ensure we are at the repo root so relative paths below work
cd /home/ubuntu/Safety-gen
conda activate llmstl

# DPO training for Mistral-7B on blocksworld scenario (PDDL3-500)
mkdir -p "/groups/fkong/jfan5/dpo_models/mistral_7b/blocksworld/pddl3-500"

python3 script/train_dpo_unsloth.py \
  --base_model "/groups/fkong/jfan5/sft_models/mistral_7b/blocksworld/pddl3" \
  --dataset "/home/ubuntu/Safety-gen/data/dpo/datasets/blocksworld/pddl3_dpo-500.jsonl" \
  --output_dir "/groups/fkong/jfan5/dpo_models/mistral_7b/blocksworld/pddl3-500" \
  --num_epochs 3 \
  --batch_size 1 \
  --gradient_accumulation_steps 8 \
  --learning_rate 1e-5 \
  --save_steps 500 \
  --eval_steps 500 \
  --logging_steps 25 \
  --beta 0.1 \
  --memory_efficient \
  --run_name "dpo-mistral-blocksworld-pddl3-500" \
  --dataloader_num_workers 0
