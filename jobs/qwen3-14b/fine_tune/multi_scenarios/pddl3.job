#!/bin/bash

#$ -M jfan5@nd.edu
#$ -m abe
#$ -q gpu
#$ -l gpu_card=1
#$ -o job_outputs/fine_tune/qwen3_4b_instruct/multi_scenarios/pddl3.o
#$ -pe smp 8          # Specify parallel environment and legal core size
#$ -N fine_tune_qwen3_4b_instruct_multi_scenarios_pddl3     # Specify job name

conda activate llmstl

# Aggregated multi-scenario fine-tune for Qwen3-4B (PDDL3 dataset).
# Generate the dataset via:
#   python3 script/collect_finetuned_dataset_together.py --pddl PDDL3 --output /groups/fkong/jfan5/data/sft/multi_scenarios/pddl3.hf
MODEL_NAME="unsloth/Qwen3-4B-Instruct-2507"
OUTPUT_NAME="qwen3_4b_instruct/multi_scenarios/pddl3"
DATASET_PATH="/groups/fkong/jfan5/data/sft/multi_scenarios/pddl3.hf"

python3 pddl_finetune.py \
  --mode train \
  --model "$MODEL_NAME" \
  --output "$OUTPUT_NAME" \
  --dataset "$DATASET_PATH" \
  --scenarios all
  --max-seq-length 1536 \
  --per-device-train-batch-size 4 \
  --gradient-accumulation-steps 2 \
  --load-in-4bit \
  --scenarios all
