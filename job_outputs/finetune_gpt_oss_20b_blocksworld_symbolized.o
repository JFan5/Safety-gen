
CondaError: Run 'conda init' before 'conda activate'

==========================================
Fine-tuning GPT-OSS-20B on Blocksworld (PDDL3 symbolized)
==========================================
Model: unsloth/gpt-oss-20b-unsloth-bnb-4bit
Dataset: /jfan5/sft_data/pddl3_symbolized_four_scenarios/blocksworld.hf
Output: /jfan5/sft_models/gpt_oss_20b/blocksworld_pddl3_symbolized

Training parameters:
  Epochs: 3
  Batch size: 8
  Gradient accumulation: 4
  Learning rate: 2e-4
  Max sequence length: 4096
==========================================

ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
wandb: Currently logged in as: fjl2401 (fjl2401-university-of-notre-dame) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
============================================================
PDDL Fine-tuning with unsloth/gpt-oss-20b-unsloth-bnb-4bit
============================================================
GPU count: 1
GPU 0: NVIDIA H100 PCIe

Loading model and tokenizer...
==((====))==  Unsloth 2025.11.6: Fast Gpt_Oss patching. Transformers: 4.56.2. vLLM: 0.12.0.
   \\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.109 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.9.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:17<00:51, 17.28s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:36<00:37, 18.55s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:52<00:17, 17.35s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:55<00:00, 11.72s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:55<00:00, 13.93s/it]
Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.
Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.
Configuring LoRA...
Unsloth: Making `model.base_model.model.model` require gradients

Loading dataset from /jfan5/sft_data/pddl3_symbolized_four_scenarios/blocksworld.hf...
Loading HuggingFace dataset...
Detected single Dataset
Dataset loaded with 1000 entries
Scenario distribution:
  blocksworld: 1000
Validation ratio: 0.05
Processing dataset format (chat template)...
Map:   0%|          | 0/950 [00:00<?, ? examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 688/950 [00:00<00:00, 6833.36 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 950/950 [00:00<00:00, 6626.70 examples/s]
Map:   0%|          | 0/50 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 4909.18 examples/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
wandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /home/ubuntu/Safety-gen/wandb/run-20251212_150900-wkh6fdtg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pddl_sft_blocksworld_pddl3_symbolized
wandb: â­ï¸ View project at https://wandb.ai/fjl2401-university-of-notre-dame/pddl-finetune
wandb: ðŸš€ View run at https://wandb.ai/fjl2401-university-of-notre-dame/pddl-finetune/runs/wkh6fdtg
wandb: Detected [huggingface_hub.inference, openai] in use.
wandb: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/

Testing initial model performance...
Initial model output:
analysisWe have a PDDL problem. The domain defines actions op_1 to op_4 with preconditions/effects. We have initial state and goal state with some constraints that a certain action sequence must satisfy some ordering relationship: pred_2 obj_06 obj_01 must occur before pred_2 obj_04 obj_02.

Need to produce plan or answer a question. The user hasn't asked a direct question. They just provided domain and problem. Perhaps they want to know if goal is reachable? Or they want a solution or explanati...

Resolved training arguments:
  num_train_epochs: 3.0
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 0.0002
  warmup_ratio: 0.1
  weight_decay: 0.05
  max_seq_length: 4096
  load_in_4bit: True

Creating trainer...
Unsloth: Tokenizing ["text"] (num_proc=32):   0%|          | 0/950 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):   3%|â–Ž         | 30/950 [00:01<01:00, 15.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):   6%|â–‹         | 60/950 [00:02<00:29, 29.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):   9%|â–‰         | 90/950 [00:02<00:18, 46.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  13%|â–ˆâ–Ž        | 120/950 [00:02<00:12, 64.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  16%|â–ˆâ–Œ        | 150/950 [00:03<00:13, 60.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  22%|â–ˆâ–ˆâ–       | 210/950 [00:03<00:07, 102.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 240/950 [00:03<00:06, 113.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 270/950 [00:03<00:05, 124.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 300/950 [00:03<00:04, 133.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–      | 330/950 [00:04<00:04, 141.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 360/950 [00:04<00:04, 147.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 390/950 [00:04<00:03, 151.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420/950 [00:04<00:03, 155.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 450/950 [00:04<00:03, 158.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 480/950 [00:05<00:03, 124.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 540/950 [00:05<00:02, 173.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 570/950 [00:05<00:02, 170.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 600/950 [00:05<00:02, 142.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 630/950 [00:06<00:02, 134.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 689/950 [00:06<00:01, 142.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 718/950 [00:06<00:01, 139.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 747/950 [00:07<00:01, 138.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 776/950 [00:07<00:01, 134.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 805/950 [00:07<00:01, 102.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 863/950 [00:07<00:00, 133.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 892/950 [00:08<00:00, 143.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 921/950 [00:08<00:00, 131.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 950/950 [00:08<00:00, 134.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 950/950 [00:09<00:00, 105.31 examples/s]
Unsloth: Tokenizing ["text"] (num_proc=32):   0%|          | 0/50 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):   4%|â–         | 2/50 [00:02<00:56,  1.18s/ examples]Unsloth: Tokenizing ["text"] (num_proc=32):  12%|â–ˆâ–        | 6/50 [00:02<00:15,  2.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  16%|â–ˆâ–Œ        | 8/50 [00:02<00:11,  3.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  20%|â–ˆâ–ˆ        | 10/50 [00:03<00:08,  4.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 14/50 [00:03<00:05,  6.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:03<00:05,  6.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [00:04<00:04,  7.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [00:04<00:03,  7.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [00:04<00:03,  7.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [00:05<00:02,  7.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [00:05<00:02,  7.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [00:05<00:02,  7.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [00:06<00:01,  9.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [00:06<00:01,  8.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [00:06<00:02,  5.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [00:07<00:01,  6.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [00:07<00:01,  5.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [00:07<00:00,  5.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [00:08<00:00,  4.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [00:08<00:00,  6.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [00:08<00:00,  6.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:08<00:00,  6.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:09<00:00,  5.49 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 950 | Num Epochs = 3 | Total steps = 90
O^O/ \_/ \    Batch size per device = 8 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32
 "-____-"     Trainable parameters = 15,925,248 of 20,930,682,432 (0.08% trained)

Starting training...
  0%|          | 0/90 [00:00<?, ?it/s]  1%|          | 1/90 [01:41<2:30:00, 101.13s/it]  2%|â–         | 2/90 [02:03<1:20:40, 55.00s/it]   3%|â–Ž         | 3/90 [02:25<57:48, 39.87s/it]    4%|â–         | 4/90 [02:47<46:49, 32.67s/it]  6%|â–Œ         | 5/90 [03:08<40:17, 28.45s/it]  7%|â–‹         | 6/90 [03:29<36:12, 25.86s/it]  8%|â–Š         | 7/90 [03:50<33:40, 24.34s/it]  9%|â–‰         | 8/90 [04:11<31:46, 23.25s/it] 10%|â–ˆ         | 9/90 [04:32<30:26, 22.54s/it] 11%|â–ˆ         | 10/90 [04:53<29:43, 22.29s/it]                                                11%|â–ˆ         | 10/90 [04:53<29:43, 22.29s/it]Unsloth: Not an error, but GptOssForCausalLM does not accept `num_items_in_batch`.
Using gradient accumulation will be very slightly less accurate.
Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 3.2593, 'grad_norm': 2.0805776119232178, 'learning_rate': 0.0002, 'epoch': 0.34}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:06,  1.32s/it][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:05<00:07,  1.94s/it][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:08<00:07,  2.48s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:11<00:05,  2.56s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:14<00:02,  2.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:16<00:00,  2.40s/it][A                                               
                                             [A 11%|â–ˆ         | 10/90 [05:14<29:43, 22.29s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:16<00:00,  2.40s/it][A
                                             [A 12%|â–ˆâ–        | 11/90 [05:35<37:09, 28.23s/it] 13%|â–ˆâ–Ž        | 12/90 [05:56<33:49, 26.02s/it] 14%|â–ˆâ–        | 13/90 [06:17<31:27, 24.51s/it] 16%|â–ˆâ–Œ        | 14/90 [06:38<29:34, 23.35s/it] 17%|â–ˆâ–‹        | 15/90 [06:59<28:16, 22.61s/it] 18%|â–ˆâ–Š        | 16/90 [07:20<27:17, 22.13s/it] 19%|â–ˆâ–‰        | 17/90 [07:41<26:29, 21.77s/it] 20%|â–ˆâ–ˆ        | 18/90 [08:01<25:43, 21.44s/it] 21%|â–ˆâ–ˆ        | 19/90 [08:22<25:10, 21.27s/it] 22%|â–ˆâ–ˆâ–       | 20/90 [08:43<24:35, 21.08s/it]                                                22%|â–ˆâ–ˆâ–       | 20/90 [08:43<24:35, 21.08s/it]{'eval_loss': 0.744996190071106, 'eval_runtime': 20.6438, 'eval_samples_per_second': 2.422, 'eval_steps_per_second': 0.339, 'epoch': 0.34}
{'loss': 0.4036, 'grad_norm': 0.8979178071022034, 'learning_rate': 0.00019257239692688907, 'epoch': 0.67}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:07,  1.45s/it][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:05<00:08,  2.05s/it][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:08<00:07,  2.43s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:11<00:05,  2.62s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:14<00:02,  2.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:15<00:00,  2.14s/it][A                                               
                                             [A 22%|â–ˆâ–ˆâ–       | 20/90 [09:01<24:35, 21.08s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:15<00:00,  2.14s/it][A
                                             [A 23%|â–ˆâ–ˆâ–Ž       | 21/90 [09:22<30:30, 26.53s/it] 24%|â–ˆâ–ˆâ–       | 22/90 [09:43<28:05, 24.78s/it] 26%|â–ˆâ–ˆâ–Œ       | 23/90 [10:04<26:21, 23.61s/it] 27%|â–ˆâ–ˆâ–‹       | 24/90 [10:25<25:04, 22.80s/it] 28%|â–ˆâ–ˆâ–Š       | 25/90 [10:45<24:01, 22.18s/it] 29%|â–ˆâ–ˆâ–‰       | 26/90 [11:07<23:25, 21.96s/it] 30%|â–ˆâ–ˆâ–ˆ       | 27/90 [11:27<22:39, 21.58s/it] 31%|â–ˆâ–ˆâ–ˆ       | 28/90 [11:48<22:05, 21.38s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 29/90 [12:09<21:35, 21.24s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 30/90 [12:48<26:24, 26.41s/it]                                                33%|â–ˆâ–ˆâ–ˆâ–Ž      | 30/90 [12:48<26:24, 26.41s/it]{'eval_loss': 0.13052581250667572, 'eval_runtime': 18.276, 'eval_samples_per_second': 2.736, 'eval_steps_per_second': 0.383, 'epoch': 0.67}
{'loss': 0.0889, 'grad_norm': 0.4050913453102112, 'learning_rate': 0.00017139297345578994, 'epoch': 1.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:07,  1.44s/it][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:05<00:08,  2.05s/it][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:08<00:07,  2.42s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:11<00:05,  2.59s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:14<00:02,  2.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:15<00:00,  2.12s/it][A                                               
                                             [A 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 30/90 [13:06<26:24, 26.41s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:15<00:00,  2.12s/it][A
                                             [A 34%|â–ˆâ–ˆâ–ˆâ–      | 31/90 [13:38<33:07, 33.69s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 32/90 [13:51<26:29, 27.41s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 33/90 [14:04<21:46, 22.91s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 34/90 [14:18<18:52, 20.23s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 35/90 [14:30<16:28, 17.98s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 36/90 [14:43<14:47, 16.43s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 37/90 [14:56<13:34, 15.37s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 38/90 [15:09<12:42, 14.66s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 39/90 [15:22<11:58, 14.09s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 40/90 [15:34<11:20, 13.62s/it]                                                44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 40/90 [15:34<11:20, 13.62s/it]{'eval_loss': 0.0716814324259758, 'eval_runtime': 18.2091, 'eval_samples_per_second': 2.746, 'eval_steps_per_second': 0.384, 'epoch': 1.0}

==================================================
Testing model performance...
==================================================
Generated solution:
(op_1 obj_04 obj_05)
(op_2 obj_04)
(op_1 obj_02 obj_06)
(op_4 obj_02)
(op_3 obj_02 obj_03)
(op_4 obj_06)
(op_3 obj_06 obj_01)
(op_1 obj_01 obj_05)
(op_2 obj_01)
(op_1 obj_06 obj_01)
(op_1 obj_04 obj_05)
(op_4 obj_04)
(op_3 obj_04 obj_02)
(op_4 obj_06)
(op_3 obj_06 obj_01)
(op_4 obj_04)
(op_3 obj_04 obj_02)
==================================================
{'loss': 0.0611, 'grad_norm': 0.2378343641757965, 'learning_rate': 0.0001396079766039157, 'epoch': 1.34}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:07,  1.42s/it][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:05<00:08,  2.01s/it][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:08<00:07,  2.36s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:11<00:05,  2.61s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:14<00:02,  2.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:15<00:00,  2.14s/it][A                                               
                                             [A 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 40/90 [15:53<11:20, 13.62s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:15<00:00,  2.14s/it][A
                                             [A 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 41/90 [16:05<15:19, 18.76s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 42/90 [16:18<13:43, 17.15s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 43/90 [16:31<12:23, 15.82s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 44/90 [16:43<11:15, 14.69s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 45/90 [16:55<10:27, 13.95s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 46/90 [17:08<09:51, 13.45s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 47/90 [17:17<08:48, 12.29s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 48/90 [17:27<08:05, 11.56s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 49/90 [17:33<06:38,  9.72s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 50/90 [17:39<05:49,  8.74s/it]                                                56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 50/90 [17:39<05:49,  8.74s/it]{'eval_loss': 0.05769187584519386, 'eval_runtime': 18.2208, 'eval_samples_per_second': 2.744, 'eval_steps_per_second': 0.384, 'epoch': 1.34}
{'loss': 0.0518, 'grad_norm': 0.16808609664440155, 'learning_rate': 0.00010193913317718244, 'epoch': 1.67}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:03,  1.46it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:02<00:03,  1.04it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  1.14s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  1.27s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:07<00:01,  1.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  1.03s/it][A                                               
                                             [A 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 50/90 [17:48<05:49,  8.74s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  1.03s/it][A
                                             [A 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 51/90 [17:53<06:45, 10.40s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 52/90 [17:59<05:38,  8.91s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 53/90 [18:04<04:51,  7.87s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 54/90 [18:10<04:16,  7.13s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 55/90 [18:15<03:51,  6.62s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 56/90 [18:20<03:32,  6.25s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 57/90 [18:26<03:18,  6.01s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 58/90 [18:32<03:16,  6.13s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 59/90 [18:38<03:03,  5.91s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 60/90 [18:42<02:40,  5.34s/it]                                                67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 60/90 [18:42<02:40,  5.34s/it]{'eval_loss': 0.05287550017237663, 'eval_runtime': 8.732, 'eval_samples_per_second': 5.726, 'eval_steps_per_second': 0.802, 'epoch': 1.67}
{'loss': 0.0484, 'grad_norm': 0.17046743631362915, 'learning_rate': 6.398222751952899e-05, 'epoch': 2.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:03,  1.45it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:02<00:03,  1.03it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  1.15s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  1.27s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:07<00:01,  1.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  1.03s/it][A                                               
                                             [A 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 60/90 [18:50<02:40,  5.34s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  1.03s/it][A
                                             [A 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 61/90 [19:11<06:02, 12.51s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 62/90 [19:16<04:51, 10.40s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 63/90 [19:22<04:01,  8.93s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 64/90 [19:27<03:25,  7.90s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 65/90 [19:33<02:59,  7.20s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 66/90 [19:40<02:48,  7.03s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 67/90 [19:45<02:32,  6.63s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 68/90 [19:51<02:18,  6.31s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 69/90 [19:56<02:07,  6.08s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 70/90 [20:02<01:58,  5.91s/it]                                                78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 70/90 [20:02<01:58,  5.91s/it]{'eval_loss': 0.049706120043992996, 'eval_runtime': 8.7327, 'eval_samples_per_second': 5.726, 'eval_steps_per_second': 0.802, 'epoch': 2.0}

==================================================
Testing model performance...
==================================================
Generated solution:
(op_1 obj_01 obj_05)
(op_4 obj_06)
(op_3 obj_06 obj_01)
(op_1 obj_04 obj_05)
(op_4 obj_02)
(op_3 obj_02 obj_06)
(op_4 obj_04)
(op_3 obj_04 obj_02)
(op_1 obj_06 obj_01)
(op_2 obj_06)
(op_4 obj_01)
(op_3 obj_01 obj_05)
(op_1 obj_04 obj_02)
(op_3 obj_04 obj_03)
(op_1 obj_01 obj_05)
(op_3 obj_01 obj_04)
(op_4 obj_02)
(op_3 obj_02 obj_03)
(op_4 obj_06)
(op_3 obj_06 obj_01)
(op_1 obj_04 obj_03)
(op_3 obj_04 obj_02)
(op_1 obj_06 obj_01)
(op_2 obj_06)
(op_1 obj_01 obj_04)
(op_3 obj_01 obj_05)
==================================================
{'loss': 0.0463, 'grad_norm': 0.13754820823669434, 'learning_rate': 3.137583621312665e-05, 'epoch': 2.34}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:03,  1.46it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:02<00:03,  1.04it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  1.14s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  1.27s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:07<00:01,  1.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  1.03s/it][A                                               
                                             [A 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 70/90 [20:11<01:58,  5.91s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  1.03s/it][A
                                             [A 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 71/90 [20:16<02:39,  8.41s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 72/90 [20:22<02:15,  7.53s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 73/90 [20:27<01:56,  6.88s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 74/90 [20:33<01:47,  6.70s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 75/90 [20:39<01:34,  6.31s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 76/90 [20:44<01:24,  6.04s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 77/90 [20:50<01:16,  5.87s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 78/90 [20:55<01:08,  5.74s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 79/90 [21:00<01:02,  5.65s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 80/90 [21:06<00:55,  5.58s/it]                                                89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 80/90 [21:06<00:55,  5.58s/it]{'eval_loss': 0.04829205200076103, 'eval_runtime': 8.7312, 'eval_samples_per_second': 5.727, 'eval_steps_per_second': 0.802, 'epoch': 2.34}
{'loss': 0.0442, 'grad_norm': 0.21768216788768768, 'learning_rate': 8.963705903385345e-06, 'epoch': 2.67}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:03,  1.46it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:02<00:03,  1.04it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  1.14s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  1.28s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:07<00:01,  1.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  1.03s/it][A                                               
                                             [A 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 80/90 [21:15<00:55,  5.58s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  1.03s/it][A
                                             [A 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 81/90 [21:20<01:13,  8.19s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 82/90 [21:26<00:58,  7.36s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 83/90 [21:32<00:49,  7.11s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 84/90 [21:38<00:39,  6.60s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 85/90 [21:43<00:31,  6.25s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 86/90 [21:48<00:23,  5.99s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 87/90 [21:54<00:17,  5.83s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 88/90 [21:59<00:11,  5.71s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 89/90 [22:05<00:05,  5.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [22:09<00:00,  5.13s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [22:09<00:00,  5.13s/it]{'eval_loss': 0.047730717808008194, 'eval_runtime': 8.7363, 'eval_samples_per_second': 5.723, 'eval_steps_per_second': 0.801, 'epoch': 2.67}
{'loss': 0.0437, 'grad_norm': 0.15731362998485565, 'learning_rate': 7.520474957699586e-08, 'epoch': 3.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:03,  1.46it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:02<00:03,  1.04it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  1.14s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  1.28s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:07<00:01,  1.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  1.04s/it][A                                               
                                             [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [22:17<00:00,  5.13s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  1.04s/it][A
                                             [A                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [22:28<00:00,  5.13s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [22:28<00:00, 14.98s/it]
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:               eval/loss â–ˆâ–‚â–â–â–â–â–â–â–
wandb:            eval/runtime â–ˆâ–‡â–‡â–‡â–â–â–â–â–
wandb: eval/samples_per_second â–â–‚â–‚â–‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   eval/steps_per_second â–â–‚â–‚â–‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:             train/epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:       train/global_step â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         train/grad_norm â–ˆâ–„â–‚â–â–â–â–â–â–
wandb:     train/learning_rate â–ˆâ–ˆâ–‡â–†â–…â–ƒâ–‚â–â–
wandb:              train/loss â–ˆâ–‚â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               eval/loss 0.04781
wandb:            eval/runtime 8.7416
wandb: eval/samples_per_second 5.72
wandb:   eval/steps_per_second 0.801
wandb:              total_flos 2.8890204183985536e+17
wandb:             train/epoch 3
wandb:       train/global_step 90
wandb:         train/grad_norm 0.15731
wandb:     train/learning_rate 0.0
wandb:              train/loss 0.0437
wandb:                      +4 ...
wandb: 
wandb: ðŸš€ View run pddl_sft_blocksworld_pddl3_symbolized at: https://wandb.ai/fjl2401-university-of-notre-dame/pddl-finetune/runs/wkh6fdtg
wandb: â­ï¸ View project at: https://wandb.ai/fjl2401-university-of-notre-dame/pddl-finetune
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251212_150900-wkh6fdtg/logs
{'eval_loss': 0.04781312867999077, 'eval_runtime': 8.7416, 'eval_samples_per_second': 5.72, 'eval_steps_per_second': 0.801, 'epoch': 3.0}

==================================================
Testing model performance...
==================================================
Generated solution:
(op_1 obj_04 obj_05)
(op_2 obj_04)
(op_1 obj_02 obj_06)
(op_3 obj_02 obj_03)
(op_4 obj_04)
(op_3 obj_04 obj_02)
(op_4 obj_06)
(op_3 obj_06 obj_01)
(op_4 obj_01)
(op_3 obj_01 obj_05)
(op_1 obj_06 obj_01)
(op_2 obj_06)
(op_4 obj_04)
(op_3 obj_04 obj_02)
(op_1 obj_02 obj_03)
(op_2 obj_02)
(op_4 obj_06)
(op_3 obj_06 obj_01)
==================================================
{'train_runtime': 1348.1247, 'train_samples_per_second': 2.114, 'train_steps_per_second': 0.067, 'train_loss': 0.4496932506561279, 'epoch': 3.0}

Saving model to /jfan5/sft_models/gpt_oss_20b/blocksworld_pddl3_symbolized ...
Training completed!

==========================================
Fine-tuning completed!
==========================================
Model saved to: /jfan5/sft_models/gpt_oss_20b/blocksworld_pddl3_symbolized

