
CondaError: Run 'conda init' before 'conda activate'

==========================================
DPO Training for Mistral-7B
==========================================
Base model: /jfan5/sft_models/mistral_variant-blocksworld
Dataset: /home/ubuntu/Safety-gen/data/dpo/new_four/blocksworld_pddl3_dpo.jsonl
Output: /jfan5/dpo_models/mistral_7b-1206

Training parameters:
  Epochs: 2
  Batch size: 8
  Gradient accumulation: 4
  Learning rate: 5e-6
  Beta: 0.02
==========================================

[torchao|WARNING]Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Unsloth 2025.10.12 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2025.10.12: Fast Mistral patching. Transformers: 4.56.2.
   \\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.109 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
unsloth/mistral-7b-instruct-v0.3-bnb-4bit does not have a padding token! Will use pad_token = [control_768].
Extracting prompt in train dataset (num_proc=32):   0%|          | 0/2745 [00:00<?, ? examples/s]Extracting prompt in train dataset (num_proc=32):   3%|â–Ž         | 86/2745 [00:00<00:21, 123.21 examples/s]Extracting prompt in train dataset (num_proc=32):   6%|â–‹         | 172/2745 [00:00<00:10, 247.68 examples/s]Extracting prompt in train dataset (num_proc=32):  13%|â–ˆâ–Ž        | 344/2745 [00:00<00:04, 511.89 examples/s]Extracting prompt in train dataset (num_proc=32):  19%|â–ˆâ–‰        | 516/2745 [00:01<00:03, 704.40 examples/s]Extracting prompt in train dataset (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 688/2745 [00:01<00:02, 877.44 examples/s]Extracting prompt in train dataset (num_proc=32):  34%|â–ˆâ–ˆâ–ˆâ–      | 946/2745 [00:01<00:01, 1023.75 examples/s]Extracting prompt in train dataset (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1118/2745 [00:01<00:01, 1131.78 examples/s]Extracting prompt in train dataset (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1290/2745 [00:01<00:01, 1211.18 examples/s]Extracting prompt in train dataset (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1548/2745 [00:01<00:00, 1306.83 examples/s]Extracting prompt in train dataset (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1720/2745 [00:01<00:00, 1342.17 examples/s]Extracting prompt in train dataset (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1892/2745 [00:02<00:00, 1380.13 examples/s]Extracting prompt in train dataset (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2064/2745 [00:02<00:00, 1376.49 examples/s]Extracting prompt in train dataset (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2235/2745 [00:02<00:00, 1389.93 examples/s]Extracting prompt in train dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2405/2745 [00:02<00:00, 1400.52 examples/s]Extracting prompt in train dataset (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2575/2745 [00:02<00:00, 1409.46 examples/s]Extracting prompt in train dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2745/2745 [00:02<00:00, 958.08 examples/s] 
Applying chat template to train dataset (num_proc=32):   0%|          | 0/2745 [00:00<?, ? examples/s]Applying chat template to train dataset (num_proc=32):   3%|â–Ž         | 86/2745 [00:00<00:29, 89.49 examples/s]Applying chat template to train dataset (num_proc=32):   9%|â–‰         | 258/2745 [00:01<00:08, 290.07 examples/s]Applying chat template to train dataset (num_proc=32):  13%|â–ˆâ–Ž        | 344/2745 [00:01<00:06, 371.13 examples/s]Applying chat template to train dataset (num_proc=32):  19%|â–ˆâ–‰        | 516/2745 [00:01<00:03, 576.06 examples/s]Applying chat template to train dataset (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 688/2745 [00:01<00:02, 792.76 examples/s]Applying chat template to train dataset (num_proc=32):  34%|â–ˆâ–ˆâ–ˆâ–      | 946/2745 [00:01<00:01, 973.04 examples/s]Applying chat template to train dataset (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1290/2745 [00:01<00:01, 1251.27 examples/s]Applying chat template to train dataset (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1548/2745 [00:01<00:00, 1396.60 examples/s]Applying chat template to train dataset (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1892/2745 [00:02<00:00, 1598.14 examples/s]Applying chat template to train dataset (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2150/2745 [00:02<00:00, 1458.98 examples/s]Applying chat template to train dataset (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2405/2745 [00:02<00:00, 1632.48 examples/s]Applying chat template to train dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2745/2745 [00:02<00:00, 975.51 examples/s] 
Tokenizing train dataset (num_proc=32):   0%|          | 0/2745 [00:00<?, ? examples/s]Tokenizing train dataset (num_proc=32):   2%|â–         | 50/2745 [00:00<00:53, 50.12 examples/s]Tokenizing train dataset (num_proc=32):   6%|â–Œ         | 171/2745 [00:01<00:14, 176.11 examples/s]Tokenizing train dataset (num_proc=32):  13%|â–ˆâ–Ž        | 344/2745 [00:01<00:06, 357.22 examples/s]Tokenizing train dataset (num_proc=32):  16%|â–ˆâ–Œ        | 430/2745 [00:01<00:05, 431.69 examples/s]Tokenizing train dataset (num_proc=32):  21%|â–ˆâ–ˆ        | 566/2745 [00:01<00:03, 595.54 examples/s]Tokenizing train dataset (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 700/2745 [00:01<00:03, 628.13 examples/s]Tokenizing train dataset (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 874/2745 [00:01<00:02, 811.03 examples/s]Tokenizing train dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1020/2745 [00:02<00:02, 805.72 examples/s]Tokenizing train dataset (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1168/2745 [00:02<00:01, 937.15 examples/s]Tokenizing train dataset (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1290/2745 [00:02<00:01, 829.36 examples/s]Tokenizing train dataset (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1421/2745 [00:02<00:01, 791.43 examples/s]Tokenizing train dataset (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1608/2745 [00:02<00:01, 854.73 examples/s]Tokenizing train dataset (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1923/2745 [00:02<00:00, 1121.31 examples/s]Tokenizing train dataset (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2235/2745 [00:03<00:00, 1468.85 examples/s]Tokenizing train dataset (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2441/2745 [00:03<00:00, 1560.19 examples/s]Tokenizing train dataset (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2723/2745 [00:03<00:00, 1803.88 examples/s]Tokenizing train dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2745/2745 [00:03<00:00, 783.58 examples/s] 
Extracting prompt in eval dataset (num_proc=32):   0%|          | 0/305 [00:00<?, ? examples/s]Extracting prompt in eval dataset (num_proc=32):   3%|â–Ž         | 10/305 [00:00<00:21, 13.56 examples/s]Extracting prompt in eval dataset (num_proc=32):  10%|â–‰         | 30/305 [00:00<00:06, 41.98 examples/s]Extracting prompt in eval dataset (num_proc=32):  16%|â–ˆâ–‹        | 50/305 [00:00<00:03, 67.68 examples/s]Extracting prompt in eval dataset (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 70/305 [00:01<00:02, 89.41 examples/s]Extracting prompt in eval dataset (num_proc=32):  30%|â–ˆâ–ˆâ–‰       | 90/305 [00:01<00:02, 106.66 examples/s]Extracting prompt in eval dataset (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 110/305 [00:01<00:01, 120.10 examples/s]Extracting prompt in eval dataset (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 130/305 [00:01<00:01, 130.31 examples/s]Extracting prompt in eval dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 150/305 [00:01<00:01, 137.66 examples/s]Extracting prompt in eval dataset (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 170/305 [00:01<00:00, 144.73 examples/s]Extracting prompt in eval dataset (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 188/305 [00:01<00:00, 143.30 examples/s]Extracting prompt in eval dataset (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 206/305 [00:02<00:00, 142.33 examples/s]Extracting prompt in eval dataset (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 224/305 [00:02<00:00, 142.44 examples/s]Extracting prompt in eval dataset (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 242/305 [00:02<00:00, 142.84 examples/s]Extracting prompt in eval dataset (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 260/305 [00:02<00:00, 142.79 examples/s]Extracting prompt in eval dataset (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 278/305 [00:02<00:00, 142.63 examples/s]Extracting prompt in eval dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 296/305 [00:02<00:00, 142.48 examples/s]Extracting prompt in eval dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 305/305 [00:02<00:00, 104.05 examples/s]
Applying chat template to eval dataset (num_proc=32):   0%|          | 0/305 [00:00<?, ? examples/s]Applying chat template to eval dataset (num_proc=32):   3%|â–Ž         | 10/305 [00:00<00:23, 12.37 examples/s]Applying chat template to eval dataset (num_proc=32):  16%|â–ˆâ–‹        | 50/305 [00:00<00:03, 69.74 examples/s]Applying chat template to eval dataset (num_proc=32):  30%|â–ˆâ–ˆâ–‰       | 90/305 [00:01<00:01, 124.81 examples/s]Applying chat template to eval dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 140/305 [00:01<00:00, 188.96 examples/s]Applying chat template to eval dataset (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 179/305 [00:01<00:00, 230.79 examples/s]Applying chat template to eval dataset (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 215/305 [00:01<00:00, 256.90 examples/s]Applying chat template to eval dataset (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 251/305 [00:01<00:00, 278.06 examples/s]Applying chat template to eval dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 296/305 [00:01<00:00, 308.16 examples/s]Applying chat template to eval dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 305/305 [00:01<00:00, 167.04 examples/s]
Tokenizing eval dataset (num_proc=32):   0%|          | 0/305 [00:00<?, ? examples/s]Tokenizing eval dataset (num_proc=32):   3%|â–Ž         | 10/305 [00:00<00:23, 12.34 examples/s]Tokenizing eval dataset (num_proc=32):  13%|â–ˆâ–Ž        | 40/305 [00:00<00:04, 54.33 examples/s]Tokenizing eval dataset (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 80/305 [00:01<00:01, 112.90 examples/s]Tokenizing eval dataset (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 120/305 [00:01<00:01, 159.08 examples/s]Tokenizing eval dataset (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 160/305 [00:01<00:00, 204.84 examples/s]Tokenizing eval dataset (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 197/305 [00:01<00:00, 232.11 examples/s]Tokenizing eval dataset (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 233/305 [00:01<00:00, 235.94 examples/s]Tokenizing eval dataset (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 278/305 [00:01<00:00, 280.31 examples/s]Tokenizing eval dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 305/305 [00:01<00:00, 154.20 examples/s]
wandb: Currently logged in as: fjl2401 (fjl2401-university-of-notre-dame) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run dseasmhi
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /home/ubuntu/Safety-gen/wandb/run-20251206_070243-dseasmhi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dpo_mistral_7b-blocksworld-1206
wandb: â­ï¸ View project at https://wandb.ai/fjl2401-university-of-notre-dame/pddl-dpo-mistral7b
wandb: ðŸš€ View run at https://wandb.ai/fjl2401-university-of-notre-dame/pddl-dpo-mistral7b/runs/dseasmhi
wandb: Detected [huggingface_hub.inference] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 2,745 | Num Epochs = 2 | Total steps = 172
O^O/ \_/ \    Batch size per device = 8 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32
 "-____-"     Trainable parameters = 83,886,080 of 7,331,909,632 (1.14% trained)
eval steps:  60
  0%|          | 0/172 [00:00<?, ?it/s]  1%|          | 1/172 [00:19<54:47, 19.23s/it]  1%|          | 2/172 [00:32<44:05, 15.56s/it]  2%|â–         | 3/172 [00:46<42:37, 15.13s/it]  2%|â–         | 4/172 [01:01<41:58, 14.99s/it]  3%|â–Ž         | 5/172 [01:16<41:06, 14.77s/it]  3%|â–Ž         | 6/172 [01:29<39:23, 14.24s/it]  4%|â–         | 7/172 [01:42<38:05, 13.85s/it]  5%|â–         | 8/172 [01:55<37:33, 13.74s/it]  5%|â–Œ         | 9/172 [02:10<37:58, 13.98s/it]  6%|â–Œ         | 10/172 [02:23<36:55, 13.67s/it]                                                  6%|â–Œ         | 10/172 [02:23<36:55, 13.67s/it]  6%|â–‹         | 11/172 [02:35<35:40, 13.29s/it]  7%|â–‹         | 12/172 [02:49<35:57, 13.49s/it]  8%|â–Š         | 13/172 [03:01<34:16, 12.93s/it]  8%|â–Š         | 14/172 [03:14<34:14, 13.00s/it]  9%|â–Š         | 15/172 [03:28<35:13, 13.46s/it]  9%|â–‰         | 16/172 [03:43<35:51, 13.79s/it] 10%|â–‰         | 17/172 [03:58<36:12, 14.02s/it] 10%|â–ˆ         | 18/172 [04:11<35:28, 13.82s/it] 11%|â–ˆ         | 19/172 [04:22<33:01, 12.95s/it] 12%|â–ˆâ–        | 20/172 [04:36<33:22, 13.17s/it]                                                 12%|â–ˆâ–        | 20/172 [04:36<33:22, 13.17s/it] 12%|â–ˆâ–        | 21/172 [04:49<33:02, 13.13s/it] 13%|â–ˆâ–Ž        | 22/172 [05:00<31:41, 12.67s/it] 13%|â–ˆâ–Ž        | 23/172 [05:12<30:55, 12.45s/it] 14%|â–ˆâ–        | 24/172 [05:25<31:17, 12.68s/it] 15%|â–ˆâ–        | 25/172 [05:38<31:22, 12.80s/it] 15%|â–ˆâ–Œ        | 26/172 [05:52<31:26, 12.92s/it] 16%|â–ˆâ–Œ        | 27/172 [06:06<32:12, 13.33s/it] 16%|â–ˆâ–‹        | 28/172 [06:21<33:03, 13.77s/it] 17%|â–ˆâ–‹        | 29/172 [06:35<33:29, 14.05s/it] 17%|â–ˆâ–‹        | 30/172 [06:50<33:34, 14.19s/it]                                                 17%|â–ˆâ–‹        | 30/172 [06:50<33:34, 14.19s/it] 18%|â–ˆâ–Š        | 31/172 [07:03<32:22, 13.78s/it] 19%|â–ˆâ–Š        | 32/172 [07:17<32:41, 14.01s/it] 19%|â–ˆâ–‰        | 33/172 [07:31<31:57, 13.80s/it] 20%|â–ˆâ–‰        | 34/172 [07:45<32:17, 14.04s/it] 20%|â–ˆâ–ˆ        | 35/172 [07:56<29:37, 12.97s/it] 21%|â–ˆâ–ˆ        | 36/172 [08:08<28:50, 12.73s/it] 22%|â–ˆâ–ˆâ–       | 37/172 [08:21<28:59, 12.89s/it] 22%|â–ˆâ–ˆâ–       | 38/172 [08:35<29:08, 13.05s/it] 23%|â–ˆâ–ˆâ–Ž       | 39/172 [08:44<26:40, 12.04s/it] 23%|â–ˆâ–ˆâ–Ž       | 40/172 [08:57<27:14, 12.38s/it]                                                 23%|â–ˆâ–ˆâ–Ž       | 40/172 [08:57<27:14, 12.38s/it] 24%|â–ˆâ–ˆâ–       | 41/172 [09:10<27:29, 12.59s/it] 24%|â–ˆâ–ˆâ–       | 42/172 [09:22<26:50, 12.39s/it] 25%|â–ˆâ–ˆâ–Œ       | 43/172 [09:35<27:04, 12.59s/it] 26%|â–ˆâ–ˆâ–Œ       | 44/172 [09:49<27:42, 12.99s/it] 26%|â–ˆâ–ˆâ–Œ       | 45/172 [10:04<28:31, 13.48s/it] 27%|â–ˆâ–ˆâ–‹       | 46/172 [10:17<27:57, 13.32s/it] 27%|â–ˆâ–ˆâ–‹       | 47/172 [10:30<27:44, 13.32s/it] 28%|â–ˆâ–ˆâ–Š       | 48/172 [10:42<26:28, 12.81s/it] 28%|â–ˆâ–ˆâ–Š       | 49/172 [10:54<25:44, 12.55s/it] 29%|â–ˆâ–ˆâ–‰       | 50/172 [11:07<25:47, 12.68s/it]                                                 29%|â–ˆâ–ˆâ–‰       | 50/172 [11:07<25:47, 12.68s/it] 30%|â–ˆâ–ˆâ–‰       | 51/172 [11:19<25:23, 12.59s/it] 30%|â–ˆâ–ˆâ–ˆ       | 52/172 [11:33<25:57, 12.98s/it] 31%|â–ˆâ–ˆâ–ˆ       | 53/172 [11:46<25:54, 13.06s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 54/172 [11:59<25:21, 12.89s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 55/172 [12:13<26:06, 13.39s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 56/172 [12:24<24:29, 12.67s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 57/172 [12:37<24:31, 12.80s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 58/172 [12:51<24:32, 12.92s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 59/172 [13:04<24:36, 13.07s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 60/172 [13:17<24:17, 13.02s/it]                                                 35%|â–ˆâ–ˆâ–ˆâ–      | 60/172 [13:17<24:17, 13.02s/it]Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.4133, 'grad_norm': 3.852126359939575, 'learning_rate': 2.5e-06, 'rewards/chosen': -0.4159819483757019, 'rewards/rejected': -1.7534888982772827, 'rewards/accuracies': 0.8687499761581421, 'rewards/margins': 1.3375072479248047, 'logps/chosen': -87.5261001586914, 'logps/rejected': -192.2236785888672, 'logits/chosen': -2.6288580894470215, 'logits/rejected': -2.5425894260406494, 'epoch': 0.12}
{'loss': 0.2113, 'grad_norm': 0.9918320775032043, 'learning_rate': 4.967532467532468e-06, 'rewards/chosen': -0.18337871134281158, 'rewards/rejected': -2.9139344692230225, 'rewards/accuracies': 0.956250011920929, 'rewards/margins': 2.730556011199951, 'logps/chosen': -72.33013916015625, 'logps/rejected': -250.51284790039062, 'logits/chosen': -2.7047224044799805, 'logits/rejected': -2.6236395835876465, 'epoch': 0.23}
{'loss': 0.0887, 'grad_norm': 0.4614536166191101, 'learning_rate': 4.642857142857144e-06, 'rewards/chosen': -0.12427932024002075, 'rewards/rejected': -4.544276237487793, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 4.419996738433838, 'logps/chosen': -69.10557556152344, 'logps/rejected': -331.6340026855469, 'logits/chosen': -2.8427019119262695, 'logits/rejected': -2.750291347503662, 'epoch': 0.35}
{'loss': 0.0738, 'grad_norm': 2.3694443702697754, 'learning_rate': 4.3181818181818185e-06, 'rewards/chosen': -0.23112574219703674, 'rewards/rejected': -5.684042453765869, 'rewards/accuracies': 0.9937499761581421, 'rewards/margins': 5.452917575836182, 'logps/chosen': -71.08235931396484, 'logps/rejected': -387.7677917480469, 'logits/chosen': -2.9938671588897705, 'logits/rejected': -2.9020228385925293, 'epoch': 0.47}
{'loss': 0.0499, 'grad_norm': 1.2713416814804077, 'learning_rate': 3.993506493506494e-06, 'rewards/chosen': -0.3950876295566559, 'rewards/rejected': -6.229940414428711, 'rewards/accuracies': 0.9937499761581421, 'rewards/margins': 5.83485221862793, 'logps/chosen': -81.76793670654297, 'logps/rejected': -414.459716796875, 'logits/chosen': -3.12083101272583, 'logits/rejected': -3.014962673187256, 'epoch': 0.58}
{'loss': 0.0432, 'grad_norm': 0.14713385701179504, 'learning_rate': 3.6688311688311688e-06, 'rewards/chosen': -0.276132732629776, 'rewards/rejected': -6.357146739959717, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 6.081014156341553, 'logps/chosen': -73.4156265258789, 'logps/rejected': -422.3128967285156, 'logits/chosen': -3.2339751720428467, 'logits/rejected': -3.129711866378784, 'epoch': 0.7}

  0%|          | 0/39 [00:00<?, ?it/s][A
  5%|â–Œ         | 2/39 [00:01<00:20,  1.78it/s][A
  8%|â–Š         | 3/39 [00:03<00:42,  1.18s/it][A
 10%|â–ˆ         | 4/39 [00:05<00:51,  1.48s/it][A
 13%|â–ˆâ–Ž        | 5/39 [00:07<00:56,  1.65s/it][A
 15%|â–ˆâ–Œ        | 6/39 [00:09<00:58,  1.76s/it][A
 18%|â–ˆâ–Š        | 7/39 [00:10<00:50,  1.57s/it][A
 21%|â–ˆâ–ˆ        | 8/39 [00:11<00:44,  1.44s/it][A
 23%|â–ˆâ–ˆâ–Ž       | 9/39 [00:12<00:40,  1.35s/it][A
 26%|â–ˆâ–ˆâ–Œ       | 10/39 [00:14<00:44,  1.54s/it][A
 28%|â–ˆâ–ˆâ–Š       | 11/39 [00:16<00:47,  1.68s/it][A
 31%|â–ˆâ–ˆâ–ˆ       | 12/39 [00:18<00:47,  1.77s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [00:19<00:41,  1.60s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [00:21<00:42,  1.71s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [00:23<00:43,  1.80s/it][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [00:24<00:36,  1.57s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [00:25<00:31,  1.43s/it][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [00:27<00:33,  1.60s/it][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [00:28<00:28,  1.44s/it][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [00:30<00:25,  1.36s/it][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [00:32<00:27,  1.55s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [00:33<00:24,  1.43s/it][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [00:35<00:25,  1.61s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [00:36<00:23,  1.59s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [00:38<00:24,  1.72s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [00:39<00:20,  1.54s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [00:41<00:20,  1.69s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [00:43<00:16,  1.52s/it][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [00:45<00:16,  1.66s/it][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [00:47<00:15,  1.75s/it][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [00:48<00:12,  1.58s/it][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [00:50<00:11,  1.68s/it][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [00:52<00:10,  1.77s/it][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [00:53<00:07,  1.58s/it][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [00:54<00:06,  1.56s/it][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [00:56<00:05,  1.68s/it][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [00:57<00:03,  1.56s/it][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [00:59<00:01,  1.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:00<00:00,  1.26s/it][A                                                
                                               [A 35%|â–ˆâ–ˆâ–ˆâ–      | 60/172 [14:19<24:17, 13.02s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:00<00:00,  1.26s/it][A
                                               [A 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 61/172 [14:35<1:00:02, 32.45s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 62/172 [14:48<49:10, 26.82s/it]   37%|â–ˆâ–ˆâ–ˆâ–‹      | 63/172 [15:01<41:09, 22.66s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 64/172 [15:13<34:51, 19.37s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 65/172 [15:27<31:30, 17.67s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 66/172 [15:41<29:30, 16.70s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 67/172 [15:55<27:31, 15.73s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 68/172 [16:08<26:10, 15.10s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 69/172 [16:23<25:33, 14.89s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 70/172 [16:37<25:06, 14.77s/it]                                                 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 70/172 [16:37<25:06, 14.77s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 71/172 [16:49<23:16, 13.82s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 72/172 [17:04<23:34, 14.15s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 73/172 [17:17<22:52, 13.86s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 74/172 [17:29<21:34, 13.21s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 75/172 [17:42<21:23, 13.23s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 76/172 [17:55<21:04, 13.17s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 77/172 [18:07<20:12, 12.76s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 78/172 [18:20<20:12, 12.90s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 79/172 [18:32<19:37, 12.66s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 80/172 [18:47<20:16, 13.22s/it]                                                 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 80/172 [18:47<20:16, 13.22s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 81/172 [19:01<20:41, 13.64s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 82/172 [19:14<20:14, 13.49s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 83/172 [19:27<19:43, 13.30s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 84/172 [19:39<18:47, 12.81s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 85/172 [19:52<18:44, 12.93s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 86/172 [20:02<17:13, 12.01s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 87/172 [20:13<16:25, 11.59s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 88/172 [20:24<16:19, 11.66s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 89/172 [20:36<16:19, 11.80s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 90/172 [20:49<16:13, 11.87s/it]                                                 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 90/172 [20:49<16:13, 11.87s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 91/172 [21:00<15:53, 11.77s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 92/172 [21:09<14:26, 10.83s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 93/172 [21:23<15:46, 11.98s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 94/172 [21:35<15:28, 11.90s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 95/172 [21:47<15:07, 11.79s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 96/172 [22:00<15:36, 12.33s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 97/172 [22:14<15:49, 12.66s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 98/172 [22:26<15:31, 12.59s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 99/172 [22:38<14:55, 12.27s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 100/172 [22:49<14:26, 12.04s/it]                                                  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 100/172 [22:49<14:26, 12.04s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 101/172 [23:02<14:35, 12.33s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 102/172 [23:15<14:42, 12.60s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 103/172 [23:28<14:35, 12.69s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 104/172 [23:41<14:17, 12.61s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 105/172 [23:51<13:14, 11.86s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 106/172 [24:03<13:02, 11.85s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 107/172 [24:16<13:19, 12.29s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 108/172 [24:29<13:25, 12.58s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 109/172 [24:42<13:25, 12.79s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 110/172 [24:56<13:19, 12.90s/it]                                                  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 110/172 [24:56<13:19, 12.90s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 111/172 [25:10<13:34, 13.35s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 112/172 [25:23<13:12, 13.21s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 113/172 [25:36<13:03, 13.27s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 114/172 [25:49<12:46, 13.22s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 115/172 [26:04<12:57, 13.64s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 116/172 [26:17<12:36, 13.52s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 117/172 [26:31<12:24, 13.54s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 118/172 [26:41<11:15, 12.51s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 119/172 [26:54<11:14, 12.72s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 120/172 [27:07<11:07, 12.83s/it]                                                  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 120/172 [27:07<11:07, 12.83s/it]{'eval_loss': 0.027501238510012627, 'eval_runtime': 62.2, 'eval_samples_per_second': 4.904, 'eval_steps_per_second': 0.627, 'eval_rewards/chosen': -0.24858829379081726, 'eval_rewards/rejected': -6.3840179443359375, 'eval_rewards/accuracies': 0.9967948794364929, 'eval_rewards/margins': 6.135429382324219, 'eval_logps/chosen': -70.27487182617188, 'eval_logps/rejected': -423.0710144042969, 'eval_logits/chosen': -3.2922868728637695, 'eval_logits/rejected': -3.1582911014556885, 'epoch': 0.7}
{'loss': 0.0433, 'grad_norm': 1.0473313331604004, 'learning_rate': 3.3441558441558443e-06, 'rewards/chosen': -0.3245992064476013, 'rewards/rejected': -6.081927299499512, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 5.757328987121582, 'logps/chosen': -77.6043930053711, 'logps/rejected': -407.72613525390625, 'logits/chosen': -3.330681324005127, 'logits/rejected': -3.1956562995910645, 'epoch': 0.81}
{'loss': 0.0237, 'grad_norm': 0.2480527013540268, 'learning_rate': 3.01948051948052e-06, 'rewards/chosen': -0.2634586691856384, 'rewards/rejected': -6.402543544769287, 'rewards/accuracies': 0.9937499761581421, 'rewards/margins': 6.139085292816162, 'logps/chosen': -76.05207824707031, 'logps/rejected': -425.6564025878906, 'logits/chosen': -3.418267011642456, 'logits/rejected': -3.2781245708465576, 'epoch': 0.93}
{'loss': 0.0216, 'grad_norm': 0.5698544979095459, 'learning_rate': 2.694805194805195e-06, 'rewards/chosen': -0.4288578927516937, 'rewards/rejected': -6.353527069091797, 'rewards/accuracies': 0.996874988079071, 'rewards/margins': 5.924668788909912, 'logps/chosen': -85.5241470336914, 'logps/rejected': -423.0213928222656, 'logits/chosen': -3.5165858268737793, 'logits/rejected': -3.3902976512908936, 'epoch': 1.05}
{'loss': 0.0167, 'grad_norm': 0.525371253490448, 'learning_rate': 2.37012987012987e-06, 'rewards/chosen': -0.24310004711151123, 'rewards/rejected': -6.737284183502197, 'rewards/accuracies': 1.0, 'rewards/margins': 6.4941840171813965, 'logps/chosen': -73.51174926757812, 'logps/rejected': -437.5391540527344, 'logits/chosen': -3.606948137283325, 'logits/rejected': -3.513563632965088, 'epoch': 1.16}
{'loss': 0.0145, 'grad_norm': 0.4589625298976898, 'learning_rate': 2.0454545454545457e-06, 'rewards/chosen': -0.3527407944202423, 'rewards/rejected': -6.6281280517578125, 'rewards/accuracies': 1.0, 'rewards/margins': 6.275387763977051, 'logps/chosen': -79.98146057128906, 'logps/rejected': -437.496337890625, 'logits/chosen': -3.7004764080047607, 'logits/rejected': -3.5960819721221924, 'epoch': 1.28}
{'loss': 0.0098, 'grad_norm': 0.3944315016269684, 'learning_rate': 1.7207792207792209e-06, 'rewards/chosen': -0.4994466304779053, 'rewards/rejected': -6.992535591125488, 'rewards/accuracies': 1.0, 'rewards/margins': 6.493088722229004, 'logps/chosen': -88.40072631835938, 'logps/rejected': -452.4508361816406, 'logits/chosen': -3.771195650100708, 'logits/rejected': -3.699357271194458, 'epoch': 1.4}

  0%|          | 0/39 [00:00<?, ?it/s][A
  5%|â–Œ         | 2/39 [00:01<00:20,  1.78it/s][A
  8%|â–Š         | 3/39 [00:03<00:42,  1.18s/it][A
 10%|â–ˆ         | 4/39 [00:05<00:51,  1.48s/it][A
 13%|â–ˆâ–Ž        | 5/39 [00:07<00:56,  1.65s/it][A
 15%|â–ˆâ–Œ        | 6/39 [00:09<00:57,  1.76s/it][A
 18%|â–ˆâ–Š        | 7/39 [00:10<00:49,  1.56s/it][A
 21%|â–ˆâ–ˆ        | 8/39 [00:11<00:44,  1.43s/it][A
 23%|â–ˆâ–ˆâ–Ž       | 9/39 [00:12<00:40,  1.35s/it][A
 26%|â–ˆâ–ˆâ–Œ       | 10/39 [00:14<00:44,  1.54s/it][A
 28%|â–ˆâ–ˆâ–Š       | 11/39 [00:16<00:47,  1.68s/it][A
 31%|â–ˆâ–ˆâ–ˆ       | 12/39 [00:18<00:47,  1.77s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [00:19<00:41,  1.60s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [00:21<00:42,  1.71s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [00:23<00:43,  1.80s/it][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [00:24<00:36,  1.57s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [00:25<00:31,  1.43s/it][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [00:27<00:33,  1.59s/it][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [00:28<00:28,  1.44s/it][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [00:30<00:25,  1.36s/it][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [00:32<00:27,  1.55s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [00:33<00:24,  1.43s/it][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [00:35<00:25,  1.61s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [00:36<00:23,  1.59s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [00:38<00:24,  1.72s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [00:39<00:20,  1.54s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [00:41<00:20,  1.69s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [00:43<00:16,  1.52s/it][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [00:45<00:16,  1.66s/it][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [00:47<00:15,  1.75s/it][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [00:48<00:12,  1.58s/it][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [00:50<00:11,  1.68s/it][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [00:52<00:10,  1.77s/it][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [00:53<00:07,  1.58s/it][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [00:54<00:06,  1.56s/it][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [00:56<00:05,  1.68s/it][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [00:58<00:03,  1.56s/it][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [00:59<00:01,  1.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:00<00:00,  1.26s/it][A                                                 
                                               [A 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 120/172 [28:09<11:07, 12.83s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:00<00:00,  1.26s/it][A
                                               [A 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 121/172 [28:22<26:43, 31.43s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 122/172 [28:35<21:38, 25.97s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 123/172 [28:48<18:03, 22.10s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 124/172 [28:57<14:28, 18.09s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 125/172 [29:10<12:58, 16.56s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 126/172 [29:23<11:53, 15.52s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 127/172 [29:36<10:55, 14.58s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 128/172 [29:50<10:42, 14.61s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 129/172 [30:04<10:18, 14.39s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 130/172 [30:14<09:11, 13.12s/it]                                                  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 130/172 [30:14<09:11, 13.12s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 131/172 [30:26<08:46, 12.84s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 132/172 [30:38<08:15, 12.38s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 133/172 [30:51<08:10, 12.57s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 134/172 [31:05<08:10, 12.92s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 135/172 [31:18<08:02, 13.04s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 136/172 [31:31<07:52, 13.13s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 137/172 [31:44<07:39, 13.14s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 138/172 [31:58<07:29, 13.23s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 139/172 [32:11<07:17, 13.26s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 140/172 [32:24<07:01, 13.17s/it]                                                  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 140/172 [32:24<07:01, 13.17s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 141/172 [32:36<06:33, 12.70s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 142/172 [32:50<06:37, 13.25s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 143/172 [33:04<06:26, 13.32s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 144/172 [33:15<05:59, 12.84s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 145/172 [33:30<06:03, 13.45s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 146/172 [33:43<05:47, 13.37s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 147/172 [33:56<05:25, 13.00s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 148/172 [34:09<05:15, 13.15s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 149/172 [34:22<05:03, 13.18s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 150/172 [34:34<04:41, 12.79s/it]                                                  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 150/172 [34:34<04:41, 12.79s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 151/172 [34:48<04:32, 12.97s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 152/172 [35:01<04:21, 13.08s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 153/172 [35:14<04:08, 13.07s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 154/172 [35:23<03:34, 11.90s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 155/172 [35:35<03:24, 12.02s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 156/172 [35:48<03:17, 12.32s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 157/172 [36:02<03:08, 12.57s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 158/172 [36:13<02:52, 12.35s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 159/172 [36:27<02:43, 12.56s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 160/172 [36:37<02:22, 11.87s/it]                                                  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 160/172 [36:37<02:22, 11.87s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 161/172 [36:51<02:17, 12.49s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 162/172 [37:04<02:07, 12.75s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 163/172 [37:18<01:56, 12.98s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 164/172 [37:32<01:47, 13.40s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 165/172 [37:45<01:32, 13.26s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 166/172 [37:58<01:19, 13.27s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 167/172 [38:12<01:07, 13.43s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 168/172 [38:26<00:54, 13.74s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 169/172 [38:38<00:39, 13.22s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 170/172 [38:51<00:26, 13.15s/it]                                                  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 170/172 [38:51<00:26, 13.15s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 171/172 [39:06<00:13, 13.56s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 172/172 [39:17<00:00, 12.65s/it]                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 172/172 [39:18<00:00, 12.65s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 172/172 [39:18<00:00, 13.71s/it]
wandb: updating run metadata
wandb: uploading console lines 24-30
wandb: 
wandb: Run history:
wandb:      eval/logits/chosen â–ˆâ–
wandb:    eval/logits/rejected â–ˆâ–
wandb:       eval/logps/chosen â–ˆâ–
wandb:     eval/logps/rejected â–ˆâ–
wandb:               eval/loss â–ˆâ–
wandb: eval/rewards/accuracies â–â–ˆ
wandb:     eval/rewards/chosen â–ˆâ–
wandb:    eval/rewards/margins â–â–ˆ
wandb:   eval/rewards/rejected â–ˆâ–
wandb:            eval/runtime â–â–ˆ
wandb:                     +15 ...
wandb: 
wandb: Run summary:
wandb:      eval/logits/chosen -3.81595
wandb:    eval/logits/rejected -3.73591
wandb:       eval/logps/chosen -77.91753
wandb:     eval/logps/rejected -469.44147
wandb:               eval/loss 0.00845
wandb: eval/rewards/accuracies 1
wandb:     eval/rewards/chosen -0.40144
wandb:    eval/rewards/margins 6.90999
wandb:   eval/rewards/rejected -7.31143
wandb:            eval/runtime 62.2175
wandb:                     +20 ...
wandb: 
wandb: ðŸš€ View run dpo_mistral_7b-blocksworld-1206 at: https://wandb.ai/fjl2401-university-of-notre-dame/pddl-dpo-mistral7b/runs/dseasmhi
wandb: â­ï¸ View project at: https://wandb.ai/fjl2401-university-of-notre-dame/pddl-dpo-mistral7b
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251206_070243-dseasmhi/logs
{'eval_loss': 0.008450192399322987, 'eval_runtime': 62.2175, 'eval_samples_per_second': 4.902, 'eval_steps_per_second': 0.627, 'eval_rewards/chosen': -0.40144142508506775, 'eval_rewards/rejected': -7.311427116394043, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 6.909985542297363, 'eval_logps/chosen': -77.91753387451172, 'eval_logps/rejected': -469.44146728515625, 'eval_logits/chosen': -3.815948009490967, 'eval_logits/rejected': -3.7359142303466797, 'epoch': 1.4}
{'loss': 0.0071, 'grad_norm': 0.26758620142936707, 'learning_rate': 1.3961038961038962e-06, 'rewards/chosen': -0.4437975287437439, 'rewards/rejected': -7.358249664306641, 'rewards/accuracies': 1.0, 'rewards/margins': 6.91445255279541, 'logps/chosen': -81.61373138427734, 'logps/rejected': -473.82928466796875, 'logits/chosen': -3.862900972366333, 'logits/rejected': -3.7763476371765137, 'epoch': 1.51}
{'loss': 0.0081, 'grad_norm': 0.24962612986564636, 'learning_rate': 1.0714285714285714e-06, 'rewards/chosen': -0.5892143249511719, 'rewards/rejected': -7.456295967102051, 'rewards/accuracies': 1.0, 'rewards/margins': 6.867081642150879, 'logps/chosen': -94.50416564941406, 'logps/rejected': -475.4353942871094, 'logits/chosen': -3.878577470779419, 'logits/rejected': -3.8331267833709717, 'epoch': 1.63}
{'loss': 0.0083, 'grad_norm': 0.46920856833457947, 'learning_rate': 7.467532467532468e-07, 'rewards/chosen': -0.43504849076271057, 'rewards/rejected': -7.624983310699463, 'rewards/accuracies': 1.0, 'rewards/margins': 7.189935207366943, 'logps/chosen': -85.23738098144531, 'logps/rejected': -486.33197021484375, 'logits/chosen': -3.9110870361328125, 'logits/rejected': -3.848876476287842, 'epoch': 1.74}
{'loss': 0.0061, 'grad_norm': 0.1348354071378708, 'learning_rate': 4.220779220779221e-07, 'rewards/chosen': -0.3771398663520813, 'rewards/rejected': -7.62805700302124, 'rewards/accuracies': 1.0, 'rewards/margins': 7.250916957855225, 'logps/chosen': -79.05526733398438, 'logps/rejected': -484.85821533203125, 'logits/chosen': -3.912954807281494, 'logits/rejected': -3.866097927093506, 'epoch': 1.86}
{'loss': 0.0055, 'grad_norm': 0.1731913685798645, 'learning_rate': 9.74025974025974e-08, 'rewards/chosen': -0.5130060315132141, 'rewards/rejected': -7.472310543060303, 'rewards/accuracies': 1.0, 'rewards/margins': 6.959303855895996, 'logps/chosen': -91.98685455322266, 'logps/rejected': -480.6224670410156, 'logits/chosen': -3.937920093536377, 'logits/rejected': -3.8979294300079346, 'epoch': 1.98}
{'train_runtime': 2358.1753, 'train_samples_per_second': 2.328, 'train_steps_per_second': 0.073, 'train_loss': 0.06083300212658076, 'epoch': 2.0}

==========================================
DPO training completed!
==========================================
Model saved to: /jfan5/dpo_models/mistral_7b-1206

