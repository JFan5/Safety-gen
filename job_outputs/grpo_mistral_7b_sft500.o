
CondaError: Run 'conda init' before 'conda activate'

==========================================
GRPO Training for Mistral-7B
==========================================
Base model: /jfan5/sft_models/mistral_7b/four_scenarios500-1124
Dataset: /jfan5/ppo_data/all_scenarios.jsonl
Output: /jfan5/grpo_models/mistral_7b-1127

Training parameters:
  Epochs: 1.0
  Batch size: 8
  Gradient accumulation: 4
  Learning rate: 2e-6
  Generations per prompt: 8
==========================================

[torchao|WARNING]Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Unsloth 2025.10.12 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 9,000 | Num Epochs = 1 | Total steps = 500
O^O/ \_/ \    Batch size per device = 8 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32
 "-____-"     Trainable parameters = 83,886,080 of 7,331,909,632 (1.14% trained)
wandb: Currently logged in as: fjl2401 (fjl2401-university-of-notre-dame) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /home/ubuntu/Safety-gen/wandb/run-20251128_154736-5qn7b4gs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grpo_mistral_7b-1127
wandb: ‚≠êÔ∏è View project at https://wandb.ai/fjl2401-university-of-notre-dame/huggingface
wandb: üöÄ View run at https://wandb.ai/fjl2401-university-of-notre-dame/huggingface/runs/5qn7b4gs
wandb: Detected [huggingface_hub.inference] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.
ü¶• Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2025.10.12: Fast Mistral patching. Transformers: 4.56.2.
   \\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.109 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
unsloth/mistral-7b-instruct-v0.3-bnb-4bit does not have a padding token! Will use pad_token = [control_768].
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:47<6:31:01, 47.02s/it]  0%|          | 2/500 [01:01<3:51:26, 27.88s/it]  1%|          | 3/500 [01:22<3:26:06, 24.88s/it]  1%|          | 4/500 [01:45<3:18:22, 24.00s/it]  1%|          | 5/500 [02:08<3:14:46, 23.61s/it]  1%|          | 6/500 [02:30<3:11:36, 23.27s/it]  1%|‚ñè         | 7/500 [02:52<3:05:10, 22.54s/it]  2%|‚ñè         | 8/500 [03:08<2:48:47, 20.58s/it]  2%|‚ñè         | 9/500 [03:25<2:38:18, 19.34s/it]  2%|‚ñè         | 10/500 [03:43<2:34:53, 18.97s/it]  2%|‚ñè         | 11/500 [03:58<2:26:15, 17.95s/it]  2%|‚ñè         | 12/500 [04:13<2:19:05, 17.10s/it]  3%|‚ñé         | 13/500 [04:28<2:11:41, 16.22s/it]  3%|‚ñé         | 14/500 [04:50<2:27:08, 18.17s/it]