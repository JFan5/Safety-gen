
CondaError: Run 'conda init' before 'conda activate'

==========================================
GRPO Training for Mistral-7B - Blocksworld
==========================================
Base model: /jfan5/sft_models/mistral_variant-blocksworld
Dataset: /jfan5/grpo_data-127/blocksworld.jsonl
Output: /jfan5/grpo_models/mistral_7b-blocksworld-stl-1208-500

Training parameters:
  Batch size: 4
  Gradient accumulation: 8
  Learning rate: 5e-6
  Generations per prompt: 4
==========================================

ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.
Unsloth 2025.11.6 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.
wandb: Currently logged in as: fjl2401 (fjl2401-university-of-notre-dame) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /home/ubuntu/Safety-gen/wandb/run-20251213_002622-yf74zkvi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grpo_mistral_7b-blocksworld-1208-500
wandb: ‚≠êÔ∏è View project at https://wandb.ai/fjl2401-university-of-notre-dame/pddl-grpo-mistral7b
wandb: üöÄ View run at https://wandb.ai/fjl2401-university-of-notre-dame/pddl-grpo-mistral7b/runs/yf74zkvi
wandb: Detected [huggingface_hub.inference, openai] in use.
wandb: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 500 | Num Epochs = 9 | Total steps = 500
O^O/ \_/ \    Batch size per device = 4 | Gradient accumulation steps = 8
\        /    Data Parallel GPUs = 1 | Total batch size (4 x 8 x 1) = 32
 "-____-"     Trainable parameters = 83,886,080 of 7,331,909,632 (1.14% trained)
ü¶• Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2025.11.6: Fast Mistral patching. Transformers: 4.56.2. vLLM: 0.12.0.
   \\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.109 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.9.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
unsloth/mistral-7b-instruct-v0.3-bnb-4bit does not have a padding token! Will use pad_token = [control_768].
============================================================
Reward Function Configuration:
  Using manual discrete reward table: compute_reward
  Scenario-specific reward functions: disabled
============================================================
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:32:58, 25.61s/it]  0%|          | 2/500 [00:41<2:47:31, 20.18s/it]  1%|          | 3/500 [00:58<2:32:44, 18.44s/it]  1%|          | 4/500 [01:14<2:25:57, 17.66s/it]  1%|          | 5/500 [01:31<2:21:51, 17.19s/it]  1%|          | 6/500 [01:47<2:19:16, 16.92s/it]  1%|‚ñè         | 7/500 [02:03<2:17:27, 16.73s/it]  2%|‚ñè         | 8/500 [02:19<2:14:29, 16.40s/it]  2%|‚ñè         | 9/500 [02:35<2:14:07, 16.39s/it]  2%|‚ñè         | 10/500 [02:51<2:12:42, 16.25s/it]  2%|‚ñè         | 11/500 [03:06<2:08:29, 15.77s/it]  2%|‚ñè         | 12/500 [03:22<2:09:41, 15.95s/it]  3%|‚ñé         | 13/500 [03:36<2:03:40, 15.24s/it]  3%|‚ñé         | 14/500 [03:51<2:03:45, 15.28s/it]  3%|‚ñé         | 15/500 [04:08<2:06:54, 15.70s/it]  3%|‚ñé         | 16/500 [04:24<2:08:15, 15.90s/it]  3%|‚ñé         | 17/500 [04:41<2:08:46, 16.00s/it]  4%|‚ñé         | 18/500 [04:57<2:09:23, 16.11s/it]  4%|‚ñç         | 19/500 [05:13<2:09:42, 16.18s/it]  4%|‚ñç         | 20/500 [05:30<2:09:55, 16.24s/it]                                                    4%|‚ñç         | 20/500 [05:30<2:09:55, 16.24s/it]  4%|‚ñç         | 21/500 [05:46<2:09:57, 16.28s/it]  4%|‚ñç         | 22/500 [06:02<2:09:42, 16.28s/it]  5%|‚ñç         | 23/500 [06:19<2:09:36, 16.30s/it]  5%|‚ñç         | 24/500 [06:35<2:09:26, 16.32s/it]  5%|‚ñå         | 25/500 [06:51<2:09:15, 16.33s/it]  5%|‚ñå         | 26/500 [07:08<2:09:00, 16.33s/it]  5%|‚ñå         | 27/500 [07:24<2:08:41, 16.33s/it]  6%|‚ñå         | 28/500 [07:38<2:03:32, 15.70s/it]  6%|‚ñå         | 29/500 [07:55<2:04:43, 15.89s/it]  6%|‚ñå         | 30/500 [08:11<2:05:31, 16.02s/it]  6%|‚ñå         | 31/500 [08:27<2:05:59, 16.12s/it]  6%|‚ñã         | 32/500 [08:44<2:06:17, 16.19s/it]  7%|‚ñã         | 33/500 [09:00<2:06:24, 16.24s/it]  7%|‚ñã         | 34/500 [09:17<2:06:32, 16.29s/it]  7%|‚ñã         | 35/500 [09:33<2:06:23, 16.31s/it]  7%|‚ñã         | 36/500 [09:49<2:06:01, 16.30s/it]  7%|‚ñã         | 37/500 [10:05<2:05:52, 16.31s/it]  8%|‚ñä         | 38/500 [10:22<2:05:28, 16.30s/it]  8%|‚ñä         | 39/500 [10:38<2:05:18, 16.31s/it]  8%|‚ñä         | 40/500 [10:54<2:05:08, 16.32s/it]                                                    8%|‚ñä         | 40/500 [10:54<2:05:08, 16.32s/it]  8%|‚ñä         | 41/500 [11:11<2:04:54, 16.33s/it]  8%|‚ñä         | 42/500 [11:27<2:04:40, 16.33s/it]  9%|‚ñä         | 43/500 [11:43<2:04:13, 16.31s/it]  9%|‚ñâ         | 44/500 [12:00<2:04:02, 16.32s/it]  9%|‚ñâ         | 45/500 [12:16<2:03:49, 16.33s/it]  9%|‚ñâ         | 46/500 [12:32<2:03:18, 16.30s/it]  9%|‚ñâ         | 47/500 [12:49<2:03:08, 16.31s/it] 10%|‚ñâ         | 48/500 [13:05<2:02:58, 16.32s/it] 10%|‚ñâ         | 49/500 [13:21<2:02:44, 16.33s/it] 10%|‚ñà         | 50/500 [13:38<2:02:32, 16.34s/it] 10%|‚ñà         | 51/500 [13:54<2:02:12, 16.33s/it] 10%|‚ñà         | 52/500 [14:10<2:01:58, 16.34s/it] 11%|‚ñà         | 53/500 [14:27<2:01:22, 16.29s/it] 11%|‚ñà         | 54/500 [14:43<2:01:14, 16.31s/it] 11%|‚ñà         | 55/500 [14:59<2:01:04, 16.32s/it] 11%|‚ñà         | 56/500 [15:16<2:00:49, 16.33s/it] 11%|‚ñà‚ñè        | 57/500 [15:32<2:00:37, 16.34s/it] 12%|‚ñà‚ñè        | 58/500 [15:48<2:00:28, 16.35s/it] 12%|‚ñà‚ñè        | 59/500 [16:05<2:00:13, 16.36s/it] 12%|‚ñà‚ñè        | 60/500 [16:21<1:59:57, 16.36s/it]                                                   12%|‚ñà‚ñè        | 60/500 [16:21<1:59:57, 16.36s/it] 12%|‚ñà‚ñè        | 61/500 [16:37<1:59:41, 16.36s/it] 12%|‚ñà‚ñè        | 62/500 [16:54<1:59:24, 16.36s/it] 13%|‚ñà‚ñé        | 63/500 [17:10<1:59:13, 16.37s/it] 13%|‚ñà‚ñé        | 64/500 [17:27<1:58:57, 16.37s/it] 13%|‚ñà‚ñé        | 65/500 [17:43<1:58:39, 16.37s/it] 13%|‚ñà‚ñé        | 66/500 [17:59<1:58:22, 16.37s/it] 13%|‚ñà‚ñé        | 67/500 [18:16<1:58:06, 16.37s/it] 14%|‚ñà‚ñé        | 68/500 [18:32<1:57:48, 16.36s/it] 14%|‚ñà‚ñç        | 69/500 [18:48<1:57:31, 16.36s/it] 14%|‚ñà‚ñç        | 70/500 [19:05<1:57:14, 16.36s/it] 14%|‚ñà‚ñç        | 71/500 [19:21<1:56:56, 16.36s/it] 14%|‚ñà‚ñç        | 72/500 [19:37<1:56:43, 16.36s/it] 15%|‚ñà‚ñç        | 73/500 [19:54<1:56:25, 16.36s/it] 15%|‚ñà‚ñç        | 74/500 [20:10<1:56:06, 16.35s/it] 15%|‚ñà‚ñå        | 75/500 [20:26<1:55:47, 16.35s/it] 15%|‚ñà‚ñå        | 76/500 [20:43<1:55:28, 16.34s/it] 15%|‚ñà‚ñå        | 77/500 [20:59<1:55:14, 16.35s/it] 16%|‚ñà‚ñå        | 78/500 [21:15<1:54:40, 16.30s/it] 16%|‚ñà‚ñå        | 79/500 [21:32<1:54:12, 16.28s/it] 16%|‚ñà‚ñå        | 80/500 [21:48<1:54:12, 16.32s/it]                                                   16%|‚ñà‚ñå        | 80/500 [21:48<1:54:12, 16.32s/it] 16%|‚ñà‚ñå        | 81/500 [22:04<1:54:04, 16.34s/it] 16%|‚ñà‚ñã        | 82/500 [22:21<1:53:53, 16.35s/it] 17%|‚ñà‚ñã        | 83/500 [22:37<1:53:23, 16.32s/it] 17%|‚ñà‚ñã        | 84/500 [22:53<1:53:13, 16.33s/it] 17%|‚ñà‚ñã        | 85/500 [23:10<1:53:02, 16.34s/it]slurmstepd-ambitious-tesla: error: *** JOB 254 ON ambitious-tesla CANCELLED AT 2025-12-13T00:49:36 ***
