#!/usr/bin/env python3
"""
Generate LaTeX table comparing PDDL3 (symbolic) vs Natural Language vs JSON input format performance.

Usage:
    python script/generate_input_format_table.py \
        --pddl3-dir <path_to_pddl3_eval> \
        --nl-dir <path_to_nl_eval> \
        [--json-dir <path_to_json_eval>] \
        --output <output_tex_file>
"""

import argparse
import json
from pathlib import Path

# Domain order for table rows
DOMAIN_ORDER = ["blocksworld", "ferry", "grippers", "spanner", "delivery"]
DOMAIN_DISPLAY_NAMES = {
    "blocksworld": "Blocksworld",
    "ferry": "Ferry",
    "grippers": "Grippers",
    "spanner": "Spanner",
    "delivery": "Delivery",
}

# Metrics to display (in order)
METRICS = [
    ("success_plans", "Success $\\uparrow$", True),  # (key, display_name, higher_is_better)
    ("precondition_violation", "Precond. Err", False),
    ("safety_constraints_violation", "Safety Viol", False),
    ("goal_not_satisfied", "Goal Err", False),
]


def load_metrics(eval_dir: str) -> dict:
    """Load metrics.json from an evaluation directory."""
    metrics_path = Path(eval_dir) / "metrics.json"
    if not metrics_path.exists():
        raise FileNotFoundError(f"metrics.json not found in {eval_dir}")

    with open(metrics_path, "r") as f:
        return json.load(f)


def format_percentage(value: float, bold: bool = False) -> str:
    """Format a percentage value for LaTeX."""
    formatted = f"{value:.1f}\\%"
    return f"\\textbf{{{formatted}}}" if bold else formatted


def find_best_value(values: list[float], higher_is_better: bool) -> int:
    """Find the index of the best value in the list."""
    if higher_is_better:
        best_val = max(values)
    else:
        best_val = min(values)
    # Return index of first occurrence of best value
    return values.index(best_val)


def generate_table_three_formats(pddl3_dir: str, nl_dir: str, json_dir: str, output_file: str) -> str:
    """Generate the LaTeX comparison table with three formats."""
    pddl3_data = load_metrics(pddl3_dir)
    nl_data = load_metrics(nl_dir)
    json_data = load_metrics(json_dir)

    lines = []

    # Table header
    lines.append("% Input Format Comparison Table: PDDL3 vs Natural Language vs JSON")
    lines.append("% Auto-generated by script/generate_input_format_table.py")
    lines.append("\\begin{table}[t]")
    lines.append("\\centering")
    lines.append("\\caption{Performance comparison: Symbolic (PDDL3) vs Natural Language vs JSON input}")
    lines.append("\\label{tab:input_format_comparison}")
    lines.append("\\resizebox{\\columnwidth}{!}{")

    # Column spec: domain name + 4 metrics * 3 (PDDL3/NL/JSON each)
    col_spec = "l|" + "|".join(["ccc"] * len(METRICS))
    lines.append(f"\\begin{{tabular}}{{{col_spec}}}")
    lines.append("\\hline")

    # Header row 1: metric names spanning three columns
    header1 = "\\textbf{Domain}"
    for i, (_, display_name, _) in enumerate(METRICS):
        if i < len(METRICS) - 1:
            header1 += f" & \\multicolumn{{3}}{{c|}}{{{display_name}}}"
        else:
            header1 += f" & \\multicolumn{{3}}{{c}}{{{display_name}}}"
    lines.append(header1 + " \\\\")

    # Header row 2: PDDL3/NL/JSON sub-columns
    header2 = ""
    for i, _ in enumerate(METRICS):
        header2 += " & PDDL3 & NL & JSON"
    lines.append(header2 + " \\\\")
    lines.append("\\hline")

    # Data rows for each domain
    for domain in DOMAIN_ORDER:
        pddl3_rates = pddl3_data["per_scenario"].get(domain, {}).get("category_rates", {})
        nl_rates = nl_data["per_scenario"].get(domain, {}).get("category_rates", {})
        json_rates = json_data["per_scenario"].get(domain, {}).get("category_rates", {})

        row = f"{DOMAIN_DISPLAY_NAMES[domain]}"

        for metric_key, _, higher_is_better in METRICS:
            pddl3_val = pddl3_rates.get(metric_key, 0.0)
            nl_val = nl_rates.get(metric_key, 0.0)
            json_val = json_rates.get(metric_key, 0.0)

            values = [pddl3_val, nl_val, json_val]
            best_idx = find_best_value(values, higher_is_better)

            # Check if there are ties (multiple values equal to best)
            if higher_is_better:
                best_val = max(values)
            else:
                best_val = min(values)
            is_best = [v == best_val for v in values]

            pddl3_str = format_percentage(pddl3_val, bold=is_best[0])
            nl_str = format_percentage(nl_val, bold=is_best[1])
            json_str = format_percentage(json_val, bold=is_best[2])

            row += f" & {pddl3_str} & {nl_str} & {json_str}"

        row += " \\\\"
        lines.append(row)

    lines.append("\\hline")

    # Overall row
    pddl3_overall = pddl3_data["overall"]["category_rates"]
    nl_overall = nl_data["overall"]["category_rates"]
    json_overall = json_data["overall"]["category_rates"]

    row = "\\textbf{Overall}"
    for metric_key, _, higher_is_better in METRICS:
        pddl3_val = pddl3_overall.get(metric_key, 0.0)
        nl_val = nl_overall.get(metric_key, 0.0)
        json_val = json_overall.get(metric_key, 0.0)

        values = [pddl3_val, nl_val, json_val]
        if higher_is_better:
            best_val = max(values)
        else:
            best_val = min(values)
        is_best = [v == best_val for v in values]

        pddl3_str = format_percentage(pddl3_val, bold=is_best[0])
        nl_str = format_percentage(nl_val, bold=is_best[1])
        json_str = format_percentage(json_val, bold=is_best[2])

        row += f" & {pddl3_str} & {nl_str} & {json_str}"

    row += " \\\\"
    lines.append(row)
    lines.append("\\hline")

    # Close table
    lines.append("\\end{tabular}")
    lines.append("}")
    lines.append("\\end{table}")

    # Join and write
    table_content = "\n".join(lines)

    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(table_content, encoding="utf-8")

    print(f"LaTeX table generated: {output_file}")
    print("\nTable preview:\n")
    print(table_content)

    return table_content


def generate_table_two_formats(pddl3_dir: str, nl_dir: str, output_file: str) -> str:
    """Generate the LaTeX comparison table with two formats (backward compatible)."""
    pddl3_data = load_metrics(pddl3_dir)
    nl_data = load_metrics(nl_dir)

    lines = []

    # Table header
    lines.append("% Input Format Comparison Table: PDDL3 vs Natural Language")
    lines.append("% Auto-generated by script/generate_input_format_table.py")
    lines.append("\\begin{table}[t]")
    lines.append("\\centering")
    lines.append("\\caption{Performance comparison: Symbolic (PDDL3) vs Natural Language input}")
    lines.append("\\label{tab:input_format_comparison}")
    lines.append("\\resizebox{\\columnwidth}{!}{")

    # Column spec: domain name + 4 metrics * 2 (PDDL3/NL each)
    col_spec = "l|" + "|".join(["cc"] * len(METRICS))
    lines.append(f"\\begin{{tabular}}{{{col_spec}}}")
    lines.append("\\hline")

    # Header row 1: metric names spanning two columns
    header1 = "\\textbf{Domain}"
    for i, (_, display_name, _) in enumerate(METRICS):
        if i < len(METRICS) - 1:
            header1 += f" & \\multicolumn{{2}}{{c|}}{{{display_name}}}"
        else:
            header1 += f" & \\multicolumn{{2}}{{c}}{{{display_name}}}"
    lines.append(header1 + " \\\\")

    # Header row 2: PDDL3/NL sub-columns
    header2 = ""
    for i, _ in enumerate(METRICS):
        header2 += " & PDDL3 & NL"
    lines.append(header2 + " \\\\")
    lines.append("\\hline")

    # Data rows for each domain
    for domain in DOMAIN_ORDER:
        pddl3_rates = pddl3_data["per_scenario"].get(domain, {}).get("category_rates", {})
        nl_rates = nl_data["per_scenario"].get(domain, {}).get("category_rates", {})

        row = f"{DOMAIN_DISPLAY_NAMES[domain]}"

        for metric_key, _, higher_is_better in METRICS:
            pddl3_val = pddl3_rates.get(metric_key, 0.0)
            nl_val = nl_rates.get(metric_key, 0.0)

            # Determine which is better for bolding
            if higher_is_better:
                pddl3_better = pddl3_val > nl_val
                nl_better = nl_val > pddl3_val
            else:
                pddl3_better = pddl3_val < nl_val
                nl_better = nl_val < pddl3_val

            pddl3_str = format_percentage(pddl3_val, bold=pddl3_better)
            nl_str = format_percentage(nl_val, bold=nl_better)

            row += f" & {pddl3_str} & {nl_str}"

        row += " \\\\"
        lines.append(row)

    lines.append("\\hline")

    # Overall row
    pddl3_overall = pddl3_data["overall"]["category_rates"]
    nl_overall = nl_data["overall"]["category_rates"]

    row = "\\textbf{Overall}"
    for metric_key, _, higher_is_better in METRICS:
        pddl3_val = pddl3_overall.get(metric_key, 0.0)
        nl_val = nl_overall.get(metric_key, 0.0)

        if higher_is_better:
            pddl3_better = pddl3_val > nl_val
            nl_better = nl_val > pddl3_val
        else:
            pddl3_better = pddl3_val < nl_val
            nl_better = nl_val < pddl3_val

        pddl3_str = format_percentage(pddl3_val, bold=pddl3_better)
        nl_str = format_percentage(nl_val, bold=nl_better)

        row += f" & {pddl3_str} & {nl_str}"

    row += " \\\\"
    lines.append(row)
    lines.append("\\hline")

    # Close table
    lines.append("\\end{tabular}")
    lines.append("}")
    lines.append("\\end{table}")

    # Join and write
    table_content = "\n".join(lines)

    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(table_content, encoding="utf-8")

    print(f"LaTeX table generated: {output_file}")
    print("\nTable preview:\n")
    print(table_content)

    return table_content


def main():
    parser = argparse.ArgumentParser(
        description="Generate LaTeX table comparing PDDL3 vs NL (vs JSON) input format performance"
    )
    parser.add_argument(
        "--pddl3-dir",
        required=True,
        help="Path to PDDL3 evaluation directory containing metrics.json",
    )
    parser.add_argument(
        "--nl-dir",
        required=True,
        help="Path to NL evaluation directory containing metrics.json",
    )
    parser.add_argument(
        "--json-dir",
        default=None,
        help="Path to JSON evaluation directory containing metrics.json (optional)",
    )
    parser.add_argument(
        "--output",
        default="paper/new_submission/exp_figure_reference/table3_input_format.tex",
        help="Output path for LaTeX table",
    )

    args = parser.parse_args()

    if args.json_dir:
        generate_table_three_formats(args.pddl3_dir, args.nl_dir, args.json_dir, args.output)
    else:
        generate_table_two_formats(args.pddl3_dir, args.nl_dir, args.output)


if __name__ == "__main__":
    main()
