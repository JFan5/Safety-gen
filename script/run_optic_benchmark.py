#!/usr/bin/env python3
"""
Benchmark OPTIC against an LLM solver on automatically generated problem suites.

This script orchestrates the benchmark process:
1. Generate or load problems
2. Run solvers (OPTIC and/or LLM)
3. Collect and summarize results
"""
from __future__ import annotations

import argparse
import sys
from pathlib import Path

from benchmark_common import (
    CACHE_FILENAME,
    DEFAULT_INSTANCES_PER_PARAMETER,
    DEFAULT_TIME_LIMIT,
    PROJECT_ROOT,
    ensure_binaries,
    load_solver_cache,
    save_solver_cache,
)

from generate_benchmark_problems import (
    filter_one_per_parameter,
    generate_complex_problems,
    load_existing_problems,
)

from solve_llm import solve_with_llm, write_llm_results
from solve_optic import solve_with_optic, write_optic_results
from solve_results import (
    compute_solver_summary,
    merge_results,
    write_summary_csv,
    write_summary_json,
)

DEFAULT_PROBLEM_COUNT = 0  # 0 means no upper limit (use all parameter levels)

try:
    from evaluate_llm_solver import MAX_NEW_TOKENS  # type: ignore

    HAVE_LLM = True
except Exception:
    HAVE_LLM = False
    MAX_NEW_TOKENS = 8000


def parse_args(argv=None):
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(description="Run OPTIC and LLM solver benchmarks.")
    parser.add_argument(
        "--benchmark-dir",
        type=Path,
        default=PROJECT_ROOT / "benchmark_problems",
        help="Directory to store generated benchmark problems.",
    )
    parser.add_argument(
        "--problems-per-scenario",
        type=int,
        default=DEFAULT_PROBLEM_COUNT,
        help="Maximum number of problems per scenario (0 means no limit).",
    )
    parser.add_argument(
        "--instances-per-parameter",
        type=int,
        default=DEFAULT_INSTANCES_PER_PARAMETER,
        help="Number of instances to keep for each parameter value (0 means treat all equally).",
    )
    parser.add_argument(
        "--time-limit",
        type=float,
        default=DEFAULT_TIME_LIMIT,
        help="Per-problem time limit (seconds) for OPTIC.",
    )
    parser.add_argument(
        "--results-csv",
        type=Path,
        default=None,
        help="Path for the combined results CSV (defaults to benchmark_dir/solver_comparison.csv).",
    )
    parser.add_argument(
        "--llm-model",
        type=str,
        default="unsloth/gpt-oss-20b-unsloth-bnb-4bit",
        help="Path or identifier of the LLM model to benchmark (default: unsloth/gpt-oss-20b-unsloth-bnb-4bit).",
    )
    parser.add_argument(
        "--llm-family",
        type=str,
        default="gpt",
        choices=["auto", "mistral", "llama", "phi", "qwen", "gemma", "gpt"],
        help="Model family for tokenizer templating (default: gpt).",
    )
    parser.add_argument(
        "--llm-max-new-tokens",
        type=int,
        default=MAX_NEW_TOKENS if HAVE_LLM else 8000,
        help="Maximum number of tokens generated by the LLM solver.",
    )
    parser.add_argument(
        "--skip-llm",
        action="store_true",
        help="Skip the LLM solver benchmark.",
    )
    parser.add_argument(
        "--skip-optic",
        action="store_true",
        help="Skip the OPTIC solver benchmark (only test LLM).",
    )
    parser.add_argument(
        "--llm-hardest-only",
        action="store_true",
        help="Only run the LLM solver on the hardest problem per scenario.",
    )
    parser.add_argument(
        "--one-per-parameter",
        action="store_true",
        help="Only run one problem per parameter value (even if multiple instances were generated).",
    )
    parser.add_argument(
        "--skip-generation",
        action="store_true",
        help="Skip problem generation and use existing problems from benchmark directory.",
    )
    parser.add_argument(
        "--skip-solved",
        action="store_true",
        help="Reuse cached solver results when available and skip solving problems that already have a valid solution.",
    )
    parser.add_argument(
        "--problems-dir",
        type=Path,
        default=None,
        help="Directory containing problem files to use directly (bypasses generation and benchmark-dir structure).",
    )
    parser.add_argument(
        "--domain",
        type=str,
        default=None,
        help="Domain/scenario name to use when --problems-dir is specified (e.g., blocksworld, ferry).",
    )
    return parser.parse_args(argv)


def run(argv=None):
    """Main benchmark orchestration function."""
    args = parse_args(argv)
    
    # Check required binaries based on what we're running
    check_optic = not args.skip_optic
    check_validator = not args.skip_llm  # Validator is needed for LLM validation
    ensure_binaries(check_optic=check_optic, check_validator=check_validator)

    results_csv = args.results_csv or (args.benchmark_dir / "solver_comparison.csv")

    cache_path = args.benchmark_dir / CACHE_FILENAME
    cache = load_solver_cache(cache_path)

    # Generate or load existing problems
    if args.problems_dir is not None:
        # Load problems directly from the specified directory (used by run_benchmark.py)
        print(f"Loading problems from specified directory: {args.problems_dir}", flush=True)
        problems_dir = args.problems_dir
        if not problems_dir.exists():
            raise ValueError(f"Problems directory not found: {problems_dir}")

        # Find all .pddl files in the directory
        pddl_files = sorted([
            f for f in problems_dir.iterdir()
            if f.is_file() and f.suffix == ".pddl"
        ], key=lambda p: p.name)

        if not pddl_files:
            raise ValueError(f"No .pddl files found in {problems_dir}")

        # Determine the scenario name
        scenario = args.domain if args.domain else "default"
        generated = {scenario: pddl_files}
        print(f"Loaded {len(pddl_files)} problems for scenario '{scenario}'", flush=True)
    elif args.skip_generation:
        print("Skipping problem generation, loading existing problems...", flush=True)
        generated = load_existing_problems(
            args.benchmark_dir,
            instances_per_parameter=args.instances_per_parameter if args.instances_per_parameter > 0 else None,
        )
        total_problems = sum(len(problems) for problems in generated.values())
        if total_problems == 0:
            raise ValueError(
                f"No existing problems found in {args.benchmark_dir}. "
                "Please run without --skip-generation first to generate problems."
            )
    else:
        generated = generate_complex_problems(
            args.benchmark_dir,
            args.problems_per_scenario,
            args.instances_per_parameter,
        )
    
    # Filter to one problem per parameter if requested
    if args.one_per_parameter:
        generated = filter_one_per_parameter(generated)
        total_problems = sum(len(problems) for problems in generated.values())
        print(f"Filtered to one problem per parameter: {total_problems} problems total")
    
    # Run OPTIC if not skipped
    optic_results = {}
    if not args.skip_optic:
        optic_csv = args.benchmark_dir / "optic_results.csv"
        optic_results = solve_with_optic(
            args.benchmark_dir,
            generated,
            args.time_limit,
            incremental_write=True,
            results_csv=optic_csv,
            skip_solved=args.skip_solved,
            cache=cache,
            cache_path=cache_path,
        )
    else:
        print("Skipping OPTIC solver benchmark (--skip-optic specified)")

    # Run LLM if not skipped
    llm_results = {}
    if not args.skip_llm:
        model_path = args.llm_model or "unsloth/gpt-oss-20b-unsloth-bnb-4bit"
        family = args.llm_family or "gpt"
        llm_csv = args.benchmark_dir / "llm_results.csv"
        llm_results = solve_with_llm(
            args.benchmark_dir,
            generated,
            model_path,
            family,
            args.llm_max_new_tokens,
            args.llm_hardest_only,
            incremental_write=True,
            results_csv=llm_csv,
            skip_solved=args.skip_solved,
            cache=cache,
            cache_path=cache_path,
        )
    else:
        print("Skipping LLM solver benchmark (--skip-llm specified)")

    # Ensure at least one solver was run
    if args.skip_optic and args.skip_llm:
        raise ValueError("Cannot skip both OPTIC and LLM solvers. At least one must be run.")

    # Write results to separate files
    has_optic = len(optic_results) > 0
    has_llm = len(llm_results) > 0
    
    if has_optic and has_llm:
        # Both solvers ran: write combined results
        merge_results(generated, optic_results, llm_results, results_csv)
        print(f"Combined results written to: {results_csv}")
    elif has_optic:
        # Only OPTIC ran: results already written incrementally, just confirm
        optic_csv = args.benchmark_dir / "optic_results.csv"
        if optic_csv.exists():
            print(f"OPTIC results already written to: {optic_csv}")
        else:
            # Fallback: write all results if incremental write didn't happen
            write_optic_results(generated, optic_results, optic_csv)
            print(f"OPTIC results written to: {optic_csv}")
    elif has_llm:
        # Only LLM ran: results already written incrementally, just confirm
        llm_csv = args.benchmark_dir / "llm_results.csv"
        if llm_csv.exists():
            print(f"LLM results already written to: {llm_csv}")
        else:
            # Fallback: write all results if incremental write didn't happen
            write_llm_results(generated, llm_results, llm_csv)
            print(f"LLM results written to: {llm_csv}")
    
    # Compute summaries
    optic_summary = compute_solver_summary(generated, optic_results, only_present=False) if not args.skip_optic else {}
    llm_summary = (
        compute_solver_summary(generated, llm_results, only_present=True) if llm_results else None
    )
    
    # Write summary CSV
    write_summary_csv(args.benchmark_dir, optic_summary, llm_summary)
    print(f"Summary written to: {args.benchmark_dir / Path('solver_summary.csv')}")
    
    # Write JSON summary (separate files if only one solver ran)
    if has_optic and has_llm:
        # Both solvers: write combined JSON
        write_summary_json(
            args.benchmark_dir,
            generated,
            optic_results,
            llm_results,
            optic_summary,
            llm_summary,
            args.instances_per_parameter,
            args.problems_per_scenario,
            args.time_limit,
            args.llm_model,
            args.llm_family,
            args.llm_max_new_tokens,
            args.llm_hardest_only,
            "solver_summary.json",
        )
        print(f"JSON summary written to: {args.benchmark_dir / Path('solver_summary.json')}")
    elif has_optic:
        # Only OPTIC: write OPTIC-only JSON
        write_summary_json(
            args.benchmark_dir,
            generated,
            optic_results,
            {},
            optic_summary,
            None,
            args.instances_per_parameter,
            args.problems_per_scenario,
            args.time_limit,
            args.llm_model,
            args.llm_family,
            args.llm_max_new_tokens,
            args.llm_hardest_only,
            "optic_summary.json",
        )
        print(f"OPTIC JSON summary written to: {args.benchmark_dir / Path('optic_summary.json')}")
    elif has_llm:
        # Only LLM: write LLM-only JSON
        write_summary_json(
            args.benchmark_dir,
            generated,
            {},
            llm_results,
            {},
            llm_summary,
            args.instances_per_parameter,
            args.problems_per_scenario,
            args.time_limit,
            args.llm_model,
            args.llm_family,
            args.llm_max_new_tokens,
            args.llm_hardest_only,
            "llm_summary.json",
        )
        print(f"LLM JSON summary written to: {args.benchmark_dir / Path('llm_summary.json')}")

    if cache is not None:
        save_solver_cache(cache_path, cache)


def main():
    """Entry point."""
    try:
        run()
    except Exception as exc:  # noqa: BLE001
        print(exc, file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
