from unsloth import FastLanguageModel
import torch
import os

base_model_path = "/jfan5/sft_models/mistral_variant-blocksworld"
max_seq_length = 4096
device = "cuda"

# 1. åŠ è½½æ¨¡å‹ï¼ˆå’Œ SFT / DPO ä¸€è‡´ï¼‰
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=base_model_path,
    max_seq_length=max_seq_length,
    dtype=None,
    load_in_4bit=True,
    load_in_8bit=False,
)

FastLanguageModel.for_inference(model)   # ç”¨äºæ¨ç†æ¨¡å¼
model.to(device)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

prompts = [
    "Please explain what is reinforcement learning in one paragraph.",
    "å†™ä¸€æ®µå…³äºæ·±åº¦å¼ºåŒ–å­¦ä¹ å®‰å…¨æ€§çš„ç®€ä»‹ã€‚",
    "Given a sequence of states and actions, how to define the robustness of an STL formula?"
]

def generate_batch(model, tokenizer, prompts):
    inputs = tokenizer(prompts, return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=128,
            do_sample=False,   # ä¿è¯ç¡®å®šæ€§
            temperature=1.0,
        )
    return tokenizer.batch_decode(outputs, skip_special_tokens=True)

print("=== A: åŸå§‹æ¨¡å‹è¾“å‡º ===")
texts_A = generate_batch(model, tokenizer, prompts)
for i, t in enumerate(texts_A):
    print(f"[Prompt {i}]")
    print(t)
    print("-" * 80)


# ==================================================================
# ğŸŸ© å…³é”®æ”¹åŠ¨ï¼šä½¿ç”¨ merge_loraï¼Œè€Œä¸æ˜¯ save_pretrained_merged
# ==================================================================

print("Merging LoRA weights into the full model...")
FastLanguageModel.merge_lora(model)   # â­ å°† LoRA åˆå¹¶åˆ°æ¨¡å‹æœ¬ä½“ä¸­

# ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹ï¼ˆåƒ SFT ä¸€æ ·ï¼‰
save_dir = "/jfan5/test_sft_roundtrip"
os.makedirs(save_dir, exist_ok=True)

print(f"Saving merged full model to: {save_dir}")
model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

print("æ¨¡å‹ä¿å­˜å®Œæˆï¼ˆå·²æ˜¯å…¨æ¨¡å‹ï¼Œæ— éœ€ _merged ç›®å½•ï¼‰")


# ==================================================================
# ğŸŸ© é‡æ–°åŠ è½½å®Œæ•´æ¨¡å‹
# ==================================================================

reload_dir = save_dir
print(f"Reloading model from: {reload_dir}")

model_B, tokenizer_B = FastLanguageModel.from_pretrained(
    model_name=reload_dir,
    max_seq_length=max_seq_length,
    dtype=None,
    load_in_4bit=True,    # æ ¹æ®ä½ çš„å®é™…éœ€æ±‚ï¼Œå¯æ”¹ä¸º fp16 / bf16
    load_in_8bit=False,
)

FastLanguageModel.for_inference(model_B)
model_B.to(device)


print("=== B: é‡æ–°åŠ è½½åçš„æ¨¡å‹è¾“å‡º ===")
texts_B = generate_batch(model_B, tokenizer_B, prompts)
for i, t in enumerate(texts_B):
    print(f"[Prompt {i}]")
    print(t)
    print("-" * 80)


# ==================================================================
# ğŸŸ© å¯¹æ¯”å·®å¼‚
# ==================================================================
for i, (a, b) in enumerate(zip(texts_A, texts_B)):
    print(f"=== Prompt {i} å·®å¼‚ ===")
    print("A:", a[:400])
    print("B:", b[:400])
    print()
