{
  "bf16": true,
  "fp16": false,
  "fsdp": [],
  "seed": 3407,
  "tf32": null,
  "debug": [],
  "dtype": "bfloat16",
  "optim": "adamw_torch",
  "top_k": 50,
  "top_p": 1,
  "_wandb": {
    "e": {
      "0najg5nd05el8wbi7ke148dnid6f9tx6": {
        "os": "Linux-6.8.0-40-generic-x86_64-with-glibc2.35",
        "git": {
          "commit": "2c2bd628db8ae128e1822fa5feeaf13a65542494",
          "remote": "git@github.com:JFan5/Safety-gen.git"
        },
        "gpu": "NVIDIA H100 PCIe",
        "args": [
          "--mode",
          "train",
          "--model",
          "unsloth/gpt-oss-20b-unsloth-bnb-4bit",
          "--family",
          "gpt",
          "--dataset",
          "/jfan5/sft_data/pddl3_symbolized_four_scenarios/blocksworld.hf",
          "--output",
          "/jfan5/sft_models/gpt_oss_20b/blocksworld_pddl3_symbolized",
          "--num-train-epochs",
          "3",
          "--per-device-train-batch-size",
          "8",
          "--gradient-accumulation-steps",
          "4",
          "--learning-rate",
          "2e-4",
          "--max-seq-length",
          "4096",
          "--load-in-4bit",
          "--eval-strategy",
          "steps",
          "--save-strategy",
          "steps",
          "--eval-steps",
          "10",
          "--save-steps",
          "30",
          "--logging-steps",
          "10",
          "--save-total-limit",
          "2"
        ],
        "disk": {
          "/": {
            "used": "160742338560",
            "total": "311993479168"
          }
        },
        "host": "ambitious-tesla",
        "root": "/home/ubuntu/Safety-gen",
        "email": "fjl2401@gmail.com",
        "slurm": {
          "conf": "/etc/slurm/slurm.conf",
          "gtids": "0",
          "jobid": "249",
          "job_id": "249",
          "nnodes": "1",
          "nodeid": "0",
          "procid": "0",
          "job_gid": "1000",
          "job_uid": "1000",
          "localid": "0",
          "job_name": "finetune_gpt_oss_20b_blocksworld_sym",
          "job_user": "ubuntu",
          "nodelist": "ambitious-tesla",
          "task_pid": "23163",
          "submit_dir": "/home/ubuntu/Safety-gen",
          "submit_host": "ambitious-tesla",
          "cluster_name": "safety-gen",
          "cpus_on_node": "1",
          "job_nodelist": "ambitious-tesla",
          "mem_per_node": "181000",
          "node_aliases": "(null)",
          "prio_process": "0",
          "job_num_nodes": "1",
          "job_partition": "gpu",
          "topology_addr": "ambitious-tesla",
          "tasks_per_node": "1",
          "working_cluster": "safety-gen:ambitious-tesla:6817:9472:109",
          "job_cpus_per_node": "1",
          "topology_addr_pattern": "node"
        },
        "memory": {
          "total": "190128062464"
        },
        "python": "CPython 3.10.0",
        "program": "/home/ubuntu/Safety-gen/pddl_finetune.py",
        "codePath": "pddl_finetune.py",
        "writerId": "0najg5nd05el8wbi7ke148dnid6f9tx6",
        "cpu_count": 28,
        "gpu_count": 1,
        "startedAt": "2025-12-12T15:09:00.205500Z",
        "executable": "/home/ubuntu/miniconda3/envs/llmstl/bin/python3",
        "gpu_nvidia": [
          {
            "name": "NVIDIA H100 PCIe",
            "uuid": "GPU-d0e23bc7-e8b5-2dff-a2e7-119c21f8a7ac",
            "cudaCores": 14592,
            "memoryTotal": "85520809984",
            "architecture": "Hopper"
          }
        ],
        "cudaVersion": "12.2",
        "codePathLocal": "pddl_finetune.py",
        "cpu_count_logical": 28
      }
    },
    "m": [
      {
        "1": "train/global_step",
        "6": [
          3
        ],
        "7": []
      },
      {
        "2": "*",
        "5": 1,
        "6": [
          1
        ],
        "7": []
      }
    ],
    "t": {
      "1": [
        1,
        11,
        41,
        49,
        51,
        71,
        83,
        84,
        95,
        98
      ],
      "2": [
        1,
        11,
        41,
        49,
        51,
        71,
        83,
        84,
        95,
        98
      ],
      "3": [
        2,
        7,
        13,
        16,
        19,
        62,
        66
      ],
      "4": "3.10.0",
      "5": "0.22.2",
      "6": "4.56.2",
      "9": {
        "1": "transformers_trainer"
      },
      "12": "0.22.2",
      "13": "linux-x86_64"
    },
    "cli_version": "0.22.2",
    "python_version": "3.10.0"
  },
  "family": "gpt",
  "prefix": null,
  "do_eval": true,
  "no_cuda": false,
  "packing": false,
  "use_cpu": false,
  "do_train": false,
  "head_dim": 64,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "run_name": null,
  "use_ipex": false,
  "adafactor": false,
  "data_seed": null,
  "deepspeed": null,
  "do_sample": false,
  "eos_token": "<EOS_TOKEN>",
  "eval_size": 50,
  "hub_token": "<HUB_TOKEN>",
  "log_level": "passive",
  "loss_type": "nll",
  "max_steps": -1,
  "num_beams": 1,
  "pad_token": "<PAD_TOKEN>",
  "ray_scope": "last",
  "report_to": [
    "wandb"
  ],
  "typical_p": 1,
  "use_cache": true,
  "val_ratio": 0.05,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "do_predict": false,
  "eval_delay": 0,
  "eval_steps": 10,
  "hidden_act": "silu",
  "is_decoder": false,
  "local_rank": 0,
  "max_length": 1024,
  "min_length": 0,
  "model_name": "unsloth/gpt-oss-20b-unsloth-bnb-4bit",
  "model_type": "gpt_oss",
  "optim_args": null,
  "output_dir": "/jfan5/sft_models/gpt_oss_20b/blocksworld_pddl3_symbolized",
  "past_index": -1,
  "rope_theta": 150000,
  "save_steps": 30,
  "train_size": 950,
  "vocab_size": 201088,
  "ddp_backend": null,
  "ddp_timeout": 1800,
  "fsdp_config": {
    "xla": false,
    "xla_fsdp_v2": false,
    "min_num_params": 0,
    "xla_fsdp_grad_ckpt": false
  },
  "hidden_size": 2880,
  "label_names": null,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "logging_dir": "/jfan5/sft_models/gpt_oss_20b/blocksworld_pddl3_symbolized/runs/Dec12_15-09-01_ambitious-tesla",
  "peft_config": {
    "default": {
      "r": 32,
      "bias": "none",
      "revision": null,
      "use_dora": false,
      "lora_bias": false,
      "peft_type": "LORA",
      "task_type": "CAUSAL_LM",
      "eva_config": null,
      "lora_alpha": 64,
      "use_qalora": false,
      "use_rslora": false,
      "auto_mapping": {
        "unsloth_fixed": true,
        "parent_library": "transformers.models.gpt_oss.modeling_gpt_oss",
        "base_model_class": "GptOssForCausalLM"
      },
      "corda_config": null,
      "lora_dropout": 0.05,
      "megatron_core": "megatron.core",
      "fan_in_fan_out": false,
      "inference_mode": false,
      "layers_pattern": null,
      "runtime_config": {
        "ephemeral_gpu_offload": false
      },
      "target_modules": [
        "up_proj",
        "k_proj",
        "gate_proj",
        "down_proj",
        "o_proj",
        "q_proj",
        "v_proj"
      ],
      "exclude_modules": null,
      "megatron_config": null,
      "modules_to_save": null,
      "init_lora_weights": true,
      "layer_replication": null,
      "qalora_group_size": 16,
      "target_parameters": null,
      "layers_to_transform": null,
      "base_model_name_or_path": "unsloth/gpt-oss-20b-unsloth-bnb-4bit",
      "trainable_token_indices": null
    }
  },
  "push_to_hub": false,
  "return_dict": true,
  "temperature": 1,
  "torchdynamo": null,
  "torchscript": false,
  "adam_epsilon": 1e-08,
  "bos_token_id": 199998,
  "dataset_size": 1000,
  "disable_tqdm": false,
  "eos_token_id": 200002,
  "eval_packing": null,
  "fp16_backend": "auto",
  "hub_model_id": null,
  "hub_revision": null,
  "hub_strategy": "every_save",
  "load_in_4bit": true,
  "pad_token_id": 200002,
  "padding_free": false,
  "problem_type": null,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32,
    "truncate": false,
    "beta_fast": 32,
    "beta_slow": 1,
    "rope_type": "yarn",
    "original_max_position_embeddings": 4096
  },
  "sep_token_id": null,
  "swiglu_limit": 7,
  "use_bfloat16": false,
  "warmup_ratio": 0.1,
  "warmup_steps": 0,
  "weight_decay": 0.05,
  "_name_or_path": "unsloth/gpt-oss-20b-unsloth-bnb-4bit",
  "architectures": [
    "GptOssForCausalLM"
  ],
  "bad_words_ids": null,
  "eval_on_start": false,
  "eval_strategy": "steps",
  "jit_mode_eval": false,
  "learning_rate": 0.0002,
  "logging_steps": 10,
  "max_grad_norm": 1,
  "mp_parameters": "",
  "output_scores": false,
  "save_strategy": "steps",
  "torch_compile": false,
  "tpu_num_cores": null,
  "attention_bias": true,
  "bf16_full_eval": true,
  "dataset_kwargs": null,
  "early_stopping": false,
  "fp16_full_eval": false,
  "fp16_opt_level": "O1",
  "length_penalty": 1,
  "max_seq_length": 4096,
  "sliding_window": 128,
  "tf_legacy_loss": false,
  "use_mps_device": false,
  "finetuning_task": null,
  "group_by_length": false,
  "hub_always_push": false,
  "num_beam_groups": 1,
  "save_only_model": false,
  "suppress_tokens": null,
  "tokenizer_class": null,
  "unsloth_version": "2025.11.6",
  "dataset_num_proc": null,
  "full_determinism": false,
  "hub_private_repo": null,
  "ignore_data_skip": false,
  "log_on_each_node": true,
  "logging_strategy": "steps",
  "num_train_epochs": 3,
  "packing_strategy": "bfd",
  "save_safetensors": true,
  "save_total_limit": 2,
  "use_liger_kernel": false,
  "attention_dropout": 0,
  "ddp_bucket_cap_mb": null,
  "diversity_penalty": 0,
  "experts_per_token": 4,
  "greater_is_better": false,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "log_level_replica": "warning",
  "lr_scheduler_type": "cosine",
  "model_init_kwargs": null,
  "num_hidden_layers": 24,
  "num_local_experts": 32,
  "output_attentions": false,
  "push_to_hub_token": "<PUSH_TO_HUB_TOKEN>",
  "save_on_each_node": false,
  "tpu_metrics_debug": false,
  "accelerator_config": {
    "even_batches": true,
    "non_blocking": false,
    "split_batches": false,
    "dispatch_batches": null,
    "use_seedable_sampler": true,
    "gradient_accumulation_kwargs": null
  },
  "batch_eval_metrics": false,
  "chat_template_path": null,
  "dataset_text_field": "text",
  "is_encoder_decoder": false,
  "length_column_name": "length",
  "logging_first_step": false,
  "pad_to_multiple_of": null,
  "parallelism_config": null,
  "repetition_penalty": 1,
  "torch_compile_mode": null,
  "add_cross_attention": false,
  "assistant_only_loss": false,
  "forced_bos_token_id": null,
  "forced_eos_token_id": null,
  "fsdp_min_num_params": 0,
  "include_for_metrics": [],
  "liger_kernel_config": null,
  "neftune_noise_alpha": null,
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_key_value_heads": 8,
  "quantization_config": {
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes",
    "llm_int8_threshold": 6,
    "bnb_4bit_quant_type": "nf4",
    "llm_int8_skip_modules": null,
    "bnb_4bit_compute_dtype": "bfloat16",
    "llm_int8_has_fp16_weight": false,
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false
  },
  "skip_memory_metrics": true,
  "tie_encoder_decoder": false,
  "tie_word_embeddings": false,
  "auto_find_batch_size": false,
  "completion_only_loss": null,
  "dataloader_drop_last": false,
  "model/num_parameters": 20930682432,
  "no_repeat_ngram_size": 0,
  "num_return_sequences": 1,
  "optim_target_modules": null,
  "output_hidden_states": false,
  "output_router_logits": false,
  "overwrite_output_dir": false,
  "prediction_loss_only": false,
  "push_to_hub_model_id": null,
  "router_aux_loss_coef": 0.9,
  "task_specific_params": null,
  "transformers_version": "4.56.2",
  "activation_offloading": false,
  "begin_suppress_tokens": null,
  "dataloader_pin_memory": true,
  "ddp_broadcast_buffers": null,
  "metric_for_best_model": "eval_loss",
  "remove_invalid_values": false,
  "remove_unused_columns": true,
  "torch_compile_backend": null,
  "dataloader_num_workers": 0,
  "decoder_start_token_id": null,
  "eval_do_concat_batches": true,
  "eval_use_gather_object": false,
  "gradient_checkpointing": false,
  "half_precision_backend": "auto",
  "initial_context_length": 4096,
  "label_smoothing_factor": 0,
  "load_best_model_at_end": true,
  "logging_nan_inf_filter": true,
  "resume_from_checkpoint": null,
  "chunk_size_feed_forward": 0,
  "eval_accumulation_steps": 4,
  "max_position_embeddings": 131072,
  "per_gpu_eval_batch_size": null,
  "return_dict_in_generate": false,
  "torch_empty_cache_steps": null,
  "per_gpu_train_batch_size": null,
  "push_to_hub_organization": null,
  "include_tokens_per_second": false,
  "dataloader_prefetch_factor": null,
  "ddp_find_unused_parameters": null,
  "include_inputs_for_metrics": false,
  "per_device_eval_batch_size": 8,
  "use_legacy_prediction_loop": false,
  "cross_attention_hidden_size": null,
  "gradient_accumulation_steps": 4,
  "per_device_train_batch_size": 8,
  "encoder_no_repeat_ngram_size": 0,
  "average_tokens_across_devices": false,
  "dataloader_persistent_workers": false,
  "gradient_checkpointing_kwargs": null,
  "include_num_input_tokens_seen": false,
  "exponential_decay_length_penalty": null,
  "fsdp_transformer_layer_cls_to_wrap": null,
  "restore_callback_states_from_checkpoint": false
}