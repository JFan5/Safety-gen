\begin{abstract}
    Safety-critical task planning in robotic systems remains challenging: classical planners suffer from poor scalability, reinforcement learning (RL)-based methods generalize poorly, and base large language models (LLMs) cannot guarantee safety. We construct a multi-domain Planning Domain Definition Language 3 (PDDL3) benchmark with explicit safety constraints and introduce a two-stage post-training framework: Supervised Fine-Tuning (SFT) on a constraint-compliant planning dataset to learn planning syntax and semantics, followed by Group Relative Policy Optimization (GRPO) guided by fine-grained reward machines derived from formal verification to enforce safety alignment. With curriculum learning for training stability, our approach achieves strong safety generalization across multi-domain planning tasks and outperforms frontier proprietary baselines.
\end{abstract}
