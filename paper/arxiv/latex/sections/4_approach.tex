


\section{SafeGen-LLM}\label{IV}

Our framework consists of three stages: dataset construction (Section~\ref{subsec:dataset}), SFT (Section~\ref{subsec:sft}), and online RL via GRPO (Section~\ref{subsec:grpo}).

\subsection{Dataset Construction}\label{subsec:dataset}

We construct a unified safety-oriented dataset that is used for both SFT and as the problem pool for GRPO training.
The construction process involves three steps: domain selection and safety knowledge design, constrained problem generation and solving, and instruction--response formatting.

\noindent\textbf{Domain-specific safety knowledge design.}
We start by selecting four task-planning domains from the open-source PDDL2 problem generators~\cite{seipp2022pddl}.
The domains are chosen using the following criteria:
\begin{itemize}
  \item relevance to real-world robotic task planning;
  \item presence of safety-critical objects, locations, or actions;
  \item availability of problem instances with varying difficulty.
\end{itemize}
Based on these criteria, we select four representative domains: Blocksworld, Ferry, Grippers, and Spanner.
A brief overview of each domain and its associated safety constraints is given in Table~\ref{tab:domain-introduction}.
For each domain, we design additional domain-specific safety knowledge in the form of high-level constraints that mirror realistic robotic requirements (e.g., collision avoidance, load limits, safe ordering of operations).
These constraints are subsequently encoded in PDDL3 \texttt{:constraints} format and used to generate safety-compliant plan demonstrations.

\noindent\textbf{Dataset construction pipeline.}
The pipeline for constructing the dataset is illustrated in Figure~\ref{fig:dataset-construction}.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{sft_diagram.pdf}
  \caption{Pipeline for dataset construction.}
  \label{fig:dataset-construction}
\end{figure}

First, we generate planning problems for each domain using the PDDL2 problem generators.
We remove isomorphic or trivially equivalent problems to reduce redundancy and employ a classical planner to ensure that the remaining problems are feasible.

Second, we encode the domain-specific safety constraints from Table~\ref{tab:domain-introduction} into PDDL3 \texttt{:constraints}.
We then use the temporal PDDL3 planner OPTIC~\cite{benton2012temporal} to solve the resulting constrained planning problems, and verify each candidate solution using the VAL tool~\cite{howey2004val}.
Only solutions that are successfully validated by VAL (i.e., respect both domain preconditions and encoded safety constraints) are retained.

Third, we convert each verified solution into an instruction--response pair.
The instruction consists of a natural-language prompt that includes the planning domain, problem instance, and (when applicable) safety constraints; the response is the corresponding validated plan.
Each problem is wrapped in a fixed instruction template that specifies the task format and expected output structure, as shown below.

\begin{userbox}
  You are a planning expert. Your task is to generate a \textbf{valid plan} for the given domain and problem.

  \texttt{DOMAIN:}
  \{\{domain\_content\}\}

  \texttt{PROBLEM:}
  \{\{problem\_content\}\}

  \textbf{Output Requirements:}
  \begin{itemize}
    \item Return \textbf{ONLY} the plan steps, one per line.
    \item Each line must follow the format: \texttt{(<ACTION\_NAME> <param1> <param2> ...)}.
    \item Use only objects defined in the \texttt{PROBLEM}.
    \item Do \textbf{NOT} include any explanations, comments, or headers.
    \item Do \textbf{NOT} output anything except the plan lines.
    \item The output must \textbf{NOT} contain natural language sentences.
    \item If the \texttt{PROBLEM} includes constraints, the plan must satisfy all of them; otherwise, solve as a standard goal-directed task.
    \item Ensure that all action preconditions hold and no constraints or invariants are violated at any step.
  \end{itemize}

  \textbf{Plan:}
\end{userbox}

This template explicitly instructs the model to behave as a planning expert and to produce strictly formatted action sequences.
By providing the domain and problem specifications in a structured form and enforcing strong output requirements (no extra text, strict syntax, constraint satisfaction), the template helps ensure that the generated plans remain syntactically correct, executable, and consistent with the safety constraints.

The resulting instruction--response pairs form the supervised dataset $\mathcal{D}_{\text{SFT}} = \{(x_i, y_i)\}_{i=1}^{N}$.
For SFT, these pairs are used directly as training examples.
For GRPO, the same problems serve as training prompts---the model generates its own candidate plans online---and the validated reference solutions $y_i$ are retained to compute the reference solution length $L_{\text{ref}}$ used in the progress-based reward function (Section~\ref{subsubsec:reward}).

\subsection{Supervised Fine-Tuning}\label{subsec:sft}

SFT adapts a pre-trained LLM to a specialized task by training on
task-specific instruction--response pairs.
Given a dataset $\mathcal{D}_{\text{SFT}}=\{(x_i, y_i)\}_{i=1}^{N}$, where $x_i$ is an
input prompt and $y_i$ is the target response, SFT optimizes the model parameters
$\theta$ by minimizing the negative log-likelihood:
\begin{equation}
\mathcal{L}_{\text{SFT}}(\theta)=
- \mathbb{E}_{(x,y)\sim\mathcal{D}_{\text{SFT}}}
\left[ \log \pi_{\theta}(y\mid x) \right],
\label{eq:sft_loss}
\end{equation}
where $\pi_{\theta}(y\mid x)$ denotes the probability of producing $y$ given input $x$.

In our framework, SFT serves to:
(i) encode domain and safety knowledge,
(ii) enforce syntactic and semantic plan validity, and
(iii) provide a strong initialization for subsequent RL via GRPO.

Given the constructed dataset $\mathcal{D}_{\text{SFT}}$ from Section~\ref{subsec:dataset}, we fine-tune the LLM using the standard SFT objective (Eq.~\ref{eq:sft_loss}).
By minimizing this objective, SFT encourages the model to reproduce planning behaviors that are syntactically valid, executable, and safety-compliant.
Beyond learning \emph{what} plan to output, SFT also enforces \emph{how} to output it, such as adhering to strict action syntax and avoiding natural-language explanations.
The resulting SFT model provides a capable initialization for the subsequent GRPO stage, which further refines safety compliance through online RL.


\subsection{Group Relative Policy Optimization}\label{subsec:grpo}

GRPO~\cite{shao2024deepseekmath} is an online RL algorithm designed to optimize language models using
verifiable, programmatic reward signals.
Given a prompt $x$, GRPO samples a group of $K$ candidate responses
$\{y_1, \dots, y_K\}$ from the current policy $\pi_\theta(\cdot \mid x)$.
Each response is evaluated by a reward function $r(x, y_i)$, and policy updates are
performed by comparing responses \emph{within the same group} rather than relying on
absolute reward values.

Specifically, GRPO defines a relative advantage for each response by normalizing
rewards within the group:
\begin{equation}
  A_i = r(x, y_i) - \frac{1}{K} \sum_{j=1}^{K} r(x, y_j),
\end{equation}
and optimizes the policy by maximizing:
\begin{equation}
  \mathcal{L}_{\mathrm{GRPO}}(\theta)
  = \mathbb{E}_{x,\, y_i \sim \pi_\theta}
  \big[ A_i \log \pi_\theta(y_i \mid x) \big].
\end{equation}

We adopt GRPO as the RL algorithm because it naturally integrates with our verifiable reward signal derived from the plan validator, while being significantly more lightweight than PPO by eliminating the need for a separate critic network.
Starting from the SFT-trained policy, GRPO encourages the model to generate safer and more successful plans through group-relative advantage estimation.
For each prompt, GRPO samples $K$ candidate plans from the current policy and evaluates each with a programmatic reward function.
We next describe the two key components of our GRPO training: the reward function that provides the learning signal, and the curriculum strategy that controls training difficulty.

\subsubsection{Reward design.}\label{subsubsec:reward}
We design a dense, hierarchical reward function based on automated plan verification using the VAL tool~\cite{howey2004val}.
For each generated plan, the validation pipeline classifies it into one of five ordered categories $\mathcal{C} = \{c_1, c_2, c_3, c_4, c_5\}$:
\begin{itemize}
  \item $c_1$: \textbf{Plan Format Error} --- syntactically invalid or unparseable;
  \item $c_2$: \textbf{Safety Constraint Violation} --- violates at least one PDDL3 safety constraint;
  \item $c_3$: \textbf{Precondition Violation} --- one or more action preconditions fail during execution;
  \item $c_4$: \textbf{Goal Not Satisfied} --- executes safely but fails to achieve the goal;
  \item $c_5$: \textbf{Success Plan} --- satisfies all safety constraints and achieves the goal.
\end{itemize}

\noindent\textbf{Hierarchical reward with progress-based interpolation.}
We assign each category $c_k$ a reward interval $[r_k^{-}, r_k^{+}]$ satisfying the \emph{strict separation} constraint:
\begin{equation}\label{eq:separation}
  r_1 \;\leq\; r_2^{-} \;<\; r_2^{+} \;\leq\; r_3^{-} \;<\; r_3^{+} \;\leq\; r_4^{-} \;<\; r_4^{+} \;\leq\; r_5,
\end{equation}
which guarantees that any plan in a more severe failure category always receives a lower reward than any plan in a less severe category, regardless of within-category progress.

For the two anchor categories, the reward is fixed: $r(x,y) = r_5$ for success and $r(x,y) = r_1$ for format errors.
For intermediate failure categories $c_k$ ($k \in \{2,3,4\}$), we interpolate using a \emph{progress function} $\rho_k(x, y) \in [0, 1]$:
\begin{equation}\label{eq:reward}
  r(x, y) = r_k^{-} + (r_k^{+} - r_k^{-}) \cdot \rho_k(x, y).
\end{equation}

The progress function is defined according to the failure type:
\begin{equation}\label{eq:progress}
\rho_k(x, y) =
\begin{cases}
  t_v / L_{\text{ref}} & k \in \{2, 3\}, \\[4pt]
  n_{\text{sat}} / n_{\text{total}} & k = 4,
\end{cases}
\end{equation}
where $t_v$ is the plan step at which the first violation occurs, $L_{\text{ref}}$ is the length of the ground-truth reference solution, and $n_{\text{sat}} / n_{\text{total}}$ is the fraction of satisfied goal predicates.

\noindent\textbf{Design rationale.}
Three principles guide the reward design.
\emph{First}, the strict separation (Eq.~\ref{eq:separation}) encodes a severity hierarchy (safety $>$ precondition $>$ goal), ensuring that the model learns to prioritize constraint compliance over goal achievement.
We rank safety violations as most severe because they correspond to explicitly specified domain-level invariants; precondition violations indicate local action misuse but do not necessarily imply unsafe states.
\emph{Second}, the progress-based interpolation (Eq.~\ref{eq:reward}) provides dense gradient signals within each failure category, encouraging the model to generate progressively longer valid prefixes rather than receiving a single uninformative penalty.
\emph{Third}, the denominator $L_{\text{ref}}$ in Eq.~\ref{eq:progress} uses the reference solution length rather than the length of the generated plan.
This reference-anchored normalization prevents \textbf{reward hacking}: without it, a model could inflate its progress ratio by generating shorter (and likely incorrect) plans.

\subsubsection{Curriculum learning.}\label{subsubsec:curriculum}
To improve training stability and sample efficiency, we adopt a curriculum learning strategy that progressively increases problem difficulty during GRPO training.

\noindent\textbf{Difficulty scoring.}
We define domain-specific difficulty scores based on structural parameters that correlate with planning complexity: $d = n^2$ for Blocksworld ($n$ blocks), $d = l \times c$ for Ferry ($l$ locations, $c$ cars), $d = n \times r \times o$ for Grippers ($n$ robots, $r$ rooms, $o$ objects), and $d = s \times n \times l$ for Spanner ($s$ spanners, $n$ nuts, $l$ locations).

\noindent\textbf{Three-level bucketing.}
Within each domain, problems are sorted by difficulty score and partitioned into three buckets using percentile thresholds: \emph{Easy} ($\leq$ 40th percentile), \emph{Medium} (40th--80th percentile), and \emph{Hard} ($>$ 80th percentile).
Bucketing is performed per domain since raw scores are not comparable across domains.

\noindent\textbf{Phased training schedule.}
Training is divided into three phases---early, mid, and late---with shifting sampling probabilities that progressively increase the proportion of harder problems.
The early phase builds foundational skills on easy problems; the mid phase transitions to a balanced distribution; the late phase emphasizes hard problems requiring complex multi-constraint reasoning.

\noindent\textbf{Domain-balanced batching.}
Each training batch contains an equal number of samples from every domain, preventing overfitting to any single domain and ensuring balanced exposure at every training step.

The combination of difficulty-based curriculum and domain-balanced batching enables GRPO to efficiently explore the solution space: easy problems build syntax and sequencing foundations, while progressively harder problems introduce complex constraint interactions that require deeper safety reasoning.
