


\section{SafeGen-LLM}\label{IV}

Our framework consists of three stages: dataset construction (Section~\ref{subsec:dataset}), SFT (Section~\ref{subsec:sft}), and online RL via GRPO (Section~\ref{subsec:grpo}).

\subsection{Dataset Construction}\label{subsec:dataset}

We construct a unified safety-oriented dataset that is used for both SFT and as the problem pool for GRPO training.
The construction process involves three steps: domain selection and safety knowledge design, constrained problem generation and solving, and instruction--response formatting.

\noindent\textbf{Domain-specific safety knowledge design.}
We start by selecting four task-planning domains from the open-source PDDL2 problem generators~\cite{seipp2022pddl}.
The domains are chosen using the following criteria:
\begin{itemize}
  \item relevance to real-world robotic task planning;
  \item presence of safety-critical objects, locations, or actions;
  \item availability of problem instances with varying difficulty.
\end{itemize}
Based on these criteria, we select four representative domains: Blocksworld, Ferry, Grippers, and Spanner.
A brief overview of each domain and its associated safety constraints is given in Table~\ref{tab:domain-introduction}.
For each domain, we design additional domain-specific safety knowledge in the form of high-level constraints that mirror realistic robotic requirements (e.g., collision avoidance, load limits, safe ordering of operations).
These constraints are subsequently encoded in PDDL3 \texttt{:constraints} format and used to generate safety-compliant plan demonstrations.

\noindent\textbf{Dataset construction pipeline.}
The pipeline for constructing the dataset is illustrated in Figure~\ref{fig:dataset-construction}.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{sft_diagram.pdf}
  \caption{Pipeline for dataset construction.}
  \label{fig:dataset-construction}
\end{figure}

First, we generate planning problems for each domain using the PDDL2 problem generators.
We remove isomorphic or trivially equivalent problems to reduce redundancy and employ a classical planner to ensure that the remaining problems are feasible.

Second, we encode the domain-specific safety constraints from Table~\ref{tab:domain-introduction} into PDDL3 \texttt{:constraints}.
We then use the temporal PDDL3 planner OPTIC~\cite{benton2012temporal} to solve the resulting constrained planning problems, and verify each candidate solution using the VAL tool~\cite{howey2004val}.
Only solutions that are successfully validated by VAL (i.e., respect both domain preconditions and encoded safety constraints) are retained.

Third, we convert each verified solution into an instruction--response pair.
The instruction consists of a natural-language prompt that includes the planning domain, problem instance, and (when applicable) safety constraints; the response is the corresponding validated plan.
Formally, we obtain a supervised dataset
\[
\mathcal{D}_{\text{SFT}} = \{(x_i, y_i)\}_{i=1}^{N},
\]
where $x_i$ is an input prompt and $y_i$ is the desired output plan.
For SFT, the instruction--response pairs are used directly as training examples.
For GRPO, the same problems serve as training prompts---the model generates its own candidate plans online---and the validated reference solutions $y_i$ are retained to compute the reference solution length $L_{\text{ref}}$ used in the progress-based reward function (Section~\ref{subsubsec:reward}).

To enhance data diversity and reduce overfitting, we design ten instruction templates.
For each problem, we randomly sample a fixed number of templates from this pool, which encourages variation in surface forms and prevents the model from simply memorizing a single prompt pattern.
A representative template is shown below.

\begin{userbox}
  You are a planning expert. Your task is to generate a \textbf{valid plan} for the given domain and problem.

  \texttt{DOMAIN:}
  \{\{domain\_content\}\}

  \texttt{PROBLEM:}
  \{\{problem\_content\}\}

  \textbf{Output Requirements:}
  \begin{itemize}
    \item Return \textbf{ONLY} the plan steps, one per line.
    \item Each line must follow the format: \texttt{(<ACTION\_NAME> <param1> <param2> ...)}.
    \item Use only objects defined in the \texttt{PROBLEM}.
    \item Do \textbf{NOT} include any explanations, comments, or headers.
    \item Do \textbf{NOT} output anything except the plan lines.
    \item The output must \textbf{NOT} contain natural language sentences.
    \item If the \texttt{PROBLEM} includes constraints, the plan must satisfy all of them; otherwise, solve as a standard goal-directed task.
    \item Ensure that all action preconditions hold and no constraints or invariants are violated at any step.
  \end{itemize}

  \textbf{Plan:}
\end{userbox}

This template explicitly instructs the model to behave as a planning expert and to produce strictly formatted action sequences.
By providing the domain and problem specifications in a structured form and enforcing strong output requirements (no extra text, strict syntax, constraint satisfaction), the template helps ensure that the generated plans remain syntactically correct, executable, and consistent with the safety constraints.

\subsection{Supervised Fine-Tuning}\label{subsec:sft}

SFT adapts a pre-trained LLM to a specialized task by training on
task-specific instruction--response pairs.
Given a dataset $\mathcal{D}_{\text{SFT}}=\{(x_i, y_i)\}_{i=1}^{N}$, where $x_i$ is an
input prompt and $y_i$ is the target response, SFT optimizes the model parameters
$\theta$ by minimizing the negative log-likelihood:
\begin{equation}
\mathcal{L}_{\text{SFT}}(\theta)=
- \mathbb{E}_{(x,y)\sim\mathcal{D}_{\text{SFT}}}
\left[ \log \pi_{\theta}(y\mid x) \right],
\label{eq:sft_loss}
\end{equation}
where $\pi_{\theta}(y\mid x)$ denotes the probability of producing $y$ given input $x$.

In our framework, SFT serves to:
(i) encode domain and safety knowledge,
(ii) enforce syntactic and semantic plan validity, and
(iii) provide a strong initialization for subsequent RL via GRPO.

Given the constructed dataset $\mathcal{D}_{\text{SFT}}$ from Section~\ref{subsec:dataset}, we fine-tune the LLM using the standard SFT objective (Eq.~\ref{eq:sft_loss}).
By minimizing this objective, SFT encourages the model to reproduce planning behaviors that are syntactically valid, executable, and safety-compliant.
Beyond learning \emph{what} plan to output, SFT also enforces \emph{how} to output it, such as adhering to strict action syntax and avoiding natural-language explanations.
The resulting SFT model provides a capable initialization for the subsequent GRPO stage, which further refines safety compliance through online RL.


\subsection{GRPO}\label{subsec:grpo}

GRPO~\cite{shao2024deepseekmath} is an online RL algorithm designed to optimize language models using
verifiable, programmatic reward signals.
Given a prompt $x$, GRPO samples a group of $K$ candidate responses
$\{y_1, \dots, y_K\}$ from the current policy $\pi_\theta(\cdot \mid x)$.
Each response is evaluated by a reward function $r(x, y_i)$, and policy updates are
performed by comparing responses \emph{within the same group} rather than relying on
absolute reward values.

Specifically, GRPO defines a relative advantage for each response by normalizing
rewards within the group:
\begin{equation}
  A_i = r(x, y_i) - \frac{1}{K} \sum_{j=1}^{K} r(x, y_j),
\end{equation}
and optimizes the policy by maximizing:
\begin{equation}
  \mathcal{L}_{\mathrm{GRPO}}(\theta)
  = \mathbb{E}_{x,\, y_i \sim \pi_\theta}
  \big[ A_i \log \pi_\theta(y_i \mid x) \big].
\end{equation}

We adopt GRPO as the RL algorithm because it naturally integrates with our verifiable reward signal derived from the plan validator, while being significantly more lightweight than PPO by eliminating the need for a separate critic network.
Starting from the SFT-trained policy, GRPO encourages the model to generate safer and more successful plans through group-relative advantage estimation.
For each prompt, GRPO samples $K$ candidate plans from the current policy and evaluates each with a programmatic reward function.
We next describe the two key components of our GRPO training: the reward function that provides the learning signal, and the curriculum strategy that controls training difficulty.

\subsubsection{Reward design.}\label{subsubsec:reward}
We design a dense, hierarchical reward function based on automated plan verification using the VAL tool~\cite{howey2004val}.
For each generated plan, the validation pipeline classifies it into one of five ordered categories $\mathcal{C} = \{c_1, c_2, c_3, c_4, c_5\}$:
\begin{itemize}
  \item $c_1$: \textbf{Plan Format Error} --- syntactically invalid or unparseable;
  \item $c_2$: \textbf{Safety Constraint Violation} --- violates at least one PDDL3 safety constraint;
  \item $c_3$: \textbf{Precondition Violation} --- one or more action preconditions fail during execution;
  \item $c_4$: \textbf{Goal Not Satisfied} --- executes safely but fails to achieve the goal;
  \item $c_5$: \textbf{Success Plan} --- satisfies all safety constraints and achieves the goal.
\end{itemize}

\noindent\textbf{Hierarchical reward with progress-based interpolation.}
We assign each category $c_k$ a reward interval $[r_k^{-}, r_k^{+}]$ satisfying the \emph{strict separation} constraint:
\begin{equation}\label{eq:separation}
  r_1 \;\leq\; r_2^{-} \;<\; r_2^{+} \;\leq\; r_3^{-} \;<\; r_3^{+} \;\leq\; r_4^{-} \;<\; r_4^{+} \;\leq\; r_5,
\end{equation}
which guarantees that any plan in a more severe failure category always receives a lower reward than any plan in a less severe category, regardless of within-category progress.

For the two anchor categories, the reward is fixed: $r(x,y) = r_5$ for success and $r(x,y) = r_1$ for format errors.
For intermediate failure categories $c_k$ ($k \in \{2,3,4\}$), we interpolate using a \emph{progress function} $\rho_k(x, y) \in [0, 1]$:
\begin{equation}\label{eq:reward}
  r(x, y) = r_k^{-} + (r_k^{+} - r_k^{-}) \cdot \rho_k(x, y).
\end{equation}

The progress function is defined according to the failure type:
\begin{equation}\label{eq:progress}
\rho_k(x, y) =
\begin{cases}
  t_v / L_{\text{ref}} & k \in \{2, 3\}, \\[4pt]
  n_{\text{sat}} / n_{\text{total}} & k = 4,
\end{cases}
\end{equation}
where $t_v$ is the plan step at which the first violation occurs, $L_{\text{ref}}$ is the length of the ground-truth reference solution, and $n_{\text{sat}} / n_{\text{total}}$ is the fraction of satisfied goal predicates.

\noindent\textbf{Design rationale.}
Three principles guide the reward design.
\emph{First}, the strict separation (Eq.~\ref{eq:separation}) encodes a severity hierarchy (safety $>$ precondition $>$ goal), ensuring that the model learns to prioritize constraint compliance over goal achievement.
We rank safety violations as most severe because they correspond to explicitly specified domain-level invariants; precondition violations indicate local action misuse but do not necessarily imply unsafe states.
\emph{Second}, the progress-based interpolation (Eq.~\ref{eq:reward}) provides dense gradient signals within each failure category, encouraging the model to generate progressively longer valid prefixes rather than receiving a single uninformative penalty.
\emph{Third}, the denominator $L_{\text{ref}}$ in Eq.~\ref{eq:progress} uses the reference solution length rather than the length of the generated plan.
This reference-anchored normalization prevents \textbf{reward hacking}: without it, a model could inflate its progress ratio by generating shorter (and likely incorrect) plans.

\subsubsection{Curriculum learning.}\label{subsubsec:curriculum}
To improve training stability and sample efficiency, we adopt a curriculum learning strategy that progressively increases problem difficulty during GRPO training.

\noindent\textbf{Difficulty scoring.}
We define domain-specific difficulty scores based on problem parameters that correlate with planning complexity.
For each domain, we extract structural parameters from the problem filename and compute a scalar difficulty score:
\begin{itemize}
  \item \textbf{Blocksworld}: $d = n^2$, where $n$ is the number of blocks.
  \item \textbf{Ferry}: $d = l \times c$, where $l$ is the number of locations and $c$ is the number of cars.
  \item \textbf{Grippers}: $d = n \times r \times o$, where $n$ is the number of robots, $r$ is the number of rooms, and $o$ is the number of objects.
  \item \textbf{Spanner}: $d = s \times n \times l$, where $s$ is the number of spanners, $n$ is the number of nuts, and $l$ is the number of locations.
\end{itemize}

\noindent\textbf{Three-level bucketing.}
Within each domain, problems are sorted by difficulty score and partitioned into three buckets using percentile thresholds: \emph{Easy} (score $\leq$ 40th percentile), \emph{Medium} (40th--80th percentile), and \emph{Hard} ($>$ 80th percentile).
Bucketing is performed per domain to ensure that each domain has a balanced distribution of difficulty levels, since raw scores are not comparable across domains.

\noindent\textbf{Phased training schedule.}
Training is divided into three phases---early, mid, and late---with shifting sampling probabilities that progressively increase the proportion of harder problems.
In the early phase, the model predominantly trains on easy problems to build foundational skills such as correct action syntax, basic sequencing, and simple constraint satisfaction.
The mid phase transitions to a more balanced distribution, exposing the model to a wider range of constraint complexity.
In the late phase, hard problems dominate, requiring the model to handle complex multi-constraint interactions and longer planning horizons.

\noindent\textbf{Domain-balanced batching.}
To prevent the model from overfitting to any single domain, each training batch contains an equal number of samples from every domain.
Concretely, given a batch size $B$ and $K{=}4$ generations per prompt, the number of prompts per step is $B/K$, and each domain contributes exactly $(B/K)/|\mathcal{D}|$ prompts, where $|\mathcal{D}|$ is the number of domains.
This constraint ensures that the model receives balanced domain exposure at every training step, which is critical for cross-domain generalization.

The combination of difficulty-based curriculum and domain-balanced batching enables GRPO to efficiently explore the solution space: easy problems build syntax and sequencing foundations, while progressively harder problems introduce complex constraint interactions that require deeper safety reasoning.
