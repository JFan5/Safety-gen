\section{Introduction}


Cyber-Physical Systems (CPS) tightly integrate computation, communication, and physical processes, and are widely deployed in safety-critical domains such as autonomous driving, industrial automation, and robotics. Unlike conventional computing systems, CPS interact directly with the physical world, where unsafe decisions may lead to irreversible consequences. For instance, in autonomous driving, a planning error can result in collisions; in industrial automation, unsafe operations may damage equipment or harm workers. These examples highlight that CPS task planning must go beyond efficiency and task completion: it must ensure \emph{verifiable safety} under diverse and dynamic operating conditions.

Task planning is a core capability for CPS, endowing agents with the ability to organize and execute long-horizon tasks in constrained environments. Traditional task planners are predominantly search-based and operate on formal models expressed in the Planning Domain Definition Language (PDDL). Planners such as Fast Downward~\cite{helmert2006fast} and Metric-FF~\cite{hoffmann2001ff} employ heuristic search over symbolic state spaces to generate plans that can be verified against the underlying model. However, their performance heavily depends on hand-crafted heuristics and carefully engineered domain representations. When additional factors such as resource limits or safety constraints are introduced, these heuristics and search operators may no longer capture the relevant problem dynamics, leading to degraded performance or infeasible plans. Moreover, classical planners assume precise, fully known, and deterministic models, making them brittle and expensive to maintain in realistic CPS settings.

Learning-based planners~\cite{wang2022ensuring,yu2021learning} attempt to alleviate these issues by using deep or reinforcement learning to learn heuristics or policies directly from data, enabling better adaptation to complex dynamics and noisy observations. Such methods can, in principle, incorporate safety constraints into the learning objective and generate safety-aware policies. In practice, however, they are typically trained for a specific domain, require large amounts of task-specific data, and often lack formal safety guarantees. As a result, they struggle to generalize to new domains or novel tasks with different safety requirements, limiting their applicability in mission-critical CPS.

Recently, large language models (LLMs) have emerged as powerful general-purpose reasoning engines capable of capturing knowledge, following instructions, and generalizing across domains~\cite{cao2025large,plaat2024reasoning,liang2025ai}. Early studies show that LLMs can generate plausible plans from natural-language or symbolic inputs~\cite{yang2022automaton}, or translate instructions into temporal logic specifications that can be consumed by symbolic planners~\cite{pan2023data,van2024vernacopter}. Other works demonstrate that frozen LLMs can directly solve PDDL planning tasks under few-shot prompting~\cite{silver2024generalized}, suggesting their potential as a foundation for CPS task planning by bridging adaptability and generality.

However, directly applying LLMs to safety-critical CPS is far from straightforward. First, off-the-shelf LLMs often lack domain-specific knowledge needed to fully understand CPS operational contexts, making it difficult to generate feasible, executable plans. Second, without an explicit notion of safety, LLMs may produce plans that achieve task goals while violating safety constraints, potentially leading to hazardous behaviors in real-world deployments. Third, even with domain-specific fine-tuning on safety-constrained data, existing approaches typically operate in a single-domain setting and provide limited \emph{cross-problem} and \emph{cross-domain} safety generalization: they do not reliably satisfy \emph{unseen} safety constraints in \emph{unseen} domains.

To address these challenges, we propose \emph{Safe-Gen LLM}, a fine-tuning framework that enables Large Language Models to perform safety-aware task planning in CPS by incorporating verifiable safety knowledge into the training process. As illustrated in Figure~\ref{fig:safe-gen-llm}, the framework consists of two stages. First, we perform Supervised Fine-Tuning (SFT) on plans that are formally verified under PDDL3 safety constraints, injecting task-level and problem-level safety knowledge and enforcing strict output formats. Second, we apply Direct Preference Optimization (DPO) to further align the model’s behavior with safety-critical preferences derived from automated verification, explicitly distinguishing safe plans from unsafe or infeasible ones. Experimental results show that the resulting models achieve improved safety compliance and generalization in zero-shot CPS task planning, both across problems within a domain and across domains with different safety requirements.

\begin{figure*}[htbp!]
  \centering
  \includegraphics[width=0.9\textwidth]{diagram.pdf}
  \caption{Overview of the proposed Safe-Gen LLM framework. The first stage performs SFT on formally verified, safety-constrained plans. The second stage applies DPO using preference data derived from verification outcomes to further align the model with safety-critical behavior.}
  \label{fig:safe-gen-llm}
\end{figure*}

Our main contributions are summarized as follows:
\begin{itemize}
    \item \textbf{Safety-constrained CPS planning benchmark.}
    We construct a benchmark dataset covering multiple CPS-inspired task-planning domains, where each problem instance is equipped with explicitly defined PDDL3 safety constraints. All reference solutions are obtained using classical planners and verified by an independent validator to ensure compliance with the specified safety requirements.

    \item \textbf{Safe-Gen LLM: a two-stage safety-aligned fine-tuning framework.}
    We develop a two-stage fine-tuning framework that combines SFT and DPO to adapt LLMs for safety-aware planning. SFT injects task- and domain-level safety knowledge from verified plans, while DPO optimizes the model’s behavior toward safety-aligned decision-making by leveraging preference pairs that contrast safe and unsafe plans.

    \item \textbf{Safety-generalizable LLM-based planning for CPS.}
    Through extensive experiments, we show that the fine-tuned models effectively solve unseen problems within existing domains (cross-problem safety generalization) and generalize to an unseen domain with new safety constraints (cross-domain safety generalization), while maintaining safety compliance. Moreover, the learned models can be efficiently adapted to downstream CPS applications, reducing the fine-tuning effort required for domain-specific deployment.
\end{itemize}

The remainder of this paper is organized as follows.
Section~\ref{sec:related} reviews related work on classical task planning, AI-based planners, and LLM-based planning.
Section~\ref{sec:preliminaries} introduces preliminaries on classical planning and PDDL, and formulates the safety-aware and safety-generalizable planning problems.
Section~\ref{IV} presents the proposed Safe-Gen LLM framework.
Section~\ref{V} describes the experimental setup and results.
Finally, Section~\ref{VI} concludes with a discussion of limitations and future research directions.


% \OS{I am a bit concerned by the focus on "safety generalization", prominently featured in the title.  This gives an impression that LLM planning without safety constraints works ok, but when safety constraints are added, generalization is much worse.  I don't think that's what we are showing here.  Aspects of the two fine-tuning steps that are safety-specific should be highlighted more in the contribution.  It also seems to me that the SFT improves not so much safety but mostly validity of the plan: it reduces the number of precondition violations.  Maybe it is a good idea to change the evaluation order for generated plans: first see, how many plans reach the goal and how many of those are safe.  Then show that DPO increases the fraction of safe plans.  This, to me, would demonstrate safety generalizability better.}


