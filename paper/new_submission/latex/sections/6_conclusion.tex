\section{Conclusion}\label{VI} In this paper, we proposed a formal safetyâ€“guided fine-tuning framework that enables Large Language Models to perform safety-aware task planning for Cyber-Physical Systems. By combining Supervised Fine-Tuning with Direct Preference Optimization on a safety-constrained planning dataset, our approach injects verifiable safety knowledge into LLMs and aligns their behavior with safety-critical preferences. Extensive experiments on five PDDL-based domains demonstrate three key findings. First, the fine-tuned models substantially improve success rates while reducing precondition violations, indicating that they have effectively acquired domain-level planning knowledge. Second, the learned planners exhibit cross-problem and cross-domain safety generalization: they can solve unseen problems with new safety constraints in the training domains and achieve non-trivial performance in an unseen domain. Third, the LLM-based solver shows favorable scalability compared to a classical search-based planner, maintaining relatively stable runtime as problem complexity increases. Our study is conducted with quantized lightweight models and a moderately sized safety dataset, suggesting that the reported performance is still conservative. Future work includes scaling the framework to more complex CPS settings, integrating richer formal verification tools, and exploring automatic construction or refinement of safety constraints from interaction data.