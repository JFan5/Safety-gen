\section{Safe-Gen LLM}\label{IV}

In this section, we present \emph{Safe-Gen LLM}, a safety-oriented framework for adapting LLMs to CPS task planning under formal constraints.
The framework consists of two core components.
First, we perform SFT on a curated dataset of safety-compliant plans, injecting domain knowledge and enforcing strict output formats.
Second, we apply Direct Preference Optimization (DPO) to align the model with safety-critical preferences, explicitly distinguishing safe plans from unsafe or infeasible ones.

\subsection{Supervised Fine-Tuning for Safety-Compliant Planning}\label{subsec:sft}
Building on the SFT framework introduced in Section~\ref{sec:preliminaries}, we apply supervised fine-tuning to adapt LLMs for CPS task planning.
In our framework, SFT serves to:
(i) encode domain and safety knowledge,
(ii) enforce syntactic and semantic plan validity, and
(iii) provide a stable reference policy for subsequent preference-based alignment.
The SFT process consists of three steps:
designing domain-specific safety knowledge, constructing the supervised dataset, and fine-tuning the model.

\begin{table*}[h]
  \centering
  \caption{Brief introduction of the domains and their safety constraints.}
  \label{tab:domain-introduction}
  \begin{tabularx}{0.95\linewidth}{l|X|X}
    \hline
    \textbf{Domain} & \textbf{Description} & \textbf{Safety Constraints} \\
    \hline
    \textbf{Blocksworld} &
    The agent must rearrange stacked blocks into a desired configuration using pick-and-place actions. &
    Blocks must be stacked in a safe and stable order, e.g., a supporting block must be positioned before the blocks it supports. \\
    \hline
    \textbf{Ferry} &
    The agent controls a ferry that transports cars between locations. &
    The ferry must never be overloaded, and cars must safely reach their destinations before the ferry departs or returns. \\
    \hline
    \textbf{Grippers} &
    A robot with two grippers must pick up and deliver all objects to a target room. &
    Robots (or manipulators) must avoid operating simultaneously in the same narrow room, and certain grippers may be restricted for safety-critical objects. \\
    \hline
    \textbf{Spanner} &
    A worker must collect spanners and tighten all nuts across connected locations. &
    Bolts must be tightened in a safe sequence (e.g., foundation nuts before upper ones), and access to shared tools (e.g., entering the tool shed) is limited to prevent unsafe congestion. \\
    \hline
  \end{tabularx}
\end{table*}

\noindent\textbf{Domain-specific safety knowledge design.}
We start by selecting five task-planning domains from the open-source PDDL2 problem generators~\cite{seipp-et-al-zenodo2022}.
The domains are chosen using the following criteria:
\begin{itemize}
  \item relevance to real-world CPS task planning;
  \item presence of safety-critical objects, locations, or actions;
  \item availability of problem instances with varying difficulty.
\end{itemize}
Based on these criteria, we select four representative domains: Blocksworld, Ferry, Grippers, and Spanner.
A brief overview of each domain and its associated safety constraints is given in Table~\ref{tab:domain-introduction}.
For each domain, we design additional domain-specific safety knowledge in the form of high-level constraints that mirror realistic CPS requirements (e.g., collision avoidance, load limits, safe ordering of operations).
These constraints are subsequently encoded in PDDL3 \texttt{:constraints} format and used to generate safety-compliant plan demonstrations.

\subsubsection{Dataset construction.}
The pipeline for constructing the SFT dataset is illustrated in Figure~\ref{fig:dataset-construction}.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{sft_diagram.pdf}
  \caption{Pipeline for supervised fine-tuning dataset construction.}
  \label{fig:dataset-construction}
\end{figure}

First, we generate planning problems for each domain using the PDDL2 problem generators.
We remove isomorphic or trivially equivalent problems to reduce redundancy and employ a classical planner to ensure that the remaining problems are feasible.

Second, we encode the domain-specific safety constraints from Table~\ref{tab:domain-introduction} into PDDL3 \texttt{:constraints}.
We then use the temporal PDDL3 planner OPTIC~\cite{benton2012temporal} to solve the resulting constrained planning problems, and verify each candidate solution using the VAL tool~\cite{howey2004val}.
Only solutions that are successfully validated by VAL (i.e., respect both domain preconditions and encoded safety constraints) are retained.

Third, we convert each verified solution into an instruction--response pair.
The instruction consists of a natural-language prompt that includes the planning domain, problem instance, and (when applicable) safety constraints; the response is the corresponding validated plan.
Formally, we obtain a supervised dataset
\[
\mathcal{D}_{\text{SFT}} = \{(x_i, y_i)\}_{i=1}^{N},
\]
where $x_i$ is an input prompt and $y_i$ is the desired output plan.

To enhance data diversity and reduce overfitting, we design ten instruction templates.
For each problem, we randomly sample a fixed number of templates from this pool, which encourages variation in surface forms and prevents the model from simply memorizing a single prompt pattern.
A representative template is shown below.

\begin{userbox}
  You are a planning expert. Your task is to generate a \textbf{valid plan} for the given domain and problem.
  
  \texttt{DOMAIN:}
  \{\{domain\_content\}\}
  
  \texttt{PROBLEM:}
  \{\{problem\_content\}\}
  
  \textbf{Output Requirements:}
  \begin{itemize}
    \item Return \textbf{ONLY} the plan steps, one per line.
    \item Each line must follow the format: \texttt{(<ACTION\_NAME> <param1> <param2> ...)}.
    \item Use only objects defined in the \texttt{PROBLEM}.
    \item Do \textbf{NOT} include any explanations, comments, or headers.
    \item Do \textbf{NOT} output anything except the plan lines.
    \item The output must \textbf{NOT} contain natural language sentences.
    \item If the \texttt{PROBLEM} includes constraints, the plan must satisfy all of them; otherwise, solve as a standard goal-directed task.
    \item Ensure that all action preconditions hold and no constraints or invariants are violated at any step.
  \end{itemize}
  
  \textbf{Plan:}
\end{userbox}

This template explicitly instructs the model to behave as a planning expert and to produce strictly formatted action sequences.
By providing the domain and problem specifications in a structured form and enforcing strong output requirements (no extra text, strict syntax, constraint satisfaction), the template helps ensure that the generated plans remain syntactically correct, executable, and consistent with the safety constraints.

\subsubsection{Supervised fine-tuning.}
Given the constructed dataset $\mathcal{D}_{\text{SFT}}$, we fine-tune the LLM using the standard SFT objective (Eq.~\ref{eq:sft_loss}).
By minimizing this objective, SFT encourages the model to reproduce planning behaviors that are syntactically valid, executable, and safety-compliant.
Beyond learning \emph{what} plan to output, SFT also enforces \emph{how} to output it, such as adhering to strict action syntax and avoiding natural-language explanations.
Moreover, the SFT model serves as a reference policy $\pi_{\text{ref}}$ for subsequent preference-based alignment via DPO.

\subsection{Safety Preference Alignment via DPO}
Building on DPO (Section~\ref{sec:preliminaries}), we apply preference optimization to encode safety-critical preferences on top of the SFT model, biasing the policy toward plans that are not only feasible but also strictly aligned with formal safety constraints.
We decompose this process into two stages: preference data design and model alignment.

\subsubsection{Preference data design.}
We first construct a preference dataset for each domain by generating and verifying candidate plans.
For each plan produced by the LLM planner, we invoke VAL~\cite{howey2004val} to automatically check its behavior against the corresponding PDDL domain and problem (including PDDL3 \texttt{:constraints}).
The verifier outputs are parsed to assess validity, executability, safety, and goal satisfaction.

Based on these verification results, we categorize LLM-generated plans into five error and success types:
\begin{itemize}
  \item \textbf{Plan Format Error}: the plan is syntactically invalid or cannot be parsed and thus cannot be executed or evaluated.
  \item \textbf{Precondition Violation}: one or more action preconditions fail during execution.
  \item \textbf{Safety Constraint Violation}: the plan reaches the goal but violates at least one safety constraint along the execution trace.
  \item \textbf{Goal Not Satisfied}: the plan executes safely but fails to achieve the goal.
  \item \textbf{Success Plan}: the plan both satisfies all safety constraints and achieves the goal.
\end{itemize}

We assume that these categories follow an increasing order of preference, with \textbf{Success Plan} being strictly preferred over all other types, and plans that violate safety constraints being ranked particularly low.
We assign relative preference scores accordingly and construct preference pairs $(y^{+}, y^{-})$ where $y^{+}$ belongs to a higher-ranked category than $y^{-}$.
To ensure diversity and coverage, we sample candidate plans from the SFT model under multiple temperature settings, which induces variation in plan length, structure, and error types.

To further emphasize safety, we additionally incorporate solutions from PDDL2-based problem solvers that \emph{ignore} our added safety constraints.
These plans are feasible with respect to the original domain but violate the new safety constraints, and are therefore labeled as \textbf{Safety Constraint Violation} examples.
By contrasting such unsafe-but-feasible plans with fully safe plans in the preference data, we explicitly teach the model to prefer safety-compliant behavior over merely goal-reaching behavior.

\subsubsection{Model alignment.}
Using the DPO objective (Eq.~\ref{eq:dpo_loss}), we optimize the policy with the SFT model as the reference policy $\pi_{\text{ref}}$.
By constructing preference pairs that explicitly contrast safe and unsafe plans (e.g., success vs.\ safety-violating plans, or safety-violating vs.\ precondition-violating plans), we guide the model toward a policy that favors safety-compliant behaviors while preserving the task competence acquired during SFT.
The resulting Safe-Gen LLM is therefore both instruction-following and safety-aligned.

\subsection{Online Reinforcement Learning via GRPO}

Building on GRPO (Section~\ref{sec:preliminaries}), we apply online reinforcement learning to further improve the model's safety-aware planning capabilities.
Unlike DPO which relies on offline preference data, GRPO allows the model to explore and learn from its own generated plans using verifiable rewards.

\subsubsection{Reward design.}
We design a hierarchical reward function based on the VAL verifier output. For each generated plan, we assign rewards according to a four-stage validation pipeline:
\begin{itemize}
  \item \textbf{Format correctness}: The plan must be syntactically valid and parseable.
  \item \textbf{Precondition satisfaction}: All action preconditions must hold during execution.
  \item \textbf{Safety constraint compliance}: The plan must satisfy all PDDL3 constraints.
  \item \textbf{Goal achievement}: The plan must reach the specified goal state.
\end{itemize}
Plans that pass all stages receive the highest reward, while failures at earlier stages receive progressively lower rewards.

\subsubsection{Curriculum learning.}
To improve training stability, we adopt a curriculum learning strategy that progressively increases problem difficulty. Problems are grouped by complexity (e.g., number of objects, constraint types), and the model is first trained on simpler instances before advancing to harder ones. This approach helps the model build foundational planning skills before tackling complex safety constraints.

The GRPO objective optimizes the policy by comparing multiple sampled responses within each group, encouraging the model to consistently generate high-reward plans while maintaining diversity in its outputs.
