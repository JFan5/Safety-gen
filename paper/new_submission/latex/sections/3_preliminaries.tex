\section{Preliminaries}\label{sec:preliminaries}
This section reviews classical planning and PDDL, and then introduces a formal CPS task-planning model together with the safety-aware planning problems studied in this work.

\subsection{Classical Planning and PDDL}
Classical planning concerns finding a sequence of actions that transforms an initial state into a goal state in a deterministic setting.
In the STRIPS formulation, a planning problem is written as
$\Pi=\langle \mathcal{F}, \mathcal{A}, I, \mathcal{G} \rangle$,
where $\mathcal{F}$ is a finite set of fluents (propositional state variables), $\mathcal{A}$ is the action set, $I\subseteq\mathcal{F}$ is the initial state (the set of fluents that are true initially), and $\mathcal{G}$ is the goal condition (a set of fluents or a formula that must hold in the final state).
Each action $a\in\mathcal{A}$ has a precondition $\mathrm{Pre}(a)$---a formula over $\mathcal{F}$ that must be satisfied to apply the action---and effects $\mathrm{Eff}(a)$, typically split into add and delete lists that update the fluents when the action executes.
A plan for $\Pi$ is a sequence $\pi=[a_1,\dots,a_n]$ such that executing $\pi$ from $I$ yields a state satisfying $\mathcal{G}$.
In practice, plans are found by searching the state space or plan space, often guided by heuristics or other systematic search procedures.

To standardize planning problem specifications, the community introduced the Planning Domain Definition Language (PDDL)~\cite{aeronautiques1998pddl}.
PDDL is a structured, text-based format for describing a planning domain---its actions, predicates, and objects---and a concrete problem instance---the initial state, goal condition, and any extra requirements.
Typically, the domain file declares predicates (state variables) and action schemas with parameters, preconditions, and effects, while the problem file lists the objects, initial facts, and the goal~\cite{haslum2019introduction}.
PDDL has expanded across versions (e.g., 1.2, 2.1, 2.2), and it remains the de facto standard for encoding planning benchmarks~\cite{zuo2025planetarium,taitler20242023,wang2025leveraging}.

PDDL3 extends PDDL with constructs for temporal plan constraints and preferences~\cite{gerevini2005plan}.
In PDDL 3.0, temporal constraints can be specified with the \texttt{:constraints} keyword and expressed---using operators such as \texttt{sometime-before} and \texttt{always}---as a restricted form of linear temporal logic (LTL) over plan steps~\cite{bonassi2021planning}.
As a simple example, consider a Blocksworld constraint stating that a fragile block can never have another block placed on top of it.
This can be expressed as:
\begin{lstlisting}[style=lispstyle]
(:constraints
  (always
    (forall (?x - fragile-block ?y - block)
      (not (on ?y ?x)))))
\end{lstlisting}
which restricts all intermediate states of any valid plan~\cite{gerevini2005plan}.
PDDL3 also supports soft constraints (preferences) that planners try to satisfy for higher-quality solutions~\cite{gerevini2005plan}, but in this work we focus on hard safety constraints that cannot be violated.
In summary, PDDL---and in particular PDDL3---provides the formal constructs to encode not only task goals but also trajectory-level, domain-specific safety requirements as part of the planning problem specification, which we leverage for CPS tasks with explicit safety information.

\subsection{CPS Task Planning}
When applying planning to Cyber-Physical Systems (CPS), we consider a similar formal structure, with additional emphasis on continuous dynamics and safety constraints.
We define a CPS task-planning problem as a tuple
$\mathcal{P} = (\mathcal{S}, \mathcal{A}, s_0, \mathcal{G}, \mathcal{C})$.
Here $\mathcal{S}$ is the (potentially infinite or hybrid) state space capturing both discrete modes and relevant continuous variables of the CPS, $\mathcal{A}$ is the set of available actions or control decisions, $s_0 \in \mathcal{S}$ is the initial state of the system, and $\mathcal{G}$ is the goal condition as before.
The new element $\mathcal{C}$ is a set of formal safety constraints that delineate the safe operating region of the system.
Each constraint $C \in \mathcal{C}$ may be:
(i) a state invariant that must hold at every state along the execution, or
(ii) a trajectory-level constraint over the entire sequence of states and actions, capturing temporal patterns such as ``eventually'', ``until'', or ``before''.

A plan in the CPS context is defined as a finite sequence of actions
$\pi = [a_1, a_2, \dots, a_n]$
that, when applied starting from $s_0$, yields a state sequence
$s_0 \xrightarrow{a_1} s_1 \xrightarrow{a_2} \cdots \xrightarrow{a_n} s_n$.
We say that $\pi$ is \emph{goal-reaching} if the final state satisfies the goal condition, i.e., $s_n \models \mathcal{G}$.
We say that $\pi$ is \emph{safe} if it respects all safety constraints in $\mathcal{C}$, which we denote by
$\pi \models \mathcal{C}$.
For state invariants, this means that every intermediate state $s_j$ satisfies each invariant $C$.
For trajectory-level constraints, this means that the entire run induced by $\pi$ satisfies $C$ under the relevant temporal semantics.
A \emph{safe solution} to the CPS task-planning problem is a plan $\pi$ such that both $s_n \models \mathcal{G}$ and $\pi \models \mathcal{C}$ hold.

\subsection{Problem Formulation}
\subsubsection{Safety-Aware Planning}
Given the above definitions, we formulate safety-aware planning in CPS as a constrained planning problem: the planner must find a plan that reaches the task goal while never violating the safety constraints.
Let $\pi = [a_1,\dots,a_n]$ be a plan generated from initial state $s_0$, inducing a state sequence
$s_0 \xrightarrow{a_1} s_1 \xrightarrow{}\cdots\xrightarrow{a_n} s_n$.
A plan is goal-reaching if $s_n \models \mathcal{G}$ and safe if $\pi \models \mathcal{C}$.
The planning objective can therefore be written as:
\begin{equation}
  \text{find } \pi \text{ s.t. } s_n \models \mathcal{G} \text{ and } \pi \models \mathcal{C}.
\end{equation}
One may further prefer plans with better task efficiency---e.g., shorter length or lower resource usage---but our focus is on safety-compliant goal achievement rather than optimizing a specific cost metric.

Search-based planners enforce $\pi \models \mathcal{C}$ explicitly during search, pruning partial plans that would cause violations and guaranteeing that any returned plan is safe if one exists.
In contrast, LLM-based planners treat planning as sequence generation and do not inherently simulate state transitions against formal constraints.
Thus, safety must be learned (via data or feedback) or enforced by external verification and correction.
Our goal is to bridge this gap by structuring learning so that an LLM-based planner reliably respects $\mathcal{C}$ while achieving $\mathcal{G}$.

\subsubsection{Safety-Generalizable Planning}
Generalization is a key requirement for task planning: a planner should solve not only a fixed set of training problems but also unseen tasks or domains.
In safety-critical planning, this requirement naturally extends to safety constraints: the planner should solve new problems while satisfying \emph{unseen} safety constraints, a property we call \textbf{safety generalizability}.
We formalize two complementary levels of safety generalizability.

\noindent\textbf{Cross-Problem Safety Generalizability.}
Fix a domain $D=(\mathcal{S},\mathcal{A},\mathcal{C})$ with shared state/action semantics and a common safety-constraint set.
A model $\mathcal{M}$ exhibits cross-problem safety generalization if, for unseen problems $\mathcal{P}_i \in D$ (with new initial states and goals), it produces plans that both reach the goal and remain safe:
\begin{equation}
  \forall \mathcal{P}_i \in D:\;
  \pi_i = \mathcal{M}(\mathcal{P}_i)
  \Rightarrow
  (s_n^i \models \mathcal{G}_i \land \pi_i \models \mathcal{C}).
\end{equation}
Intuitively, the model reuses safety reasoning across tasks while the domain dynamics and predicates remain unchanged.

\noindent\textbf{Cross-Domain Safety Generalizability.}
Let $D'$ denote a novel domain with different state/action semantics and possibly a different safety-constraint set $\mathcal{C}'$.
A model $\mathcal{M}$ exhibits cross-domain safety generalization if, without explicit training on $D'$, it still reaches goals and maintains safety under $\mathcal{C}'$:
\begin{equation}
  \forall \mathcal{P}_j \in D':\;
  \pi_j = \mathcal{M}(\mathcal{P}_j)
  \Rightarrow
  (s_n^j \models \mathcal{G}'_j \land \pi_j \models \mathcal{C}').
\end{equation}
This requirement is stricter: the model must transfer abstract safety principles to new symbols, dynamics, and constraint structures.

Together, these two levels specify what it means for an LLM-based planner to succeed under the CPS safety-aware formulation:
it should produce goal-reaching plans that are safe by construction, and it should sustain this property across both unseen problems and unseen domains.

\subsection{Post-Training Alignment Methods}
We briefly review the post-training techniques used in this work.

\subsubsection{Supervised Fine-Tuning (SFT)}
Supervised fine-tuning adapts a pre-trained LLM to a specialized task by training on task-specific instruction--response pairs.
Given a dataset $\mathcal{D}_{\text{SFT}}=\{(x_i, y_i)\}_{i=1}^{N}$, where $x_i$ is an input prompt and $y_i$ is the target response, SFT optimizes the model parameters $\theta$ by minimizing the negative log-likelihood:
\begin{equation}
\mathcal{L}_{\text{SFT}}(\theta)=
- \mathbb{E}_{(x,y)\sim\mathcal{D}_{\text{SFT}}}
\left[ \log \pi_{\theta}(y\mid x) \right],
\label{eq:sft_loss}
\end{equation}
where $\pi_{\theta}(y\mid x)$ denotes the probability of producing $y$ given input $x$.

\subsubsection{Direct Preference Optimization (DPO)}
Direct Preference Optimization~\cite{rafailov2023direct} fine-tunes LLMs using paired preference data that specifies which of two candidate responses is preferred.
Given a dataset $\mathcal{D}_{\text{DPO}}=\{(x, y^{+}, y^{-})\}$ where $y^{+}$ is preferred over $y^{-}$, and a frozen reference policy $\pi_{\text{ref}}$, DPO optimizes:
\begin{align}
\mathcal{L}_{\text{DPO}}(\theta)
&= - \mathbb{E}_{(x, y^{+}, y^{-}) \sim \mathcal{D}_{\text{DPO}}}
\Bigg[
\log \sigma \Bigg(
\beta \Bigg(
\log \frac{\pi_{\theta}(y^{+} \mid x)}{\pi_{\text{ref}}(y^{+} \mid x)}
\nonumber\\
&\qquad\qquad
- \log \frac{\pi_{\theta}(y^{-} \mid x)}{\pi_{\text{ref}}(y^{-} \mid x)}
\Bigg)\Bigg)
\Bigg],
\label{eq:dpo_loss}
\end{align}
where $\sigma(\cdot)$ is the sigmoid function and $\beta>0$ controls the preference separation sharpness.
Unlike RLHF methods such as PPO~\cite{ouyang2022training}, DPO does not require explicit reward modeling.

\subsubsection{Group Relative Policy Optimization (GRPO)}
While DPO relies on offline preference data, Group Relative Policy Optimization (GRPO)~\cite{shao2024deepseekmath} enables online reinforcement learning with verifiable rewards.
GRPO samples multiple candidate responses for each prompt, evaluates them using a reward function, and optimizes the policy by comparing responses within each group.
This allows the model to explore and learn from its own generated outputs, guided by programmatic reward signals rather than human-labeled preferences.
