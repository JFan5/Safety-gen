\begin{abstract}
    Safety is a fundamental requirement in the task planning of Cyber-Physical Systems (CPS). However, existing approaches still exhibit significant limitations. Classical symbolic planners have difficulty representing complex or dynamic safety constraints and often require substantial domain engineering. Learning-based planners such as Reinforcement Learning (RL) can improve adaptability but typically generalize poorly to unseen environments or safety conditions. Recent advances in Large Language Models (LLMs) demonstrate strong reasoning and compositional generalization capabilities, suggesting new possibilities for safety-aware task planning. Nevertheless, current LLMs struggle to satisfy CPS safety requirements because they lack domain-specific safety knowledge and are not aligned with safety-critical decision preferences. To address these challenges, we introduce a two-stage fine-tuning framework that first incorporates CPS safety knowledge through supervised fine-tuning, and then further aligns model behavior using Direct Preference Optimization (DPO) with safety-oriented preference pairs. Experimental results show that the fine-tuned models achieve substantially improved safety adherence and demonstrate strong generalization to previously unseen CPS tasks in zero-shot settings.
\end{abstract}


\keywords{Cyber-Physical Systems, Task Planning, Large Language Models, Safety Generalizability}
