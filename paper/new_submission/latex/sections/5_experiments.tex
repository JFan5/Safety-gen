\section{Experiments}\label{V}
In this section, we empirically evaluate the proposed framework along four dimensions:
(i)~its runtime behavior compared to a classical search-based planner,
(ii)~its cross-problem safety generalization within the same domain,
(iii)~its cross-domain safety generalization to unseen domains, and
(iv)~its effectiveness in a real-world CPS deployment on a physical robot arm.

\subsection{Experimental Setup}\label{subsec:experimental-setup}
We first describe the datasets, models, and training environment used in our experiments.

\noindent\textbf{SFT dataset construction.}
As introduced in Section~\ref{subsec:sft}, we select four domains from the PDDL2 problem generators~\cite{seipp-et-al-zenodo2022} to construct the safety-oriented SFT dataset: \emph{Blocksworld}, \emph{Ferry}, \emph{Grippers}, and \emph{Spanner}.
We use Blocksworld, Ferry, Grippers, and Spanner as both training and evaluation domains.
For each domain, we generate $500$ planning problems with parameters ranging from simple to complex to diversify the dataset (e.g., different numbers of blocks, objects, or locations).
All problems are solved by the temporal planner OPTIC~\cite{benton2012temporal} and the resulting plans are validated by VAL~\cite{howey2004val} to ensure correctness.
Importantly, these problems cannot be solved by the solutions of the corresponding PDDL2 benchmark problems without incorporating the additional safety constraints, which highlights the need for a safety-aware planner.

For each problem instance, we randomly sample $5$ instruction templates from a pool of $10$ templates to construct natural-language prompts, thereby further diversifying the dataset.
This procedure yields an SFT dataset of $10{,}000$ instruction--response pairs.

\noindent\textbf{DPO dataset construction.}
For the DPO stage, we construct a preference dataset that distinguishes safety-compliant plans from unsafe or infeasible ones.
First, we use the fine-tuned SFT model to sample candidate plans under two temperature settings, $T=0.6$ and $T=0.9$, which encourages diverse error types in the generated plans.
Second, we treat valid plans from the SFT dataset---i.e., plans that both achieve the goal and satisfy all safety constraints---as the \emph{chosen} candidates.
Third, we collect \emph{rejected} candidates from two sources:
(i) plans sampled from the LLM that fail to reach the goal or violate safety constraints, and
(ii) solutions to the corresponding PDDL2 problems that are feasible in the original domain but violate the additional safety constraints; these are categorized as \textbf{Safety Constraint Violation} examples.
Overall, we construct a DPO preference dataset containing $11{,}960$ instruction--response pairs.
For both SFT and DPO, the test set consists of $50$ problems per domain.

\noindent\textbf{Model selection and training environment.}
We instantiate our framework with open-source LLMs available through the Unsloth library~\cite{unsloth} on Hugging Face~\cite{wolf-etal-2020-transformers}.
Specifically, we consider:
\begin{itemize}
  \item \texttt{unsloth/mistral-7b-instruct-v0.3-bnb-4bit}, abbreviated as \textbf{Mistral-7B};
  \item \texttt{unsloth/Qwen3-14B-unsloth-bnb-4bit}, abbreviated as \textbf{Qwen3-14B}.
\end{itemize}
Unless otherwise specified, we use the quantized 4-bit variants of these models throughout our experiments.

For training, Mistral-7B is fine-tuned on a single NVIDIA A10 GPU with 24~GB memory, and Qwen3-14B is fine-tuned on a single NVIDIA H100 GPU with 80~GB memory.
Our implementation is based on Python~3.10 and PyTorch~2.8.0~\cite{paszke2019pytorch}, and uses the Hugging Face Transformers library~\cite{wolf-etal-2020-transformers}, TRL~\cite{vonwerra2022trl}, and Unsloth~\cite{unsloth}.
We adopt LoRA~\cite{hu2022lora} and QLoRA~\cite{dettmers2023qlora} as parameter-efficient fine-tuning strategies to reduce memory usage and training time.

\subsection{Running Time Comparison}
We first compare the runtime behavior of an LLM-based solver with a classical search-based planner to evaluate scalability as problem complexity grows.

We generate planning problems whose parameter sizes range from $3$ to $50$.
Taking Blocksworld as an example, the simplest instance contains $3$ blocks, whereas the most complex instance contains $50$ blocks to be stacked.
For each parameter size, we generate $20$ problem instances, resulting in $96$ problems per domain.
We consider two domains: Blocksworld and Grippers.
As the LLM-based solver, we use GPT-OSS-20B~\cite{openai2025gptoss120bgptoss20bmodel}, and as the classical baseline, we use OPTIC~\cite{benton2012temporal}.
GPT-OSS-20B runs on a single NVIDIA A100 GPU with 80~GB memory, while OPTIC runs on an AMD EPYC~9554 64-core CPU with 180~GB memory.
The timeout for both methods is set to $100$ seconds.

Figure~\ref{fig:running-time-comparison} reports the average solving time across different problem sizes.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/blocksworld_time.png}
    \caption{Blocksworld domain.}
    \label{fig:blocksworld-time}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\linewidth}
      \centering    
    \includegraphics[width=\linewidth]{figures/grippers_time.png}
    \caption{Grippers domain.}
    \label{fig:grippers-time}
  \end{subfigure}
  \caption{Running time comparison for the LLM-based solver and classical solver in two domains.}
  \label{fig:running-time-comparison}
\end{figure}

We observe from Figure~\ref{fig:running-time-comparison} that, for relatively simple problems, the classical search-based solver is much faster than the LLM-based solver and typically completes within one second.
However, as the problem complexity increases, the runtime of the classical solver grows dramatically---almost exponentially in these settings---and eventually reaches the timeout limit of $100$ seconds.
In contrast, the runtime of the LLM-based solver remains relatively stable across different problem sizes.

Because LLM decoding is inherently stochastic, the runtime of the LLM-based solver may occasionally decrease slightly as the problem size increases within a certain range; nevertheless, the overall trend is consistent.
These results suggest that the LLM-based solver can effectively address the scalability limitations of classical search-based planners in complex planning domains.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/blocksworld_model_comparison.png}
  \caption{SFT and DPO results comparison for Mistral-7B in the Blocksworld domain.}
  \label{fig:model-comparison}
\end{figure}

\subsection{Cross-Problem Safety Generalizability}
We next evaluate the cross-problem safety generalization of the proposed framework, i.e., whether a model trained on a set of problems and safety constraints within a given domain can solve previously unseen problems with new safety constraints in the \emph{same} planning domain.

We use the Blocksworld domain as a representative example and fine-tune Mistral-7B.
We generate $500$ training problems with different numbers of blocks (from $3$ to $6$) and varying operator combinations using the PDDL2 problem generators.
During the SFT stage, we train the model for three epochs with a batch size of $4$, resulting in $714$ gradient steps.
The training loss converges steadily to $0.0262$.

For the subsequent DPO stage, following the construction procedure described in Section~\ref{subsec:experimental-setup}, we obtain a preference dataset with $3{,}050$ instruction--response pairs in Blocksworld.
Figure~\ref{fig:model-comparison} compares the performance of the pretrained, SFT, and DPO models on the Blocksworld test set.

We summarize the key observations as follows:
\begin{enumerate}
  \item \textbf{Reduction in precondition violations.}
  The precondition violation rate is significantly reduced after SFT and DPO. 
  In particular, the rate decreases from nearly $98\%$ for the pretrained model to $52\%$ for the SFT model and $42\%$ for the DPO model.
  This indicates that the SFT and DPO models have internalized the underlying planning domain knowledge and are able to generate feasible plans that respect domain preconditions.

  \item \textbf{Improvement in safety constraint satisfaction.}
  The rate of safety constraint violations also decreases after DPO, from $28\%$ for the SFT model to $26\%$ for the DPO model.
  This demonstrates that the DPO stage effectively leverages preference data to further bias the model toward safety-compliant plans.

  \item \textbf{Improvement in overall success rate.}
  The success rate---defined as generating a plan that both satisfies all safety constraints and reaches the goal---is $0\%$ for the pretrained model, but increases to $14\%$ and $20\%$ for the SFT and DPO models, respectively.
\end{enumerate}

It is worth noting that we observe two test cases categorized as \textbf{Plan Format Error}.
This is because such error types were not included in the preference dataset; they could be further reduced by adding a small number of format-error examples into the preference data.
We also emphasize that all experiments in this section are conducted with a quantized $7$B LLM and a relatively small dataset (only $500$ problems and roughly three thousand instruction--response pairs).
Therefore, the reported results likely underestimate the potential performance of the framework, yet they are still encouraging and demonstrate its effectiveness.

\begin{table*}[h]
  \centering
  \caption{Error Type Percentages by Domain}
  \label{tab:error_percentages}
  \resizebox{0.95\textwidth}{!}{%
  \begin{tabular}{l|cc|cc|cc|cc||cc}
  \hline
  \textbf{Domain} 
    & \multicolumn{2}{c}{Plan Format Error} 
    & \multicolumn{2}{c}{Precondition Violation} 
    & \multicolumn{2}{c}{Safety Constraint Violation} 
    & \multicolumn{2}{c}{Goal Not Satisfied} 
    & \multicolumn{2}{c}{Success Plans} \\
  \hline
    
     & \textbf{Pretrained} & \textbf{SFT} 
     & \textbf{Pretrained} & \textbf{SFT} 
     & \textbf{Pretrained} & \textbf{SFT} 
     & \textbf{Pretrained} & \textbf{SFT} 
     & \textbf{Pretrained} & \textbf{SFT} \\
  \hline
  \textbf{Blocksworld} & 0.0\% & 0.0\% & 70.0\% & 58.0\% & 24.0\% & 24.0\% & 2.0\% & 6.0\% & 4.0\% & \textbf{12.0\%} \\
  \textbf{Ferry      } & 0.0\% & 2.0\% & 98.0\% & 44.0\% & 2.0\% & 36.0\% & 0.0\% & 0.0\% & 0.0\% & \textbf{18.0\%} \\
  \textbf{Grippers   } & 8.0\% & 6.0\% & 42.0\% & 42.0\% & 46.0\% & 30.0\% & 4.0\% & 8.0\% & 0.0\% & \textbf{14.0\%} \\
  \textbf{Spanner    } & 6.0\% & 2.0\% & 94.0\% & 48.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% & \textbf{50.0\%} \\
  \hline
  \end{tabular}
  }% end resizebox
  \end{table*}

\subsection{SFT and GRPO Model Performance}
We now evaluate the impact of GRPO on safety performance.
Starting from the SFT model trained on the four domains, we further fine-tune it using online reinforcement learning via GRPO.

\begin{table*}[!ht]
  \centering
  \caption{Error Type Percentages by Domain on SFT and GRPO Models (Qwen3-14B)}
  \label{tab:error_percentages_sft_grpo}
  \resizebox{0.95\textwidth}{!}{%
  \begin{tabular}{l|cc|cc|cc|cc||cc}
  \hline
  \textbf{Domain}
  & \multicolumn{2}{c}{Plan Format Error}
  & \multicolumn{2}{c}{Precondition Violation}
  & \multicolumn{2}{c}{Safety Constraints Violation}
  & \multicolumn{2}{c}{Goal Not Satisfied}
  & \multicolumn{2}{c}{Success Plans} \\
  \hline

  & \textbf{SFT} & \textbf{GRPO}
  & \textbf{SFT} & \textbf{GRPO}
  & \textbf{SFT} & \textbf{GRPO}
  & \textbf{SFT} & \textbf{GRPO}
  & \textbf{SFT} & \textbf{GRPO} \\
  \hline
  \textbf{Blocksworld} & 0.0\% & 0.0\% & 24.0\% & 12.0\% & 2.0\% & 0.0\% & 4.0\% & 0.0\% & 70.0\% & \textbf{88.0\%} \\
  \textbf{Ferry      } & 0.0\% & 0.0\% & 4.0\% & 4.0\% & 10.0\% & 0.0\% & 0.0\% & 0.0\% & 86.0\% & \textbf{96.0\%} \\
  \textbf{Grippers   } & 0.0\% & 0.0\% & 4.0\% & 0.0\% & 4.0\% & 2.0\% & 0.0\% & 0.0\% & 92.0\% & \textbf{98.0\%} \\
  \textbf{Spanner    } & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 100.0\% & \textbf{100.0\%} \\
  \hline
  \end{tabular}
  }% end resizebox
\end{table*}
Table~\ref{tab:error_percentages_sft_grpo} reports the error-type distributions for the SFT and GRPO models across all four domains.
We highlight several key findings:

\begin{enumerate}
    \item \textbf{Significant success rate improvements after GRPO.}
    GRPO substantially improves success rates across all domains:
    from $70\%$ to $88\%$ in Blocksworld (+18\%), from $86\%$ to $96\%$ in Ferry (+10\%), and from $92\%$ to $98\%$ in Grippers (+6\%).
    Spanner maintains its perfect $100\%$ success rate.
    These results demonstrate that online RL with verifiable rewards effectively enhances the model's planning capabilities.

    \item \textbf{Dramatic reduction in safety violations.}
    GRPO nearly eliminates safety constraint violations: Blocksworld drops from $2\%$ to $0\%$, Ferry from $10\%$ to $0\%$, and Grippers from $4\%$ to $2\%$.
    This demonstrates that online RL with verifiable rewards effectively teaches the model to prioritize safety compliance.

    \item \textbf{Reduced precondition violations.}
    Precondition violation rates also decrease significantly, particularly in Blocksworld (from $24\%$ to $12\%$) and Grippers (from $4\%$ to $0\%$), indicating improved domain knowledge acquisition through GRPO.

    \item \textbf{Elimination of goal failures.}
    GRPO completely eliminates goal-not-satisfied errors in Blocksworld (from $4\%$ to $0\%$), demonstrating that the hierarchical reward design effectively guides the model toward complete task fulfillment.
\end{enumerate}

These results show that GRPO provides substantial improvements over SFT alone, enabling the model to generate plans that are both more successful and significantly safer.
The combination of curriculum learning and verifiable rewards allows the model to explore and learn from its own generated plans, leading to robust safety-aware planning behavior.

\begin{figure*}[!ht]
  \centering
  \includegraphics[width=0.9\linewidth]{Blocksworld.pdf}
  \caption{Case study in the Blocksworld domain.}
  \label{fig:case-study}
\end{figure*}

\subsection{Real-World Validation in Blocksworld}

To demonstrate the practical impact of safety-aware training, we evaluate our framework in both simulation and a physical Cyber-Physical System (CPS) setup using the classical Blocksworld domain.

\paragraph{Simulation.}
We first select a test-set problem instance and compare two planners:
(i) a classical PDDL2 solver, and
(ii) our safety-aware SFT+GRPO model.
The task requires satisfying a safety constraint that enforces a specific ordering between two stacking operations to avoid unsafe intermediate configurations.

As shown in Figure~\ref{fig:case-study}, the classical solver produces a plan that violates the constraint, leading to an intermediate configuration that would cause a collision between blocks.  
In contrast, the safety-aware LLM planner restructures the action sequence so that the constraint is satisfied while still achieving the goal, demonstrating that the learned safety knowledge directly influences plan generation.

\paragraph{Physical Deployment.}
We further deploy the safety-aware planner on a desktop robot arm (Elephant myCobot~280) controlled by a Raspberry~Pi.  
The LLM-generated plan is transmitted via SSH and executed on the real robot.  
We compare the safety-aware plan with a baseline unsafe sequence that mirrors the violation observed in simulation.

Figure~\ref{fig:experiment-snapshot} shows snapshots from the execution.  
The unsafe baseline results in a physical collision during stacking, whereas the safety-aware plan completes the task without violating the safety requirement.  
This experiment confirms that the proposed framework not only improves symbolic safety in simulation but also yields robust, collision-free behaviors when deployed on real CPS hardware.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{experiments.pdf}
  \caption{Snapshot of the physical robot executing the Blocksworld task.}
  \label{fig:experiment-snapshot}
\end{figure}
