\section{Experiments}\label{V}
In this section, we empirically evaluate the proposed framework along four dimensions:
(i)~its runtime behavior compared to a classical search-based planner,
(ii)~its cross-problem safety generalization within the same domain,
(iii)~its cross-domain safety generalization to unseen domains, and
(iv)~its effectiveness in a real-world CPS deployment on a physical robot arm.

\subsection{Experimental Setup}\label{subsec:experimental-setup}
We first describe the datasets, models, and training environment used in our experiments.

\noindent\textbf{SFT dataset construction.}
As introduced in Section~\ref{subsec:sft}, we select four domains from the PDDL2 problem generators~\cite{seipp-et-al-zenodo2022} to construct the safety-oriented SFT dataset: \emph{Blocksworld}, \emph{Ferry}, \emph{Grippers}, and \emph{Spanner}.
We use Blocksworld, Ferry, Grippers, and Spanner as both training and evaluation domains.
For each domain, we generate $500$ planning problems with parameters ranging from simple to complex to diversify the dataset (e.g., different numbers of blocks, objects, or locations).
All problems are solved by the temporal planner OPTIC~\cite{benton2012temporal} and the resulting plans are validated by VAL~\cite{howey2004val} to ensure correctness.
Importantly, these problems cannot be solved by the solutions of the corresponding PDDL2 benchmark problems without incorporating the additional safety constraints, which highlights the need for a safety-aware planner.

For each problem instance, we randomly sample $5$ instruction templates from a pool of $10$ templates to construct natural-language prompts, thereby further diversifying the dataset.
This procedure yields an SFT dataset of $10{,}000$ instruction--response pairs.

\noindent\textbf{DPO dataset construction.}
For the DPO stage, we construct a preference dataset that distinguishes safety-compliant plans from unsafe or infeasible ones.
First, we use the fine-tuned SFT model to sample candidate plans under two temperature settings, $T=0.6$ and $T=0.9$, which encourages diverse error types in the generated plans.
Second, we treat valid plans from the SFT dataset---i.e., plans that both achieve the goal and satisfy all safety constraints---as the \emph{chosen} candidates.
Third, we collect \emph{rejected} candidates from two sources:
(i) plans sampled from the LLM that fail to reach the goal or violate safety constraints, and
(ii) solutions to the corresponding PDDL2 problems that are feasible in the original domain but violate the additional safety constraints; these are categorized as \textbf{Safety Constraint Violation} examples.
Overall, we construct a DPO preference dataset containing $11{,}960$ instruction--response pairs.
For both SFT and DPO, the test set consists of $50$ problems per domain.

\noindent\textbf{Model selection and training environment.}
We instantiate our framework with open-source LLMs available through the Unsloth library~\cite{unsloth} on Hugging Face~\cite{wolf-etal-2020-transformers}.
Specifically, we consider:
\begin{itemize}
  \item \texttt{unsloth/mistral-7b-instruct-v0.3-bnb-4bit}, abbreviated as \textbf{Mistral-7B};
  \item \texttt{unsloth/Qwen3-14B-unsloth-bnb-4bit}, abbreviated as \textbf{Qwen3-14B}.
\end{itemize}
Unless otherwise specified, we use the quantized 4-bit variants of these models throughout our experiments.

For training, Mistral-7B is fine-tuned on a single NVIDIA A10 GPU with 24~GB memory, and Qwen3-14B is fine-tuned on a single NVIDIA H100 GPU with 80~GB memory.
Our implementation is based on Python~3.10 and PyTorch~2.8.0~\cite{paszke2019pytorch}, and uses the Hugging Face Transformers library~\cite{wolf-etal-2020-transformers}, TRL~\cite{vonwerra2022trl}, and Unsloth~\cite{unsloth}.
We adopt LoRA~\cite{hu2022lora} and QLoRA~\cite{dettmers2023qlora} as parameter-efficient fine-tuning strategies to reduce memory usage and training time.

\subsection{Running Time and Scalability Comparison}
We compare the runtime and success rates of GPT-5.2~\cite{openai2025gpt5} (LLM-based) with OPTIC~\cite{benton2012temporal} (classical planner) on $48$ problems of increasing complexity in Blocksworld and Grippers, with a $300$s timeout.

\begin{figure}[!ht]
  \centering
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/comparison_blocksworld_gpt5.2_vs_optic.png}
    \caption{Blocksworld: GPT-5.2 100\% (48/48), avg 102.3s vs OPTIC 39.6\% (19/48), avg 114.4s.}
    \label{fig:benchmark-blocksworld}
  \end{subfigure}
  \vspace{0.3em}
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/comparison_grippers_gpt5.2_vs_optic.png}
    \caption{Grippers: GPT-5.2 100\% (48/48), avg 102.6s vs OPTIC 64.6\% (31/48), avg 146.0s.}
    \label{fig:benchmark-grippers}
  \end{subfigure}
  \caption{Benchmark comparison: GPT-5.2 (solid) vs OPTIC (dashed). Green squares = success, red circles = failure.}
  \label{fig:benchmark-comparison}
\end{figure}

As shown in Figure~\ref{fig:benchmark-comparison}, GPT-5.2 achieves 100\% success in both domains with stable runtime ($\sim$102s), while OPTIC's success rate drops significantly (39.6\% Blocksworld, 64.6\% Grippers) and runtime increases dramatically as problem complexity grows. These results demonstrate that LLM-based solvers effectively address the scalability limitations of classical planners in safety-constrained planning.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/blocksworld_model_comparison.png}
  \caption{SFT and DPO results comparison for Mistral-7B in the Blocksworld domain.}
  \label{fig:model-comparison}
\end{figure}

\subsection{Cross-Problem Safety Generalizability}
We next evaluate the cross-problem safety generalization of the proposed framework, i.e., whether a model trained on a set of problems and safety constraints within a given domain can solve previously unseen problems with new safety constraints in the \emph{same} planning domain.

We use the Blocksworld domain as a representative example and fine-tune Mistral-7B.
We generate $500$ training problems with different numbers of blocks (from $3$ to $6$) and varying operator combinations using the PDDL2 problem generators.
During the SFT stage, we train the model for three epochs with a batch size of $4$, resulting in $714$ gradient steps.
The training loss converges steadily to $0.0262$.

For the subsequent DPO stage, following the construction procedure described in Section~\ref{subsec:experimental-setup}, we obtain a preference dataset with $3{,}050$ instruction--response pairs in Blocksworld.
Figure~\ref{fig:model-comparison} compares the performance of the pretrained, SFT, and DPO models on the Blocksworld test set.

We summarize the key observations as follows:
\begin{enumerate}
  \item \textbf{Reduction in precondition violations.}
  The precondition violation rate is significantly reduced after SFT and DPO. 
  In particular, the rate decreases from nearly $98\%$ for the pretrained model to $52\%$ for the SFT model and $42\%$ for the DPO model.
  This indicates that the SFT and DPO models have internalized the underlying planning domain knowledge and are able to generate feasible plans that respect domain preconditions.

  \item \textbf{Improvement in safety constraint satisfaction.}
  The rate of safety constraint violations also decreases after DPO, from $28\%$ for the SFT model to $26\%$ for the DPO model.
  This demonstrates that the DPO stage effectively leverages preference data to further bias the model toward safety-compliant plans.

  \item \textbf{Improvement in overall success rate.}
  The success rate---defined as generating a plan that both satisfies all safety constraints and reaches the goal---is $0\%$ for the pretrained model, but increases to $14\%$ and $20\%$ for the SFT and DPO models, respectively.
\end{enumerate}

It is worth noting that we observe two test cases categorized as \textbf{Plan Format Error}.
This is because such error types were not included in the preference dataset; they could be further reduced by adding a small number of format-error examples into the preference data.
We also emphasize that all experiments in this section are conducted with a quantized $7$B LLM and a relatively small dataset (only $500$ problems and roughly three thousand instruction--response pairs).
Therefore, the reported results likely underestimate the potential performance of the framework, yet they are still encouraging and demonstrate its effectiveness.

\begin{table*}[h]
  \centering
  \caption{Error Type Percentages by Domain Across Training Stages}
  \label{tab:error_percentages}
  \resizebox{0.95\textwidth}{!}{%
  \begin{tabular}{l||ccc|ccc|ccc|ccc|ccc}
  \hline
  \textbf{Domain}
  & \multicolumn{3}{c|}{Success Plans}
  & \multicolumn{3}{c|}{Plan Format Error}
  & \multicolumn{3}{c|}{Precondition Violation}
  & \multicolumn{3}{c|}{Safety Constraint Violation}
  & \multicolumn{3}{c}{Goal Not Satisfied} \\
  \hline
  & \textbf{PT} & \textbf{SFT} & \textbf{GRPO}
  & \textbf{PT} & \textbf{SFT} & \textbf{GRPO}
  & \textbf{PT} & \textbf{SFT} & \textbf{GRPO}
  & \textbf{PT} & \textbf{SFT} & \textbf{GRPO}
  & \textbf{PT} & \textbf{SFT} & \textbf{GRPO} \\
  \hline
  \textbf{Blocksworld} & 4.0 & 70.0 & \textbf{88.0} & 0.0 & 0.0 & 0.0 & 70.0 & 24.0 & 12.0 & 24.0 & 2.0 & 0.0 & 2.0 & 4.0 & 0.0 \\
  \textbf{Ferry}       & 0.0 & 86.0 & \textbf{96.0} & 0.0 & 0.0 & 0.0 & 98.0 & 4.0 & 4.0 & 2.0 & 10.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
  \textbf{Grippers}    & 0.0 & 92.0 & \textbf{98.0} & 8.0 & 0.0 & 0.0 & 42.0 & 4.0 & 0.0 & 46.0 & 4.0 & 2.0 & 4.0 & 0.0 & 0.0 \\
  \textbf{Spanner}     & 0.0 & 100.0 & \textbf{100.0} & 6.0 & 0.0 & 0.0 & 94.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
  \hline
  \end{tabular}
  }
  \vspace{0.5em}

  \footnotesize{PT = Pretrained (Mistral-7B baseline); SFT and GRPO use Qwen3-14B. All values in \%.}
\end{table*}

Table~\ref{tab:error_percentages} summarizes the error-type distributions across all training stages: Pretrained (PT), SFT, and GRPO.
We highlight several key findings that demonstrate the progressive improvements from each training stage:

\begin{enumerate}
    \item \textbf{Massive reduction in precondition violations.}
    Precondition violations drop dramatically across training stages: from 70--98\% (Pretrained) to 0--24\% (SFT) to 0--12\% (GRPO).
    This indicates that both SFT and GRPO enable the model to internalize domain-specific action semantics.

    \item \textbf{Progressive elimination of safety violations.}
    Safety constraint violations are substantially reduced at each stage.
    Pretrained models exhibit up to 46\% violations (Grippers), which SFT reduces to 2--10\%, and GRPO nearly eliminates (0--2\%).
    This demonstrates that online RL with verifiable rewards effectively teaches the model to prioritize safety compliance.

    \item \textbf{Significant success rate improvements.}
    Success rates improve from 0--4\% (Pretrained) to 70--100\% (SFT) to 88--100\% (GRPO).
    GRPO provides consistent gains across all domains: +18\% in Blocksworld, +10\% in Ferry, and +6\% in Grippers.

    \item \textbf{Elimination of goal failures.}
    GRPO completely eliminates goal-not-satisfied errors in Blocksworld (from 4\% after SFT to 0\%), demonstrating that the hierarchical reward design effectively guides the model toward complete task fulfillment.
\end{enumerate}

These results show that the proposed training pipeline---SFT followed by GRPO---enables progressive improvements in both safety and success.
The combination of curriculum learning and verifiable rewards allows the model to explore and learn from its own generated plans, leading to robust safety-aware planning behavior.

\begin{figure*}[!ht]
  \centering
  \includegraphics[width=0.9\linewidth]{Blocksworld.pdf}
  \caption{Case study in the Blocksworld domain.}
  \label{fig:case-study}
\end{figure*}

\subsection{Real-World Validation in Blocksworld}

To demonstrate the practical impact of safety-aware training, we evaluate our framework in both simulation and a physical Cyber-Physical System (CPS) setup using the classical Blocksworld domain.

\paragraph{Simulation.}
We first select a test-set problem instance and compare two planners:
(i) a classical PDDL2 solver, and
(ii) our safety-aware SFT+GRPO model.
The task requires satisfying a safety constraint that enforces a specific ordering between two stacking operations to avoid unsafe intermediate configurations.

As shown in Figure~\ref{fig:case-study}, the classical solver produces a plan that violates the constraint, leading to an intermediate configuration that would cause a collision between blocks.  
In contrast, the safety-aware LLM planner restructures the action sequence so that the constraint is satisfied while still achieving the goal, demonstrating that the learned safety knowledge directly influences plan generation.

\paragraph{Physical Deployment.}
We further deploy the safety-aware planner on a desktop robot arm (Elephant myCobot~280) controlled by a Raspberry~Pi.  
The LLM-generated plan is transmitted via SSH and executed on the real robot.  
We compare the safety-aware plan with a baseline unsafe sequence that mirrors the violation observed in simulation.

Figure~\ref{fig:experiment-snapshot} shows snapshots from the execution.  
The unsafe baseline results in a physical collision during stacking, whereas the safety-aware plan completes the task without violating the safety requirement.  
This experiment confirms that the proposed framework not only improves symbolic safety in simulation but also yields robust, collision-free behaviors when deployed on real CPS hardware.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{experiments.pdf}
  \caption{Snapshot of the physical robot executing the Blocksworld task.}
  \label{fig:experiment-snapshot}
\end{figure}
