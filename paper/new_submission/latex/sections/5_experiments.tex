\section{Experiments}\label{V}
In this section, we empirically evaluate the proposed framework along four dimensions:
(i)~its runtime behavior compared to a classical search-based planner,
(ii)~its cross-problem safety generalization within the same domain,
(iii)~its cross-domain safety generalization to unseen domains, and
(iv)~its effectiveness in a real-world CPS deployment on a physical robot arm.

\subsection{Experimental Setup}\label{subsec:experimental-setup}
We first describe the datasets, models, and training environment used in our experiments.

\noindent\textbf{SFT dataset construction.}
As introduced in Section~\ref{subsec:sft}, we select four domains from the PDDL2 problem generators~\cite{seipp-et-al-zenodo2022} to construct the safety-oriented SFT dataset: \emph{Blocksworld}, \emph{Ferry}, \emph{Grippers}, and \emph{Spanner}.
We use Blocksworld, Ferry, Grippers, and Spanner as both training and evaluation domains.
For each domain, we generate $500$ planning problems with parameters ranging from simple to complex to diversify the dataset (e.g., different numbers of blocks, objects, or locations).
All problems are solved by the temporal planner OPTIC~\cite{benton2012temporal} and the resulting plans are validated by VAL~\cite{howey2004val} to ensure correctness.
Importantly, these problems cannot be solved by the solutions of the corresponding PDDL2 benchmark problems without incorporating the additional safety constraints, which highlights the need for a safety-aware planner.

For each problem instance, we randomly sample $5$ instruction templates from a pool of $10$ templates to construct natural-language prompts, thereby further diversifying the dataset.
This procedure yields an SFT dataset of $10{,}000$ instruction--response pairs.

\noindent\textbf{DPO dataset construction.}
For the DPO stage, we construct a preference dataset that distinguishes safety-compliant plans from unsafe or infeasible ones.
First, we use the fine-tuned SFT model to sample candidate plans under two temperature settings, $T=0.6$ and $T=0.9$, which encourages diverse error types in the generated plans.
Second, we treat valid plans from the SFT dataset---i.e., plans that both achieve the goal and satisfy all safety constraints---as the \emph{chosen} candidates.
Third, we collect \emph{rejected} candidates from two sources:
(i) plans sampled from the LLM that fail to reach the goal or violate safety constraints, and
(ii) solutions to the corresponding PDDL2 problems that are feasible in the original domain but violate the additional safety constraints; these are categorized as \textbf{Safety Constraint Violation} examples.
Overall, we construct a DPO preference dataset containing $11{,}960$ instruction--response pairs.
For both SFT and DPO, the test set consists of $50$ problems per domain.

\noindent\textbf{Model selection and training environment.}
We instantiate our framework with open-source LLMs available through the Unsloth library~\cite{unsloth} on Hugging Face~\cite{wolf-etal-2020-transformers}.
Specifically, we consider:
\begin{itemize}
  \item \texttt{unsloth/mistral-7b-instruct-v0.3-bnb-4bit}, abbreviated as \textbf{Mistral-7B};
  \item \texttt{unsloth/Qwen3-14B-unsloth-bnb-4bit}, abbreviated as \textbf{Qwen3-14B}.
\end{itemize}
Unless otherwise specified, we use the quantized 4-bit variants of these models throughout our experiments.

For training, Mistral-7B is fine-tuned on a single NVIDIA A10 GPU with 24~GB memory, and Qwen3-14B is fine-tuned on a single NVIDIA H100 GPU with 80~GB memory.
Our implementation is based on Python~3.10 and PyTorch~2.8.0~\cite{paszke2019pytorch}, and uses the Hugging Face Transformers library~\cite{wolf-etal-2020-transformers}, TRL~\cite{vonwerra2022trl}, and Unsloth~\cite{unsloth}.
We adopt LoRA~\cite{hu2022lora} and QLoRA~\cite{dettmers2023qlora} as parameter-efficient fine-tuning strategies to reduce memory usage and training time.

\subsection{Running Time and Scalability Comparison}
We compare the runtime and success rates of GPT-5.2~\cite{openai2025gpt5} (LLM-based) with OPTIC~\cite{benton2012temporal} (classical planner) on $48$ problems of increasing complexity in Blocksworld and Grippers, with a $300$s timeout.

\begin{figure}[!ht]
  \centering
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/comparison_blocksworld_gpt5.2_vs_optic.png}
    \caption{Blocksworld: GPT-5.2 100\% (48/48), avg 102.3s vs OPTIC 39.6\% (19/48), avg 114.4s.}
    \label{fig:benchmark-blocksworld}
  \end{subfigure}
  \vspace{0.3em}
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/comparison_grippers_gpt5.2_vs_optic.png}
    \caption{Grippers: GPT-5.2 100\% (48/48), avg 102.6s vs OPTIC 64.6\% (31/48), avg 146.0s.}
    \label{fig:benchmark-grippers}
  \end{subfigure}
  \caption{Benchmark comparison: GPT-5.2 (solid) vs OPTIC (dashed). Green squares = success, red circles = failure.}
  \label{fig:benchmark-comparison}
\end{figure}

As shown in Figure~\ref{fig:benchmark-comparison}, GPT-5.2 achieves 100\% success in both domains with stable runtime ($\sim$102s), while OPTIC's success rate drops significantly (39.6\% Blocksworld, 64.6\% Grippers) and runtime increases dramatically as problem complexity grows. These results demonstrate that LLM-based solvers effectively address the scalability limitations of classical planners in safety-constrained planning.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/blocksworld_model_comparison.png}
  \caption{Error type distribution across training stages (Pretrained $\rightarrow$ SFT $\rightarrow$ GRPO) for Mistral-7B in the Blocksworld domain.}
  \label{fig:model-comparison}
\end{figure}

\subsection{Cross-Problem Safety Generalizability}
We evaluate the cross-problem safety generalization of the proposed framework, i.e., whether a model trained on planning problems within a given domain can solve previously unseen problems with new safety constraints in the \emph{same} domain.

We use the Blocksworld domain as a representative example and fine-tune Mistral-7B through our two-stage pipeline (SFT followed by GRPO).
Figure~\ref{fig:model-comparison} compares the error type distribution across three stages: Pretrained, SFT, and GRPO.

We summarize the key observations as follows:
\begin{enumerate}
  \item \textbf{Dramatic reduction in precondition violations.}
  The precondition violation rate decreases from 98\% (Pretrained) to 20\% (SFT) to 8\% (GRPO).
  This indicates that both training stages progressively internalize domain-specific action semantics.

  \item \textbf{Progressive improvement in safety compliance.}
  Safety constraint violations decrease from 2\% (Pretrained) to 10\% (SFT) to 4\% (GRPO).
  The slight increase after SFT reflects a trade-off: as the model learns to generate more executable plans, it initially makes more safety errors. GRPO then effectively reduces these violations through reward-guided optimization.

  \item \textbf{Substantial success rate improvement.}
  The success rate improves dramatically: from 0\% (Pretrained) to 66\% (SFT) to 82\% (GRPO).
  This demonstrates that our two-stage training pipeline enables the model to generate plans that both achieve goals and satisfy safety constraints.
\end{enumerate}

These results on Mistral-7B demonstrate that even with a relatively small model, our framework achieves substantial improvements in safety-aware planning through the combination of SFT and GRPO.

\begin{table*}[h]
  \centering
  \caption{Error Type Percentages by Domain Across Training Stages}
  \label{tab:error_percentages}
  \resizebox{0.95\textwidth}{!}{%
  \begin{tabular}{l||ccc|ccc|ccc|ccc|ccc}
  \hline
  \textbf{Domain}
  & \multicolumn{3}{c|}{Success Plans}
  & \multicolumn{3}{c|}{Plan Format Error}
  & \multicolumn{3}{c|}{Precondition Violation}
  & \multicolumn{3}{c|}{Safety Constraint Violation}
  & \multicolumn{3}{c}{Goal Not Satisfied} \\
  \hline
  & \textbf{PT} & \textbf{SFT} & \textbf{GRPO}
  & \textbf{PT} & \textbf{SFT} & \textbf{GRPO}
  & \textbf{PT} & \textbf{SFT} & \textbf{GRPO}
  & \textbf{PT} & \textbf{SFT} & \textbf{GRPO}
  & \textbf{PT} & \textbf{SFT} & \textbf{GRPO} \\
  \hline
  \textbf{Blocksworld} & 4.0 & 70.0 & \textbf{88.0} & 0.0 & 0.0 & 0.0 & 70.0 & 24.0 & 12.0 & 0.0 & 2.0 & 0.0 & 26.0 & 4.0 & 0.0 \\
  \textbf{Ferry}       & 0.0 & 86.0 & \textbf{96.0} & 0.0 & 0.0 & 0.0 & 98.0 & 4.0 & 4.0 & 0.0 & 10.0 & 0.0 & 2.0 & 0.0 & 0.0 \\
  \textbf{Grippers}    & 0.0 & 92.0 & \textbf{98.0} & 0.0 & 0.0 & 0.0 & 40.0 & 4.0 & 0.0 & 48.0 & 4.0 & 2.0 & 12.0 & 0.0 & 0.0 \\
  \textbf{Spanner}     & 0.0 & 100.0 & \textbf{100.0} & 0.0 & 0.0 & 0.0 & 94.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 6.0 & 0.0 & 0.0 \\
  \hline
  \end{tabular}
  }
  \vspace{0.5em}

  \footnotesize{PT = Pretrained; SFT = Supervised Fine-Tuning; GRPO = Group Relative Policy Optimization. All use Qwen3-14B. Values in \%.}
\end{table*}

Table~\ref{tab:error_percentages} summarizes the error-type distributions across all training stages: Pretrained (PT), SFT, and GRPO.
We highlight several key findings that demonstrate the progressive improvements from each training stage:

\begin{enumerate}
    \item \textbf{Massive reduction in precondition violations.}
    Precondition violations drop dramatically across training stages: from 70--98\% (Pretrained) to 0--24\% (SFT) to 0--12\% (GRPO).
    This indicates that both SFT and GRPO enable the model to internalize domain-specific action semantics.

    \item \textbf{Progressive elimination of safety violations.}
    Safety constraint violations are substantially reduced at each stage.
    Pretrained models exhibit up to 48\% violations (Grippers), which SFT reduces to 2--10\%, and GRPO nearly eliminates (0--2\%).
    This demonstrates that online RL with verifiable rewards effectively teaches the model to prioritize safety compliance.

    \item \textbf{Significant success rate improvements.}
    Success rates improve from 0--4\% (Pretrained) to 70--100\% (SFT) to 88--100\% (GRPO).
    GRPO provides consistent gains across all domains: +18\% in Blocksworld, +10\% in Ferry, and +6\% in Grippers.

    \item \textbf{Elimination of goal failures.}
    GRPO completely eliminates goal-not-satisfied errors in Blocksworld (from 4\% after SFT to 0\%), demonstrating that the hierarchical reward design effectively guides the model toward complete task fulfillment.
\end{enumerate}

These results show that the proposed training pipeline---SFT followed by GRPO---enables progressive improvements in both safety and success.
The combination of curriculum learning and verifiable rewards allows the model to explore and learn from its own generated plans, leading to robust safety-aware planning behavior.

\subsection{Input Format Comparison: Symbolic vs Natural Language vs JSON}

To evaluate the robustness of our framework to different input representations, we compare model performance across three input formats: symbolic PDDL3, natural language (NL), and JSON.
For NL, we convert PDDL3 specifications into human-readable prompts; for JSON, we use a structured key-value representation that preserves the semantic content while providing a machine-friendly format.

\begin{table}[t]
\centering
\caption{Performance comparison: Symbolic (PDDL3) vs Natural Language vs JSON input}
\label{tab:input_format_comparison}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|ccc|ccc|ccc|ccc|ccc}
\hline
\textbf{Domain} & \multicolumn{3}{c|}{Success $\uparrow$} & \multicolumn{3}{c|}{Format Err} & \multicolumn{3}{c|}{Precond. Err} & \multicolumn{3}{c|}{Safety Viol} & \multicolumn{3}{c}{Goal Err} \\
 & PDDL3 & NL & JSON & PDDL3 & NL & JSON & PDDL3 & NL & JSON & PDDL3 & NL & JSON & PDDL3 & NL & JSON \\
\hline
Blocksworld & \textbf{88.0\%} & 74.0\% & 80.0\% & \textbf{0.0\%} & \textbf{0.0\%} & \textbf{0.0\%} & 12.0\% & 18.0\% & \textbf{8.0\%} & \textbf{0.0\%} & 8.0\% & 8.0\% & \textbf{0.0\%} & \textbf{0.0\%} & 4.0\% \\
Ferry & \textbf{96.0\%} & 90.0\% & \textbf{96.0\%} & \textbf{0.0\%} & \textbf{0.0\%} & \textbf{0.0\%} & 4.0\% & 6.0\% & \textbf{2.0\%} & \textbf{0.0\%} & 4.0\% & 2.0\% & \textbf{0.0\%} & \textbf{0.0\%} & \textbf{0.0\%} \\
Grippers & \textbf{98.0\%} & 82.0\% & \textbf{98.0\%} & \textbf{0.0\%} & \textbf{0.0\%} & \textbf{0.0\%} & \textbf{0.0\%} & 14.0\% & 2.0\% & 2.0\% & 4.0\% & \textbf{0.0\%} & \textbf{0.0\%} & \textbf{0.0\%} & \textbf{0.0\%} \\
Spanner & \textbf{100.0\%} & 90.0\% & 96.0\% & \textbf{0.0\%} & 2.0\% & \textbf{0.0\%} & \textbf{0.0\%} & 8.0\% & 4.0\% & \textbf{0.0\%} & \textbf{0.0\%} & \textbf{0.0\%} & \textbf{0.0\%} & \textbf{0.0\%} & \textbf{0.0\%} \\
\hline
\textbf{Average} & \textbf{95.5\%} & 84.0\% & 92.5\% & \textbf{0.0\%} & 0.5\% & \textbf{0.0\%} & \textbf{4.0\%} & 11.5\% & \textbf{4.0\%} & \textbf{0.5\%} & 4.0\% & 2.5\% & \textbf{0.0\%} & \textbf{0.0\%} & 1.0\% \\
\hline
\end{tabular}
}
\end{table}

Table~\ref{tab:input_format_comparison} reports the results across four domains (excluding Delivery due to its inherent difficulty).
Although the model is trained exclusively on PDDL3 input, it achieves strong performance on both NL and JSON formats without any format errors.
JSON achieves 92.5\% average success rate across the four standard domains, matching PDDL3 in Ferry (96.0\%) and Grippers (98.0\%).
NL also performs well with 84.0\% average success, demonstrating that the model can interpret human-readable problem descriptions.
Notably, all three formats produce zero format errors, indicating that the learned planning knowledge transfers effectively to unseen input representations.
These results confirm that our GRPO-trained model generalizes beyond its training distribution, enabling flexible deployment with different input interfaces.

\subsection{Integration with LLM Agentic Workflows}

A key advantage of our approach is that the fine-tuned model can be seamlessly integrated with existing LLM agentic workflows to further improve reliability.
We evaluate this by combining our GRPO-trained model with SafePilot, a verification-guided iterative refinement framework that detects plan errors and prompts the LLM to regenerate until a valid plan is produced.

\begin{table}[t]
\centering
\caption{Comparison of Planning Success and Safety Violation Rates (\%).}
\label{tab:safepilot_comparison}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Block} & \textbf{Ferry} & \textbf{Gripp} & \textbf{Spann} & \textbf{Deliv} & \textbf{Avg.} \\
\midrule
\multicolumn{7}{l}{\textit{Success Rate (\%) $\uparrow$}} \\
Pretrained & 50.0 & 0.0 & 50.0 & 0.0 & 50.0 & \textbf{30.0} \\
GRPO & 88.0 & 96.0 & 98.0 & 100.0 & 2.0 & \textbf{76.8} \\
GRPO+SP & 94.0 & 98.0 & 98.0 & 100.0 & 6.0 & \textbf{79.2} \\
\midrule
\multicolumn{7}{l}{\textit{Safety Violation (\%) $\downarrow$}} \\
Pretrained & 0.0 & 0.0 & 0.0 & 0.0 & 50.0 & \textbf{10.0} \\
GRPO & 0.0 & 0.0 & 2.0 & 0.0 & 22.0 & \textbf{4.8} \\
GRPO+SP & 0.0 & 0.0 & 2.0 & 0.0 & 18.0 & \textbf{4.0} \\
\midrule
\multicolumn{7}{l}{\textit{Avg. Retries $\downarrow$}} \\
Pretrained & 3.50 & 5.00 & 3.00 & 5.00 & 4.50 & \textbf{4.20} \\
GRPO & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & \textbf{1.00} \\
GRPO+SP & 1.36 & 1.12 & 1.14 & 1.00 & 4.76 & \textbf{1.88} \\
\bottomrule
\end{tabular}%
}
\end{table}

Table~\ref{tab:safepilot_comparison} presents the results, where the Retry column indicates the average number of generation attempts required per problem.
We highlight two key findings:

\paragraph{GRPO Training Yields Higher Success with Fewer Retries.}
Comparing the Pretrained and GRPO columns reveals that our training pipeline achieves dramatically higher success rates while requiring significantly fewer iterations.
The pretrained model achieves only 25.0\% success with an average of 4.13 retry attempts, while the GRPO model achieves 95.5\% success with just 1.00 retry (i.e., single-pass generation).
This demonstrates that GRPO training internalizes both domain knowledge and safety constraints, enabling the model to generate valid plans on the first attempt.

\paragraph{Seamless Integration with Existing LLM Workflows.}
Combining our GRPO model with SafePilot further improves success rates from 95.5\% to 97.5\%, with only a modest increase in retries (1.00 to 1.16).
This result demonstrates that our fine-tuned model is fully compatible with existing LLM agentic workflows.
The verification-guided refinement catches the few remaining errors, providing an additional layer of reliability.
Importantly, the GRPO+SafePilot combination requires far fewer retries than Pretrained alone (1.16 vs 4.13), making the system both more reliable and more efficient.

\begin{figure*}[!ht]
  \centering
  \includegraphics[width=0.9\linewidth]{Blocksworld.pdf}
  \caption{Case study in the Blocksworld domain.}
  \label{fig:case-study}
\end{figure*}

\subsection{Real-World Validation in Blocksworld}

To demonstrate the practical impact of safety-aware training, we evaluate our framework in both simulation and a physical Cyber-Physical System (CPS) setup using the classical Blocksworld domain.

\paragraph{Simulation.}
We first select a test-set problem instance and compare two planners:
(i) a classical PDDL2 solver, and
(ii) our safety-aware SFT+GRPO model.
The task requires satisfying a safety constraint that enforces a specific ordering between two stacking operations to avoid unsafe intermediate configurations.

As shown in Figure~\ref{fig:case-study}, the classical solver produces a plan that violates the constraint, leading to an intermediate configuration that would cause a collision between blocks.  
In contrast, the safety-aware LLM planner restructures the action sequence so that the constraint is satisfied while still achieving the goal, demonstrating that the learned safety knowledge directly influences plan generation.

\paragraph{Physical Deployment.}
We further deploy the safety-aware planner on a desktop robot arm (Elephant myCobot~280) controlled by a Raspberry~Pi.  
The LLM-generated plan is transmitted via SSH and executed on the real robot.  
We compare the safety-aware plan with a baseline unsafe sequence that mirrors the violation observed in simulation.

Figure~\ref{fig:experiment-snapshot} shows snapshots from the execution.  
The unsafe baseline results in a physical collision during stacking, whereas the safety-aware plan completes the task without violating the safety requirement.  
This experiment confirms that the proposed framework not only improves symbolic safety in simulation but also yields robust, collision-free behaviors when deployed on real CPS hardware.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{experiments.pdf}
  \caption{Snapshot of the physical robot executing the Blocksworld task.}
  \label{fig:experiment-snapshot}
\end{figure}
