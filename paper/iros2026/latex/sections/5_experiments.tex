\section{Experiments}\label{V}
In this section, we empirically evaluate the proposed framework along four dimensions:
(i)~the scalability limitations of classical planners and the potential of LLM-based solvers,
(ii)~its cross-problem safety generalization within the same domain,
(iii)~its cross-domain safety generalization to multiple domains, and
(iv)~its effectiveness in a real-world robotic deployment on a physical robot arm.

\subsection{Experimental Setup}\label{subsec:experimental-setup}

We first describe the datasets, models, and training environment used in our experiments.

\noindent\textbf{Dataset construction.}
As introduced in Section~\ref{subsec:dataset}, we select four domains from the PDDL2 problem generators~\cite{seipp2022pddl}: \emph{Blocksworld}, \emph{Ferry}, \emph{Grippers}, and \emph{Spanner}.
For each domain, we generate $1{,}000$ planning problems with parameters ranging from simple to complex (e.g., different numbers of blocks, objects, or locations), of which $500$ are used for SFT and $500$ for GRPO training.
All problems are solved by the temporal planner OPTIC~\cite{benton2012temporal} and validated by VAL~\cite{howey2004val} to ensure correctness.
Importantly, these problems cannot be solved by the solutions of the corresponding PDDL2 benchmark problems without incorporating the additional safety constraints, which highlights the need for a safety-aware planner.
The test set consists of $50$ problems per domain.

\noindent\textbf{Model selection and training environment.}
We instantiate our framework with open-source LLMs available through the Unsloth library~\cite{unsloth} on Hugging Face~\cite{wolf2020transformers}.
Specifically, we consider three models: \textbf{Mistral-7B}\footnote{\texttt{unsloth/mistral-7b-instruct-v0.3-bnb-4bit}}, \textbf{Llama-8B}\footnote{\texttt{unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit}}, and \textbf{Qwen3-14B}\footnote{\texttt{unsloth/Qwen3-14B-unsloth-bnb-4bit}}.
All models use quantized 4-bit variants with LoRA (rank$=$32, $\alpha$$=$64) throughout our experiments.
All training is performed on a single NVIDIA H100 GPU.
SFT uses a learning rate of $2 \times 10^{-4}$ for 3 epochs; GRPO uses $1 \times 10^{-5}$ for 1{,}000 steps with $K{=}8$ generations per prompt and KL penalty $\beta{=}0.01$.
The reward intervals (Eq.~\ref{eq:separation}) are set as: $r_5{=}{+}1.0$ (success), $[r_4^{-}, r_4^{+}]{=}[-0.4, -0.1]$ (goal not satisfied), $[r_3^{-}, r_3^{+}]{=}[-0.6, -0.3]$ (precondition violation), $[r_2^{-}, r_2^{+}]{=}[-0.9, -0.6]$ (safety violation), and $r_1{=}{-}1.0$ (format error).
For curriculum learning, the early/mid/late phases sample easy/medium/hard problems with probabilities $(70, 25, 5)\%$, $(40, 40, 20)\%$, and $(20, 40, 40)\%$, respectively.

\subsection{Running Time and Scalability Comparison}
We compare the runtime and success rates of GPT-5.2~\cite{openai2025gpt5} (LLM-based) with two classical planners---OPTIC~\cite{benton2012temporal} and Fast Downward~\cite{helmert2006fast}---on $48$ problems of increasing complexity in Blocksworld and Grippers, with a $300$s timeout.

\begin{figure}[!ht]
  \centering
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/comparison_blocksworld_3way.png}
    \caption{Blocksworld: GPT-5.2 100\% (48/48), avg 102.3s; OPTIC 45.8\% (22/48), avg 101.4s; Fast Downward 27.1\% (13/48), avg 218.4s.}
    \label{fig:benchmark-blocksworld}
  \end{subfigure}
  \vspace{0.3em}
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/comparison_grippers_3way.png}
    \caption{Grippers: GPT-5.2 100\% (48/48), avg 102.6s; OPTIC 64.6\% (31/48), avg 148.1s; Fast Downward 14.6\% (7/48), avg 255.8s.}
    \label{fig:benchmark-grippers}
  \end{subfigure}
  \caption{Three-way benchmark comparison: GPT-5.2 vs OPTIC vs Fast Downward. Green squares = success, red circles = failure.}
  \label{fig:benchmark-comparison}
\end{figure}

As shown in Figure~\ref{fig:benchmark-comparison}, GPT-5.2 achieves 100\% success in both domains with stable runtime ($\sim$102s), while both classical planners struggle as problem complexity grows. OPTIC solves 45.8\% of Blocksworld and 64.6\% of Grippers problems; Fast Downward performs worse at 27.1\% and 14.6\%, respectively, with the highest average runtimes. These results suggest that LLM-based solvers can alleviate scalability bottlenecks of classical planners.

We note that this experiment uses GPT-5.2 (via API) rather than our fine-tuned models, as the problem instances are \textbf{deliberately chosen to be highly complex} to stress-test classical planners, exceeding the capacity of our locally trained 7--14B parameter models under current GPU constraints.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/blocksworld_model_comparison.png}
  \caption{Error type distribution across training stages (Pretrained $\rightarrow$ SFT $\rightarrow$ GRPO) for Mistral-7B in the Blocksworld domain.}
  \label{fig:model-comparison}
\end{figure}

\begin{table*}[!t]
  \centering
  \caption{Error Type Percentages by Domain Across Training Stages and GPT-5 Nano Baseline}
  \label{tab:error_percentages}
  \setlength{\tabcolsep}{3.5pt}
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{l||cccc|cccc|cccc|cccc|cccc}
  \hline
  \textbf{Domain}
  & \multicolumn{4}{c|}{Success Plans $\uparrow$}
  & \multicolumn{4}{c|}{Plan Format Error}
  & \multicolumn{4}{c|}{Precondition Violation}
  & \multicolumn{4}{c|}{Safety Constraint Violation}
  & \multicolumn{4}{c}{Goal Not Satisfied} \\
  \hline
  & \textbf{PT} & \textbf{SFT} & \textbf{GRPO} & \textbf{GPT-5}
  & \textbf{PT} & \textbf{SFT} & \textbf{GRPO} & \textbf{GPT-5}
  & \textbf{PT} & \textbf{SFT} & \textbf{GRPO} & \textbf{GPT-5}
  & \textbf{PT} & \textbf{SFT} & \textbf{GRPO} & \textbf{GPT-5}
  & \textbf{PT} & \textbf{SFT} & \textbf{GRPO} & \textbf{GPT-5} \\
  \hline
  \multicolumn{21}{l}{\textit{Qwen3-14B}} \\
  \hline
  Blocksworld & 2.0 & 70.0 & \textbf{88.0} & 18.0 & 90.0 & 0.0 & 0.0 & 2.0 & 6.0 & 24.0 & 12.0 & 6.0 & 0.0 & 2.0 & \textbf{0.0} & 68.0 & 2.0 & 4.0 & \textbf{0.0} & 4.0 \\
  Ferry       & 0.0 & 86.0 & \textbf{96.0} & 20.0 & 10.0 & 0.0 & 0.0 & 0.0 & 90.0 & 4.0 & 4.0 & 0.0 & 0.0 & 10.0 & \textbf{0.0} & 80.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
  Grippers    & 2.0 & 92.0 & \textbf{98.0} & 94.0 & 0.0 & 0.0 & 0.0 & 0.0 & 46.0 & 4.0 & 0.0 & 6.0 & 46.0 & 4.0 & \textbf{2.0} & 0.0 & 6.0 & 0.0 & 0.0 & 0.0 \\
  Spanner     & 0.0 & \textbf{100.0} & \textbf{100.0} & 66.0 & 2.0 & 0.0 & 0.0 & 4.0 & 98.0 & 0.0 & 0.0 & 30.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
  \hline
  \multicolumn{21}{l}{\textit{Llama-8B}} \\
  \hline
  Blocksworld & 0.0 & 50.0 & \textbf{78.0} & 18.0 & 78.0 & 0.0 & 0.0 & 2.0 & 22.0 & 38.0 & 16.0 & 6.0 & 0.0 & 2.0 & \textbf{2.0} & 68.0 & 0.0 & 10.0 & 4.0 & 4.0 \\
  Ferry       & 0.0 & 80.0 & \textbf{88.0} & 20.0 & 2.0 & 0.0 & 0.0 & 0.0 & 98.0 & 4.0 & 8.0 & 0.0 & 0.0 & 16.0 & \textbf{4.0} & 80.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
  Grippers    & 0.0 & 66.0 & 82.0 & \textbf{94.0} & 42.0 & 0.0 & 0.0 & 0.0 & 42.0 & 22.0 & 14.0 & 6.0 & 16.0 & 8.0 & \textbf{2.0} & 0.0 & 0.0 & 4.0 & 2.0 & 0.0 \\
  Spanner     & 0.0 & 92.0 & \textbf{94.0} & 66.0 & 100.0 & 0.0 & 0.0 & 4.0 & 0.0 & 8.0 & 6.0 & 30.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
  \hline
  \end{tabular}
  }
  \vspace{0.5em}

  \footnotesize{PT = Pretrained; SFT = Supervised Fine-Tuning; GRPO = Group Relative Policy Optimization; GPT-5 = GPT-5 Nano (zero-shot). Values in \%.}
\end{table*}

\begin{table*}[!t]
  \centering
  \caption{Performance comparison: Symbolic (PDDL3) vs Natural Language vs JSON input. All values in \%.}
  \label{tab:input_format_comparison}
  \resizebox{0.95\textwidth}{!}{%
  \begin{tabular}{l||ccc|ccc|ccc|ccc|ccc}
  \hline
  \textbf{Domain}
  & \multicolumn{3}{c|}{Success $\uparrow$}
  & \multicolumn{3}{c|}{Format Error}
  & \multicolumn{3}{c|}{Precondition Violation}
  & \multicolumn{3}{c|}{Safety Constraint Violation}
  & \multicolumn{3}{c}{Goal Not Satisfied} \\
  \hline
  & \textbf{PDDL3} & \textbf{NL} & \textbf{JSON}
  & \textbf{PDDL3} & \textbf{NL} & \textbf{JSON}
  & \textbf{PDDL3} & \textbf{NL} & \textbf{JSON}
  & \textbf{PDDL3} & \textbf{NL} & \textbf{JSON}
  & \textbf{PDDL3} & \textbf{NL} & \textbf{JSON} \\
  \hline
  Blocksworld & \textbf{88.0} & 74.0 & 80.0 & \textbf{0.0} & \textbf{0.0} & \textbf{0.0} & 12.0 & 18.0 & \textbf{8.0} & \textbf{0.0} & 8.0 & 8.0 & \textbf{0.0} & \textbf{0.0} & 4.0 \\
  Ferry       & \textbf{96.0} & 90.0 & \textbf{96.0} & \textbf{0.0} & \textbf{0.0} & \textbf{0.0} & 4.0 & 6.0 & \textbf{2.0} & \textbf{0.0} & 4.0 & 2.0 & \textbf{0.0} & \textbf{0.0} & \textbf{0.0} \\
  Grippers    & \textbf{98.0} & 82.0 & \textbf{98.0} & \textbf{0.0} & \textbf{0.0} & \textbf{0.0} & \textbf{0.0} & 14.0 & 2.0 & 2.0 & 4.0 & \textbf{0.0} & \textbf{0.0} & \textbf{0.0} & \textbf{0.0} \\
  Spanner     & \textbf{100.0} & 90.0 & 96.0 & \textbf{0.0} & 2.0 & \textbf{0.0} & \textbf{0.0} & 8.0 & 4.0 & \textbf{0.0} & \textbf{0.0} & \textbf{0.0} & \textbf{0.0} & \textbf{0.0} & \textbf{0.0} \\
  \hline
  \textbf{Average} & \textbf{95.5} & 84.0 & 92.5 & \textbf{0.0} & 0.5 & \textbf{0.0} & \textbf{4.0} & 11.5 & \textbf{4.0} & \textbf{0.5} & 4.0 & 2.5 & \textbf{0.0} & \textbf{0.0} & 1.0 \\
  \hline
  \end{tabular}
  }
\end{table*}

\subsection{Cross-Problem Safety Generalizability}
We begin with cross-problem safety generalization, i.e., whether a model trained on planning problems within a given domain can solve previously unseen problems with safety constraints in the \emph{same} domain.

We use the Blocksworld domain as a representative example and fine-tune Mistral-7B through our two-stage pipeline (SFT followed by GRPO).
Figure~\ref{fig:model-comparison} compares the error type distribution across three stages: Pretrained, SFT, and GRPO.

We summarize the key observations as follows:
\begin{enumerate}
  \item \textbf{Dramatic reduction in precondition violations.}
  The precondition violation rate decreases from 98\% (Pretrained) to 20\% (SFT) to 8\% (GRPO).
  This indicates that both training stages progressively internalize domain-specific action semantics.

  \item \textbf{Progressive improvement in safety compliance.}
  Safety constraint violations decrease from 10\% (SFT) to 4\% (GRPO).
  The low 2\% rate of the Pretrained model is an artifact---most of its plans fail at earlier validation stages before safety checks are reached.

  \item \textbf{Substantial success rate improvement.}
  The success rate improves dramatically: from 0\% (Pretrained) to 66\% (SFT) to 82\% (GRPO).
  This demonstrates that our two-stage training pipeline enables the model to generate plans that both achieve goals and satisfy safety constraints.
\end{enumerate}

These results demonstrate that our framework achieves substantial improvements in safety-aware planning through the combination of SFT and GRPO.

\subsection{Cross-domain Safety Generalizability}

We next evaluate cross-domain generalization by applying the same two-stage pipeline (SFT $\rightarrow$ GRPO) to Qwen3-14B and Llama-8B, trained on all four domains simultaneously with domain-balanced batching and curriculum learning.
We also include GPT-5 Nano\footnote{\texttt{gpt-5-nano-2025-08-07}, evaluated via the OpenAI API with the same prompt template in a zero-shot setting.}~\cite{openai2025gpt5} as a proprietary baseline to contextualize the performance of our fine-tuned models.

Table~\ref{tab:error_percentages} summarizes the error-type distributions across all training stages---Pretrained (PT), SFT, and GRPO---for both models, alongside GPT-5 Nano.
We highlight several key findings:

\begin{enumerate}
    \item \textbf{Pretrained models produce mostly format and precondition errors.}
    Without fine-tuning, both models fail almost entirely: success rates are 0--2\%.
    Format errors range from 2--100\% and precondition violations from 6--98\%, as pretrained models have never encountered PDDL plan syntax.

    \item \textbf{SFT eliminates format errors and dramatically improves success.}
    After SFT, format errors drop to 0\% for both models across all domains.
    Success rates jump to 70--100\% for Qwen3-14B and 50--92\% for Llama-8B, demonstrating that supervised learning effectively teaches plan structure and domain semantics.

    \item \textbf{GRPO further improves success while reducing safety violations.}
    GRPO raises Qwen3-14B success rates to 88--100\% and Llama-8B to 78--94\%.
    Safety violations are nearly eliminated, reaching 0--2\% for Qwen3-14B and 0--4\% for Llama-8B.
    This confirms that online RL with verifiable rewards effectively teaches models to prioritize safety compliance.

    \item \textbf{GPT-5 Nano struggles with safety.}
    Despite strong general capabilities, GPT-5 Nano achieves only 18--20\% success on Blocksworld and Ferry with safety violations up to 80\%, showing that targeted fine-tuning with verifiable rewards enables smaller open-source models to outperform larger proprietary ones on safety-constrained tasks.

    \item \textbf{Consistent trends across model scales.}
    Both models follow the same PT $\rightarrow$ SFT $\rightarrow$ GRPO progression. Qwen3-14B achieves higher absolute performance given its larger capacity, but Llama-8B still attains strong results (e.g., 88\% on Ferry, 82\% on Grippers).
\end{enumerate}

\subsection{Input Format Comparison: Symbolic vs Natural Language vs JSON}

We compare model performance across three input formats: symbolic PDDL3, natural language (NL), and JSON.
Although the model is trained exclusively on PDDL3, Table~\ref{tab:input_format_comparison} shows that it generalizes well to unseen formats: JSON achieves 92.5\% average success (matching PDDL3 on Ferry and Grippers), and NL reaches 84.0\%, both with near-zero format errors.
This confirms that the learned planning knowledge transfers across input representations, enabling flexible deployment.

\subsection{Integration with LLM Agentic Workflows}

We evaluate whether our trained model can be integrated with existing LLM agentic workflows by combining it with SafePilot, a verification-guided iterative refinement framework that regenerates plans until validation passes.

\begin{table}[t]
\centering
\caption{Planning success rates and retry counts with SafePilot.}
\label{tab:safepilot_comparison}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Blocksworld} & \textbf{Ferry} & \textbf{Grippers} & \textbf{Spanner} & \textbf{Avg.} \\
\midrule
\multicolumn{6}{l}{\textit{Success Rate (\%) $\uparrow$}} \\
Pretrained & 50.0 & 0.0 & 50.0 & 0.0 & \textbf{25.0} \\
GRPO & 94.0 & 98.0 & 98.0 & 100.0 & \textbf{97.5} \\
\midrule
\multicolumn{6}{l}{\textit{Avg. Retries $\downarrow$}} \\
Pretrained & 3.50 & 5.00 & 3.00 & 5.00 & \textbf{4.13} \\
GRPO & 1.36 & 1.12 & 1.14 & 1.00 & \textbf{1.16} \\
\bottomrule
\end{tabular}%
}
\end{table}

As shown in Table~\ref{tab:safepilot_comparison}, even with SafePilot, the Pretrained model achieves only 25.0\% success with 4.13 retries, while the GRPO model reaches 97.5\% with just 1.16 retries---up from the standalone 95.5\% (Table~\ref{tab:error_percentages}).
This shows that SafePilot cannot compensate for a weak base model, but provides a meaningful reliability boost when combined with GRPO training.
\begin{figure}[!t]
  \centering
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{Blocksworld.pdf}
    \caption{Case study in the Blocksworld domain.}
    \label{fig:case-study}
  \end{subfigure}
  \vspace{0.5em}
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{experiments.pdf}
    \caption{Snapshot of the physical robot executing the Blocksworld task.}
    \label{fig:experiment-snapshot}
  \end{subfigure}
  \caption{Real-world validation in Blocksworld: (a) simulation case study comparing a classical solver (unsafe) with our safety-aware planner (safe); (b) physical deployment on a robot arm.}
  \label{fig:real-world-validation}
\end{figure}

\subsection{Real-World Validation in Blocksworld}

We validate our framework in both simulation and physical deployment using Blocksworld.

\paragraph{Simulation.}
We compare a classical PDDL2 solver with our SFT+GRPO model on a problem requiring a specific stacking order to avoid collisions.
As shown in Figure~\ref{fig:case-study}, the classical solver violates the safety constraint, while our model restructures the action sequence to satisfy the constraint while achieving the goal.

\paragraph{Physical Deployment.}
We deploy the planner on a desktop robot arm (Elephant myCobot~280) controlled by a Raspberry~Pi, transmitting LLM-generated plans via SSH.
As shown in Figure~\ref{fig:real-world-validation}(b), the unsafe baseline causes a physical collision during stacking, whereas the safety-aware plan completes the task collision-free, confirming that the framework yields robust safety behaviors on real robotic hardware.

